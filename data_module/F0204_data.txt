异构多核并行机上线性代数方程组的快速算法研究	低秩性;快速直接法;迭代法;并行计算;异构多核	异构多核体系结构是当前并行计算机的主流，而线性代数方程组的求解在复杂问题的数值模拟中占有很高的比重。本项目拟面向异构多核并行机研究快速并行算法。包括：基于矩阵低秩分解特性，设计并实现具有较好可扩展性的多核并行快速直接方法，即多核稳定的结构化多波前分解(MRSMF)方法，对所得的MRSMF方法进行理论和性能分析，并对其进行性能优化；研究基于降低全局通讯次数的并行Krylov子空间方法，进行理论分析和数值实验；针对辐射流体力学数值模拟的二维三温能量方程组的并行求解，设计出适应其结构特点的高效分块MRSMF方法，研究并行Krylov子空间方法与MRSMF方法的混合使用，以提高数值模拟的并行效率。
微波成像稀疏信号重构的FPGA加速方法研究	FPGA加速;稀疏信号重构;微波成像	微波成像在军用和民用领域有着重大需求和广泛应用。稀疏信号重构理论给高分辨微波成像提供了新的技术途径。随着重构算法的不断完善，算法稳健的实时实现成为迫切需要解决的问题。和传统的微波成像信号处理算法相比，稀疏重构算法计算复杂度急剧增加，这给实时处理带来了极大的困难。针对该问题，本课题开展微波成像稀疏信号重构的FPGA加速研究，主要研究内容：1）大字典矩阵存储计算模型和全流水线并行结构；2）结合先验信息挖掘的自适应信号重构计算策略选择和电路实现方法；3）基于统一框架的混合算法复用电路设计。本课题力争在微波成像稀疏优化算法硬件结构映射理论和FPGA加速技术上有所创新，为稀疏信号重构的VLSI电路设计奠定一定的理论和技术基础，促进稀疏优化算法在微波成像系统中的应用。
虚拟化环境中高效节能的内存资源动态管理技术研究	内存节能;内存热插拔;rank可感知;内存动态管理;虚拟化	在虚拟化环境中，高效地共享内存资源并满足虚拟机的动态内存需求是一个很大的挑战。当前虚拟机内存管理采用的"静态配置+动态调整（ballooning机制）"模式存在内存分配无法突破边界上限、分配和利用效率低下及不利于内存能耗控制等问题。本课题拟提出一种全新的虚拟机内存资源管理模式，保证内存高效利用的同时兼顾内存节能。拟开展以下工作：1)研究基于地址空间离散分布的虚拟机内存动态分配模型，真正实现内存资源的按需分配和共享；2)研究面向虚拟化的内存热插拔技术，实现虚拟机的内存地址空间动态扩展，使内存分配不再受限于静态配置上限，保障负载的动态内存需求；3)研究面向虚拟化环境的rank可感知的内存节能技术，综合利用内存页面的时空特性，设计能耗可感知的高效内存页面迁移算法及具有最佳"能耗o延迟"的内存能耗控制算法；4)研究基于公平性原则的虚拟机内存动态平衡算法，避免多虚拟机间由于内存竞争而带来的性能干扰。
高效能自适应处理器体系结构关键技术研究	数据通路;动态自适应;高速缓存;指令系统	本项目从指令系统、数据通路和片上缓存三个方面，研究能够动态匹配负载行为特征的自适应处理器体系结构设计方法，具体包括：（1）组件式的定制指令构造方法。组件即原子数据通路单元，定制指令由若干组件构成，并能在不同指令间共享，从而提高资源利用率。（2）数据通路的动态缩放方法。定制指令的数据通路有多种实现方式，包含不同数量和种类的组件。数据通路根据负载需求，动态增减组件资源，支持不同的性能/功耗级别。（3）计算/存储功能可重构的一级缓存设计方法。利用LUT存储部件可用于计算这一特性，动态调节一级缓存的存储空间和计算能力，匹配不同负载对访存和计算的需求。（4）共享/私有模式可重构的二级缓存设计方法。针对不同负载对缓存需求的差异，动态分配私有和共享空间，同时达到私有缓存的低延迟和共享缓存的低缺失率。上述研究内容不损失通用处理器的可编程性，对于不同类型的基准程序，能够显著提高处理器的性能和能量效率。
多阶段多状态系统可靠性的动态故障树模块化分析方法	决策图;模块化;动态故障树;多阶段多状态系统	多阶段多状态系统（MPMSS）是一类具有阶段性运行特征和多状态部件的复杂计算系统，其可靠性分析受到学术界高度重视，但已有分析方法在效率和精度方面不能很好的满足实际需求。本项目针对已有研究在模块化方法、动态模块分析、模块联合概率求解和模块化故障树决策图构造等方面存在的问题，重点研究：多粒度模块划分和特征依赖模块化方法选择、各种粒度模块化下通用模块联合概率求解、基于补充变量和MRGP 的高效动态模块分析、基于分治Ebendt 搜索和启发式重组的模块化故障树决策图构造。项目预期将获得高效的MPMSS 可靠性模块化混合分析方法，从而扩大可分析MPMSS 的规模和复杂度、提升可靠性分析的准确性和效率。项目的研究成果对于完善复杂计算系统可靠性评估的理论研究具有重要意义，可以为关键应用领域中实时容错计算系统的可靠性评估提供切实可行的解决方案，为冗余系统结构和容错机制等高可靠性设计决策的制定提供科学依据。
三维视频处理系统芯片动态可重构可编程体系结构研究	阵列处理器;三维视频;动态可重构;体系结构;片上系统	以多点视频压缩编码为代表的三维视频处理，是高清视频实时通信系统的关键技术。多视点和高分辨率编解码，对视频处理SoC的计算能力和编程灵活性提出了更高的要求。芯片集成度的提高，又使处理器体系结构设计面临长线、功耗与工艺缺陷等红墙问题。项目面向三维视频阵列SoC，尝试采用大量同构、规则处理器元邻接互连的思想，探索建立适应未来制造工艺发展的动态可重构可编程结构，不仅支持MVC、H.264/AVC和AVS等多种视频压缩标准，还可同时实现细粒度的大规模并行指令流计算（比特流解析、VLD等）和并行数据流计算（DCT、ME等）。研究处理器元及其互连结构、阵列处理器指令集架构和片上存储结构，预期提出的统一体系结构和开发的原型系统，既具有编程的灵活性又能通过重构达到ASIC/FPGA等专用硬件的性能。项目将为下一代片上系统体系结构研究提供新的思路，推动任意视点视频、三维电视和虚拟现实等新兴多媒体应用的发展。
加密芯片抗扫描旁路攻击的关键技术研究	物理不可克隆设计;旁路攻击;硬件安全;加密芯片	加密芯片已广泛应用于多个安全领域，但是它们容易受到扫描旁路攻击，安全性面临着极大威胁。因此，研究加密芯片抗扫描旁路攻击的技术有重要意义。针对现有技术存在的安全漏洞，本项目提出使用物理不可克隆函数（PUF）电路的、抗扫描旁路攻击的实用安全扫描链设计方案。方案引入了"锁-钥匙"机制，利用PUF电路生成"钥匙"，并通过对扫描链中部分特定扫描单元的测试控制端口加"动态锁"，实现了"只有钥匙正确，用户才可以正常使用扫描链"，有效阻止了非法用户利用扫描链观察加密中间结果从而破解密钥。该方案可以效抵抗已知的各种扫描旁路攻击，利用PUF则实现了安全"钥匙"的设计。同时，方案对原设计各方面性能影响小。在加密芯片广泛应用而扫描旁路攻击日益猖獗的今天，该技术有很高的应用价值。
基于容器技术的云工作流任务与虚拟化资源协同自适应调度研究	工作流;协同;容器;自适应;资源	本课题围绕任务分配和资源供给这两个云工作流执行过程中的关键步骤，研究以容器为新型虚拟化基本单元的云计算环境下，利用高性能机器学习算法和多agent技术，以容器簇为粒度进行多工作流任务和虚拟化容器资源协同自适应调度的理论问题、关键技术和方法，以确保在满足用户服务等级协议前提下，最大程度实现云服务供需双方的利益均衡。重点研究：(1)研究不同服务质量需求下的多工作流任务分配，包括任务优先级的初始设置、运行中的动态调整、容器间的负载均衡。(2)研究不同服务质量需求下的多工作流资源供给，包括容器资源的生成、优化部署。(3)研究云工作流任务与虚拟化容器资源协同自适应调度机制，包括多agent社会与容器簇的逻辑关系、多agent社会下的协同和自适应调度机制、Sarsa强化学习的高性能改进机制。(4)开发原型验证系统，构建试验床，验证本课题所提出的理论、技术和方法的有效性和优越性。
众核ASIP的算法映射和专用指令定制关键技术研究	集成开发框架;众核ASIP;算法并行化;专用指令定制;算法映射	嵌入式领域中，图像、信号处理等应用的复杂度已显著增加，单处理器已无法满足应用对计算能力和功耗效率的要求。作为一类特殊的异构众核结构，众核ASIP可显著提升处理器的计算能力、可扩展性和功耗效率。但众核ASIP在设计方法学方面面临3个亟待决绝的基础性问题：如何高效地分析和并行化应用算法，如何高效地将算法中的任务和通信映射到系统中的处理器核和通信通道上，如何高效地针对关键算法定制专用指令以进一步提升效率。.    针对上述挑战，本项目提出一种基于标准C/C++程序的算法并行化和算法高层次模型提取技术，提出一种采用高层次分析、高层次模拟、周期精确模拟相结合的层次化算法映射技术，提出一种考虑众核处理器的计算/通信时间的ASIP专用指令定制技术，并基于这些技术和现有研究成果构建一个众核ASIP集成开发框架。我们希望本项目的研究成果能够辅助国产高性能专用处理器的研发，并应用于实际应用中。
IaaS平台中基于虚拟机快照的失效恢复技术研究	冗余容错计算机;存储系统可靠性;高可用技术	可用性是IaaS云计算平台中的核心问题，其制约着IaaS平台的健康发展和应用推广。失效恢复技术是增强系统可用性的有效手段，然而，IaaS平台因规模庞大且结构复杂导致系统失效频发，给失效恢复技术带来了新的问题和挑战。本项目以快照生命周期中创建、存储、回滚三个阶段为切入点，开展基于虚拟机快照的失效恢复技术研究。具体内容包括：基于快照创建、存储及回滚构建失效恢复技术体系模型，在此基础上，重点针对虚拟集群计算形态，在面向虚拟集群的分布式持续快照创建、面向快照文件的高可用存储、面向虚拟集群的快速回滚三项关键技术方面开展研究，开发相应原型系统，并依托我们已建立的IaaS实验平台进行系统集成，结合体系模型开展关键技术的评测和验证。本项目可为基于快照的失效恢复系统研制提供理论基础和技术支撑，为IaaS平台的可用性增强提供技术思路，对于推动云计算平台及其业务的健康发展具有重要的应用价值。
多核系统中实时调度策略的设计与分析技术的研究	多核系统;共享资源敏感;实时调度;时间分析	随着应用需求的提升，实时系统日益复杂，因此对系统性能提出了越来越高的要求。由多核处理器组成的多核系统在性能上具有非常好的规模伸缩性，因此得到了实时嵌入式领域的广泛关注。多核系统通常采用共享Cache和通信总线的设计，多个核心上的任务对这些共享资源的细粒度访问造成了系统时间行为的复杂性和不可预测性，给实时系统的时间特性的分析带来了巨大挑战。本项目将针对主流多核系统的结构特点，研究Cache敏感的多核实时调度策略，以及通信资源敏感的多核实时调度策略，研究基于共享Cache的程序最坏情况执行时间分析技术，最后设计实现一个多核实时调度原型系统及其配套的分析工具。通过上述工作达到"在尽量少的损失多核系统平均性能的前提下，系统的时间可预测性得到充分保证"的最终目标，为多核系统在实时系统中的应用突破一些技术瓶颈。
物联网隐私保护安全关键技术研究	隐私同态;安全多方计算;信息安全;物联网;隐私保护	物联网(Internet of Things)要实现广泛应用必须要解决其现存的隐私泄露问题。物联网隐私保护可能涉及到法律、道德和技术等各个方面，本课题主要从技术层面出发，研究基于安全机制的隐私保护方法。本课题的主要研究内容分为五个方面：一是密钥管理和认证技术，作为物联网的安全基础设施，同时实现其异构子网间的安全通信；二是抗节点(物理)俘获攻击的新方法，主要考虑重加密和散列锁存机制以及追踪方法；三是实现隐私保护的两个关键技术- - 隐私同态和信息隐藏；结合密钥技术和隐私同态，研究内容四探讨安全多方计算来实现隐私数据的融合及安全共享；最后通过基于无线传感器网络的物联网应用示范来验证所提出的隐私保护的安全协议和算法。
容错存储系统的扩容问题研究	存储系统;数据容错;数据扩容	磁盘故障是大型存储系统中常见现象，容错已成为当前存储系统必备的基本功能。另一方面，数据量的高速增长以及用户对系统性能不断增长的需求导致存储系统需要扩容。本项目拟研究基于不同容错编码的存储系统的有效扩容算法与相应数据迁移算法，优化系统扩容过程中系统负载均衡、迁移数据量、IO负载、校验块的更新、元数据的管理、数据一致性以及数据存储连续性等多方面的性能，具体的研究内容包括：（1）针对各种RAID编码、纠删码、再生码，设计系统扩容算法；（2）针对以上各种编码，设计增加容错能力的扩容算法；（3）针对不同存储介质的存储系统，如固态硬盘、传统硬盘，建立扩容过程中性能参数模型，设计优化的扩容算法；（4）针对一些不同的系统访问模式，通过其trace数据分析影响扩容算法性能的关键因素，设计优化的在线扩容算法；（5）建立一个实际的异构分布式存储系统，在此系统上实现各种扩容算法，优化扩容算法在实际系统中的部署。
面向异构众核系统的统一编程框架研究	并行编程语言;异构并行计算;异构众核系统;并行编程模型;异构并行系统	针对异构众核系统面临的编程难、程序移植难、系统资源得不到充分利用的挑战，本项目围绕异构众核系统编程模型展开研究，探索降低异构众核系统编程难度、充分发挥其计算能力、提高其通用性的编程模型相关理论及关键技术。项目以协处理器为中心抽象多样化发展态势的异构众核系统逻辑架构；研究面向计算单元的多任务流多数据流编程模型及并行描述语言；研究高层并行代码到跨平台中间代码的编译转换；研究中间代码到具体异构众核系统底层硬件映射机制。从而构建一个向上为用户提供屏蔽底层实现细节，简单高效跨平台的异构众核系统并行编程环境，向下能适应异构系统底层硬件架构变化，提供高效映射运行机制的统一编程框架，提高异构众核系统可编程性、软件可移植性、充分发挥异构众核系统高效计算潜能。本课题的研究将对高性能计算机系统结构的基础理论，对促进异构计算的广泛应用，以及《国家中长期科学和技术发展规划》提出的先进计算平台建设具有重要意义。
面向异构云计算中心IaaS服务的性能分析模型	层次建模;马尔可夫链;定点迭代;性能分析;云计算	异构云计算中心IaaS（Infrastructure as a Service）服务性能分析是IaaS云服务供应商必须要解决的问题。理论建模是行之有效的性能分析方法，但传统计算系统的模型成果没有考虑云计算中心规模庞大、参数多以及虚拟化等特征；而现有云服务模型成果或者对系统做了不切实际的假设，或者近似精确度较低，或者没有进行理论分析(包括整体模型的遍历性、算法收敛性等)。本项目旨在利用嵌入马尔可夫链和层次建模等技术，解决现有云服务模型成果存在的不足，建立能分析实际异构云计算中心IaaS服务性能的高精度且高效的模型，以此为基础来研究各种性能指标的计算方法，从而定量分析各种因素对云服务性能以及云中心成本的影响，促进异构云计算中心资源分配体系的完善。
新型体系结构上的图计算技术与方法研究	新型体系结构;图计算;数据划分;语义模型;编程模型	图数据的蓬勃发展使得图计算技术与系统的研究变得越来越重要，但由于图处理的时空局部性差、访存密集、迭代计算等处理特点，以局部性原理为指导的传统体系结构设计在图处理中效率低下。针对这一问题，本课题拟开展新型体系结构上的图数据处理关键技术与方法研究。围绕新型体系结构上的图计算表达、编程以及数据划分与放置，本项目从三方面开展研究：1）针对新型体系结构的并行特征与图计算迭代特征，研究新型体系结构下的图计算语义模型，包括语义规则库、DSL解释器等；（2）针对新型体系结构的异构并行性与图处理扩展性需求，研究多体系结构支持的图处理编程模型，包括图计算并行处理抽象与体系结构运行时支撑等；（3）针对体系结构局部内存访存高、容量小特征与大图处理需求，研究兼顾图处理计算负载均衡与通信开销、访存效率与冗余开销的图数据划分与放置策略，包括图数据存储格式、数据划分策略等。
云数据中心基于应用共存特性的混合调度研究	云数据中心;调度;应用共存	随着云数据中心规模的不断增加和云应用的层出不穷，为解决资源利用最大化与应用共存冲突导致服务质量下降之间的矛盾，当前云数据中心普遍采用基于虚拟化技术的共享服务器和非虚拟化的独占服务器并存的使用模式。在该大规模混合场景下，如何将大量云应用自动、高效地映射到异构服务器成为当前云数据中心面临的一大挑战。本课题拟研究以下三个问题：1）针对如何预测具有不同资源需求特性的应用在不同负载下资源需求问题，考虑构建云应用及应用组合对异构服务器规模需求的预测模型；2）针对如何定量刻画云应用在共享资源时相互影响程度的问题，考虑进行细粒度应用资源占用模式及共存冲突分析；3）针对如何将大量云应用高效映射到大规模异构服务器的问题，考虑设计基于应用共存非线性叠加的应用自动部署方法。通过该项目的研究，将在ACM或IEEE期刊或会议上发表学术论文6篇以上，同时将为云数据中心提供高效的应用分析及部署方法，促进云数据中心的发展。
基于安全Agent的可信云计算与对等计算融合模型及关键技术的研究	可信计算;信息安全;对等计算;Agent;云计算	目前的云计算只考虑服务器端的计算能力和存储资源，忽视了网络边缘的潜在资源。申请人领导的课题组创新性地将云计算模型扩展为云计算与对等计算融合模型（Cloud-P2P），充分聚集网络服务器端和网络边缘终端上的海量资源，从而获得更高的成本效益。在动态、开放的Cloud-P2P计算环境中联合跨组织和管理域的节点来合作完成大规模的计算任务，带来了新的问题：主体行为不可靠，计算和数据的安全可信性以及执行效率都难以保障。本项目重点研究基于安全Agent的可信Cloud-P2P计算融合模型，并对多租客环境下的计算与数据的私密性保护和自销毁机制、执行体与执行点安全可信性评价机制及执行策略和大规模、异构计算环境下的协同管理监控机制展开深入研究，并构建平台及应用系统。本项目的研究成果将进一步保证Cloud-P2P的安全可信性，对目前的云计算及其它分布式计算系统的安全与可信机制的研究与构建也具有良好的参考价值。
跨云服务商的用户数据确定性删除机制研究	确定性删除;云计算;访问控制;隐私保护;属性基加密	本项目针对长期存储在公共云环境中的用户数据因所有权与管理权分离而导致的数据滥用与隐私泄露问题，结合密码学、图论和主动存储机制等技术，研究跨云服务商的用户数据确定性删除机制的关键技术和方法。具体而言：在定义授权期限和确定性删除等概念基础上，研究自适应安全的多授权中心属性基加密算法、时间区间加密算法和二者结合的弹性密钥封装机制；并针对WWW中随机网页的资源更新特征，研究用户数据授权期限的按需设定、授权期内的细粒度访问控制和授权期后的安全删除问题；在此基础上，基于有向无环图研究跨云服务商的数据多副本关联算法，并利用主动存储对象结合云端数据擦除技术研究用户数据多副本关联查找及存储空间释放问题；进一步基于证书、签名技术和无中心技术研究确认反馈机制，以实现用户对数据确定性删除的可信性和可控性。本项目研究并构建跨云服务商的用户数据确定性删除机制，将为创新云数据隐私保护方法提供重要的理论依据和技术支撑。
片上网络异构路由器关键问题研究	缓存资源;片上网络;路由算法;网络架构	集成电路技术在过去的几十年中得到了飞速的发展，在单一芯片上集成多个内核形成系统芯片以构造高性能，低功耗计算机系统已成为主要趋势。设计系统芯片的关键技术之一是设计其中的高性能片上网络，片上网络具有芯片面积受限、低时延、高吞吐和低能耗等技术要求，从而产生了一系列具挑战性问题。本项目将研究片上网络架构级的路由器设计的若干基础和关键问题。主要研究课题包括研究通用和面向各类应用的片上网络的最合适异构路由器的类型、其主要缓存资源在拓扑结构上的分布和配置方法；相关的路由协议和算法，以及新型片上网络路由器的新功能和机制的实现，设计相应的受芯片面积和功耗限制的尽可能高效的片上网络路由器架构原型。
NoC众核系统中基于可靠性的节能实时调度算法及策略研究	NoC;众核处理器系统;节能策略;实时调度;可靠性维护	随着众核技术发展以及实时应用对可靠性和节能要求不断提高,研究基于可靠性的节能实时调度算法及策略,对优化众核处理能力和提高实时应用服务质量有重要价值。本项目从NoC众核系统结构出发,致力于可靠性维护的节能实时调度关键技术研究。建立性能评测库并挖掘规则,研究在线信息处理有效策略,以优化在线决策效率并保障节能实时调度可靠性。采用动态与静态调度有机结合的方法,动态调度借助静态调度的决策及有益信息,处理NoC通信竞争问题,并在VFI电压/频率同步限制及实时可调度前提下,充分回收并合理分配在线空闲时间以有效节能。基于这些研究,权衡调度性能与成本,设计任务容错及热点温度消除的节能实时调度算法,解决维护系统及应用可靠性的技术难题。进而针对实际应用需求,将算法及策略改进、扩展到异构众核系统,并应用于多媒体实时处理等实际领域。本研究将为众核实时应用领域提供新的有效技术手段,可进一步促进众核实时处理技术的发展。
面向稀疏矩阵和图计算的自适应优化方法研究	稀疏矩阵;自适应;并行效率;图;动态优化	随着多核成为计算机体系结构的主流，影响并行程序执行效率的因素愈加复杂多样，而日益突出的能耗问题迫使多核系统上优化并行程序需要同时提高并行效率和能耗效率。考虑到稀疏矩阵和图计算在传统和新兴高性能计算应用中的重要性，同时自适应优化技术在获得性能可移植性方面将发挥日益重要的作用。本项目拟研究多核系统上稀疏矩阵和图计算并行程序自适应优化方法的三个重要内容：1）针对图算法并行扩展性差的问题，研究基于稀疏矩阵原语操作的高可扩展大规模图算法，为实现稀疏矩阵和图算法优化的统一框架提供基础；2）针对稀疏矩阵操作在多核上性能低的问题，研究算法和体系结构特征相结合的自动调优技术，获得可移植性的最优性能；3）针对程序在并行系统上运行时的并行和能耗效率问题，研究自适应的动态优化策略，使得应用程序在不同多核系统上同时获得高并行效率和高能耗效率。通过本项目的研究，为以稀疏矩阵和图为核心的应用提供可移植性的高性能库。
实时软件最坏执行时间的极值统计研究及分析工具原型	实时系统;极值统计;最坏执行时间	（1）以程序执行时间测量值的最大值序列作为样本，研究程序最坏执行时间的极值分布，建立最坏执行时间的极值估计模型，给出模型参数估计方法。(2)建立模型有效性的验证方法。(3)建立实验方法。（4）建立WCET分析工具原型。通过极值统计学与程序最坏执行时间WCET分析相结合的理论框架和实验方法的建立，为WCET估计提供新的分析方法，这种分析方法相对于静态WCET分析方法简单易行，不需要对系统的某种程度上的额外知识和限定性的假设，可以综合考虑处理器的各种硬件特性对程序执行时间的影响，因此可以获得更高精度的WCET估计，为调度分析提供准确的时间信息，提高实时系统效率和资源利用率。建立具有开放性和可扩展性的分析工具框架原型，可以很方便的集成不同的WCET分析方法。
Map/Reduce数据处理平台中内存级数据缓存技术研究	任务调度;内存级数据缓存;MapReduce;数据预取与替换	Map/Reduce数据处理平台是数据中心海量数据处理领域的最新技术进展。降低应用运行时海量数据读取开销，提高应用执行效率，是确保Map/Reduce平台服务质量的关键。内存级数据缓存技术是数据中心提升数据访问效率的一类典型技术。然而，既有的数据缓存研究成果难以适应Map/Reduce平台数据基于计算节点分布存储以及数据本地化处理的新特征，而针对Map/Reduce平台的数据缓存研究尚属空白。本项目拟发展面向Map/Reduce数据处理平台的内存级数据缓存技术；以提升应用执行效率为目标，针对Map/Reduce平台新的数据存储与处理模式，着重对数据访问特征分析方法、数据预取与替换、数据重放置以及缓存感知的Map/Reduce任务调度等关键技术展开研究，并通过原型系统对研究成果进行分析和验证，为在Map/Reduce平台引入内存级数据缓存提供切实可行的理论基础和技术方案。
基于温度受限的低能耗混合存储系统数据调度策略研究	数据分布;混合存储系统;能耗管理;温度控制属性	数据呈爆炸性增长，导致存储设备的功耗和温度上升，硬盘的失效率急剧增加，系统的可靠性降低，如何在确保系统可靠性的同时，降低系统能耗，优化数据分布，成为混合存储系统急需解决的问题。从混合存储系统的层次结构上出发， 通过建立部件级的性能模型、功耗模型和温度模型，在满足工作温度门限的前提下， 根据不同应用特征和混合存储系统的异构组织方式， 研究基于温度受限的低能耗数据分布策略。 根据温度的变化特征、用户访问特征，保证在工作温度低于可控阈值的前提条件下，主动将数据聚集在较少工作集的存储节点上，形成高能效的自适应数据分布调度策略。通过结合现代的控制理论与混合存储系统的工作特点，研究混合存储系统中以模糊控制和模型预测为基础的温度控制理论，根据模糊逻辑动态的调整温度阀值和系统运行参数的控制方法，拟合模糊因子的方法， 建立精确混合存储系统的温度模型。该研究为提高混合存储系统可靠性、数据存储安全奠定理论基础。
面向大数据保护的高效能重复数据删除存储关键技术研究	重复数据删除;协同设计;大数据保护;网络存储	重复数据删除存储通过消除数据冗余极大提升存储空间利用率，但在存储I/O性能和系统扩展能力方面存在严峻挑战，是当前网络存储研究领域的热点和难点。本课题针对数据中心海量备份数据管理的存储需求，基于应用与存储协同设计研究可有效平衡数据缩减率和系统吞吐率的高效能重复数据删除存储技术。研究思路和创新点是在海量复杂应用数据文件格式和内容分析的基础上，提出应用感知的高效数据分块策略；在此基础上，结合块索引访问模式和混合存储特征，研究高性能的重复数据删除块索引查询机制；并通过应用文件语义比对优化数据相似性开发，研究高可扩展的集群重复数据删除数据路由方法。通过本项目的研究，冀突破数据中心构建具有高存储利用率、高I/O性能和高可扩展特点的高效能重复数据删除存储系统关键技术，以满足重要行业和部门进行大数据保护的存储需求。
电力物联网中大型变压器健康状态在线监测关键技术研究	物联网;在线监测;变压器;健康状态;数据融合	随着现代电力系统向高电压、大容量方向发展，电网可靠性的要求越来越高，使用物联网进行实时在线监测成为保证电网稳定可靠运行的重要手段。本项目以电力物联网中的关键设备大型变压器系统为研究对象，研究通过使用多源感知获取状态信息和评估其健康状况的关键技术。本研究包括传感器的布设方法，通过分析变压器正常工作和典型故障情况下多物理场的分布情况，以优化传感器的布设；实时的任务调度和总线通信方法，以保证现场回传结点中任务的实时性及现场总线传输中的时间可预测性；数据级多源信息融合方法，以建立现场采集的数据间的信息关联，保持信息的时间和环境正确性；和广域关联的健康状态评估方法，以根据空间和时间方面的相关信息分析变压器所处于的健康状态。本研究为电力物联网大型关键设备的在线实时监测提供理论成果、算法和实现技术，有助于提高电网的可靠性、可维护性并降低运维成本。
基于决策图的竞争动态系统可靠性分析	可靠性分析;决策图;竞争失效;动态系统	在有功能相关的动态系统中，失效传播与失效隔离在时域上存在竞争关系，给动态系统可靠性分析带来了巨大的挑战。现有的竞争动态系统可靠性分析有两类方法：马尔可夫方法存在状态空间爆炸的问题，分解聚合方法则存在大量同构子模型重复求解的缺陷，因此它们仅能分析中小规模系统的可靠性，此外，针对存在竞争失效的多状态系统及多阶段任务系统的可靠性分析方法的研究还很不完善。本项目针对存在竞争失效行为的二态系统、多状态系统及多阶段任务系统，研究基于决策图的新型竞争失效可靠性分析模型与方法。重点研究：竞争失效扩展故障树的构建，竞争决策图模型的构建，及竞争决策图评估算法的设计。该研究将为有效地分析竞争动态系统可靠性提供解决方案，同时也将拓展决策图在动态系统可靠性分析领域中的应用，具有重要的理论和实际意义。
逻辑Petri网理论及其合成运算性质研究	协同系统;理论性质;动态性;合成运算;逻辑Petri网	为了描述与分析具有批处理功能和传值不确定性的分布式协同系统，近年来我们提出了一种逻辑Petri网方法，并将其应用于证券交易、电子商务等系统的建模与分析，获得了一些重要研究成果。在此基础上，本项目主要侧重于逻辑Petri网及其合成运算的理论研究，全面探索逻辑Petri网的理论意义和应用价值。深入研究逻辑Petri网的基本性质及行为理论，构建相应的可计算性理论，提供逻辑Petri网的性质分析方法及并发语言表达式；定义逻辑Petri网的逻辑范式，提供网语言与逻辑表达式相融合的推理方法，实现逻辑Petri网自动推理；定义逻辑Petri网的合成运算，提出相应的性质继承判据，实现复杂逻辑Petri网的有效分析和验证；针对分布协同系统中的批处理功能和传值不确定性所产生的工作流动态性，提供动态工作流的建模技术，给出相应的分析和验证方法，为动态工作流应用提供理论支持。
基于影子系统的流媒体直播平台	直播;在线;测试;性能;流媒体	互联网流媒体平台是当前国际上研究和应用的热点，涉及分布式网络系统、多媒体处理、数据挖掘等多个学科领域的交叉。获得准确的网络性能测试结果是流媒体平台研究的核心需求，在保证测试环境真实性、测试网络规模上面临巨大的挑战，需要在面向测试的系统架构设计和分布式测试控制上展开深入研究。本项目针对网络性能测试，拟基于影子系统开展在流媒体平台中直接嵌入在线测试功能的研究，实现在网络性能测试方法上的创新与突破。影子系统的基本思想是利用在线分布式系统的真实网络和真实输入数据，通过控制系统任务处理权的优先级实现在线性能测试。本项目研究如何将其基本思想映射为针对流媒体直播平台的具体方法和算法设计。本项目的研究能够为分布式网络系统的在线测试技术发展理论基础，并为下一代互联网流媒体平台积累关键技术，进而促进流媒体平台在我国的广泛应用，具有较大的理论价值和应用前景。
磁盘阵列自适应可扩展构架的研究	磁盘阵列;性能评估;可靠性;可扩展性;纠删码	随着信息存储技术的飞速发展，用户数据量的急剧增加，人们对存储系统的可扩展性也提出了越来越高的要求，特别是大型存储集群和计算中心中的磁盘阵列。然而，现有磁盘阵列的可扩展方法存在着适应性较差、不支持双向可扩展、扩展过程中的可靠性和可用性得不到保障等问题，还有部分RAID级别没有适宜的可扩展方法。为此，本项目组提出了自适应可扩展构架，该框架是一个通用支持高效可扩展的构架，具有适应范围广、支持双向扩展等特性，能够满足人们在云计算、大数据和绿色存储等方面的需求。
三维堆叠内存计算系统的可扩展性研究	性能评估;计算内存3D堆叠技术;编程模型;内存3D堆叠	随着制造工艺、体系结构和应用程序的快速发展，计算单元与存储逻辑之间的长距离数据搬移问题也日益严重。目前最为有效的缓解数据搬移问题的解决方案是基于三维堆叠的内存计算系统。但由于封装、面积和散热等问题的限制，该系统面临的主要挑战是可扩展性不够，导致难以高效地处理大规模问题。为了对该内存计算系统进行大规模扩展，需要解决来自硬件通讯、软件编程和结构评估与优化等三方面的挑战。.本项目面向这三大挑战，研究三维堆叠内存计算系统可扩展性的一系列关键技术。具体而言，项目组拟重点研究：（1）计算和存储间可扩展互连和数据通讯机制；（2）兼容现有异构并行编程接口的可扩展编程模型及其运行时系统；（3）面向内存计算系统高维空间的可扩展结构建模与优化方法。项目研究目标是形成对三维堆叠内存计算系统进行大规模横向扩展的一系列关键技术，为国产智能处理器芯片和高性能服务器芯片的研发提供有效参考。
约束条件下基于随机服务模型的网格任务协同调度策略研究	随机服务模型;网格计算;实时任务;协同调度;博弈论	协同任务调度一直是网格计算领域的基础研究内容，传统的协同任务调度策略在面对新兴网格应用时遇到了诸多瓶颈问题，其主要难点集中在，无法为资源的动态服务能力和负载压力建立可量化分析的模型和方法，从而导致协同调度策略不能充分满足上层应用的服务质量需求。本课题采用随机服务理论，并基于实际网格系统中所获取的数据来建立资源动态服务能力和负载压力的可量化分析模型。针对新兴网格应用对实时性和效用最优的约束性需求，采用量化分析模型为新型网格应用提出更高效、更具扩展性的协同任务调度策略。课题研究的资源服务能力的可量化分析模型和方法将有利于推动网格理论的纵深研究，课题研究的约束条件下的协同任务调度策略将提高网格系统在商业应用领域的服务质量。
融合网络编码的无线协作中继网络资源分配与实现机制研究	网络编码;协作中继;跨层优化;云计算;资源分配	重点研究云计算环境下融合网络编码的协作中继网络资源分配策略及以其为核心的系统级优化框架和实际系统实现机制，为构建高效实用的资源分配优化方案开辟新的研究途径。拟分别分析数字网络编码和物理层网络编码对协作中继网络资源分配的影响机理，设计能混合应用多种编码模式的网络编码模型;结合非完美信道状态信息模型，提出高鲁棒性的网络编码感知资源分配策略；分析所提资源分配策略与媒质接入控制协议和路由协议的相互影响机制，运用跨层优化方法构建以资源分配为核心的扩展性较强的系统级优化框架。基于以上理论研究结果，运用可重构通用软件无线电(USRP)平台搭建集成所提资源分配策略和优化框架的实测平台，提出基于实测数据的改进方案以得到最优化实用方案。 通过本项目的研究，可望在理论上揭示底层网络编码对协作中继网络资源分配的影响机理以及资源分配在实际系统中的优化融合机制，构建系统级的高可用性的网络编码感知的资源分配优化方案。
基于链路特征的无线传感网络重编程性能优化方法	性能优化;无线传感网络;重编程协议	无线传感网络重编程技术对于大规模传感网络应用部署、管理维护具有重要价值。无线链路的动态特征对重编程协议的性能有重要影响，已有重编程协议多针对特定网络链路状况设计，仍缺乏对链路自适应优化方法的系统研究分析。项目以无线传感网络重编程的代码分发协议的性能优化为目标，通过分析无线网络自身的链路特性对代码分发协议性能的影响，从链路不可靠性、空间相关性、低占空比等链路动态特征的角度，研究基于链路特征的重编程代码分发协议的性能优化方法，设计重编程协议的自适应优化策略，建立重编程协议性能分析评估模型，为协议设计提供理论分析方法；通过建立重编程网络测试平台进行的验证和分析改进，保证基于链路特征的代码分发协议优化策略和方法的有效性。项目研究成果将为无线传感器网络的软件更新维护提供技术支撑，对于物联网技术发展起到积极推动作用。
净室云计算理论及应用研究	可信计算;云计算;存储安全;隐私保护	云计算环境下，用户数据在远程平台存储和处理，用户失去了对数据的直接控制权，而云服务提供商作为最高权利实体，可以通过管理平台的特权接口访问用户数据，因此存在数据泄露风险。针对数据远程存储和处理引起的潜在安全风险，本项目引入"服务态"与"净室态"的全新理念，研究净室云计算理论及关键技术。提出净室云计算模型，通过定义净室态的安全边界设定可信实体的行为模式，为净室态与服务态的安全转换提供基础。研究净室态安全框架构建技术，通过对虚拟机监控器进行隐秘的测量，保证验证过程的原子性、完整性和真实性，实现可信虚拟机监控器。研究净室态安全迁移技术，通过将服务执行环境限制在安全边界以内，保证虚拟机迁移过程的完整性。研究净室态实时监控技术，通过实时检测虚拟机的动态完整性，保证虚拟机运行时的可信性，实现执行环境的安全隔离/虚拟加锁。本项研究成果对安全云计算服务的研究具有重要的意义，将有利于促进云计算技术的广泛应用。
面向不规则GPU应用的分析与优化技术研究	体系结构优化;高性能计算;性能模型;不规则GPU应用;程序改造和编译优化	近年来，GPU 架构已经被广泛的使用在高性能计算领域。GPU计算的兴起，则为GPU带来了数量繁多的通用应用，而且这些应用很多都是不规则的应用。由于控制和访存方面的多样性，输入依赖和不均衡负载等特点，不规则应用不能有效地利用硬件资源而且需要付出更多的额外开销，这就抵消了GPU架构的优势。在这样的背景下，本项目面向不规则GPU应用，开展分析和优化技术的研究。研究目标如下：针对不规则GPU应用，1) 提出统一的表征分析和提取技术，刻画其不规则性；2) 建立准确的性能预测模型并发现性能瓶颈；3) 提出程序改造和编译优化技术以提升性能； 4) 提出体系结构优化技术以提升性能；5) 建立适应高性能计算和GPU体系结构的通用函数库。本项目提出的技术可以剖析不规则应用的特点，并根据不同特点，利用程序改造、编译、体系结构的优化技术提升性能。本项目的解决方案预期可达到性能1~2个数量级的提升。
嵌入式多核环境中分区操作系统关键技术研究	多核环境;分区操作系统;安全关键;拓扑管理;虚拟化技术	本项目基于嵌入式多核系统通过虚拟化技术构建分区操作系统原型。虚拟化技术将物理资源抽象为逻辑表示，分区操作系统则对物理资源进行逻辑分配，实现时间和空间的隔离。当前嵌入式多核系统面对的应用需求日益多样，需要通过软件手段来发挥硬件资源的能力以解决不同需求的矛盾。本项目采用寄宿型Hypervisor对嵌入式多核硬件资源进行抽象和拓扑管理，以控制多核环境引入的硬件复杂性；采用半虚拟化实现降低应用对硬件的依赖性，从而增强可移植性。Hypervisor根据应用需求类型划分分区，实现无锁的分区通信机制，并参照ARINC 653标准分配时间和空间资源，基于周期和优先级对分区和任务进行调度，保证实时任务等关键服务的安全可靠以及非关键应用需求得到满足。采用有限状态机和形式化方法对系统模型进行理论抽象和验证，保证系统的正确性和可用性。本项目提出的系统原型和技术方法将为嵌入式多核环境系统软件研发提供理论和技术基础。
面向新型异构众核系统的多设备协同并行计算关键技术研究	并行编程;异构并行计算;并行算法;异构并行系统;众核加速器	由多核CPU和众核加速器构成的新型异构众核系统以其高性能、低能耗和低成本等优势在高性能计算领域得到迅猛发展，基于异构众核系统开发的并行应用也日益增多。然而，简单易用的异构协同并行编程模型的缺乏以及异构众核系统复杂的体系结构严重制约了基于异构平台的并行应用的开发效率和执行效率。项目首先研究基于指导语句的异构协同并行编程框架，以降低异构协同并行编程难度、减轻编程负担并有效支持基于异构平台的数据级并行应用的多设备协同并行计算；然后，研究如何在异构平台中利用多个计算设备合理高效地协同执行数据级并行应用，提出抢占式动态弹性任务调度策略；最后，鉴于设备间通信易成为异构协同并行计算的性能瓶颈，提出基于软件流水线三路重叠式通信优化方法，以有效隐藏设备间通信开销。本研究将为面向异构众核系统的多设备协同并行计算提供简易编程框架、高效任务调度和通信优化支撑，为提高异构众核系统的易用性和计算性能提供一种可行方法。
片上网络系统的拓扑和路由关健问题研究	交换技术;片上网络;路由算法;拓扑结构	随着大规模集成电路技术的发展，研制具有多个内核的系统芯片(system-on-chip, SOC)以构造高性能，低功耗计算机系统已成为主要趋势。设计系统芯片的关键是设计其中的高性能"片上网 络"（network-on-a-chip, NOC），使内核与内核之间能以在超高带宽进行通信，并且能够在芯片内部以每秒万亿位的速度传输数据。本项目主要研究片上网络架构抽象级上拓扑和路由中若干基础和关键问题。主要研究课题包括面向各类应用的片上网络的最合适拓扑结构选择和相应的资源配置方法；可支持资源预定的新型虫洞路(wpormhole routing)由交换技术；以及为各种片上网络的拓扑结构设计的无死锁，可支持多点通讯的路由算法和路由器。
可配置软件无线电数字信号处理器体系结构研究	NoC;数字信号处理器;可配置;软件无线电	本课题针对当前采用软件无线电技术实现的通信系统对超高性能数字信号处理系统（>100GOPS）的迫切需求，从分析下一代无线通信系统核心算法的计算复杂度、访存需求、存储需求、通信方式等程序行为特征入手，提出并研究可配置软件无线电数字信号处理器的体系结构，研究单核处理单元的多模式流水线结构、支持高效预取的非Cache存储结构、延时精确的TDM异构NoC互连结构，并重点研究处理器各层面的可扩展性和相关配置手段，使处理器能很好地适用于不同标准通信系统的算法处理。.本课题的主要的创新点有：.1）BER指导的无线通信系统的程序行为特征分析；.2）高效的标量向量混合执行流水线结构；.3）支持特殊数据预取的非Cache存储系统；.4）TDM工作模式的NoC互连结构。
流处理器大规模网络环境下协同并行处理模型研究	流处理器;大规模网络计算环境;协同并行处理	利用现有的计算机网络和基于GPU（流处理器）的计算节点，构建大规模、可扩展的网络并行计算环境，针对流处理器的特殊结构以及它和协同网络的性能匹配关系，研究该大规模网络并行计算环境下的并行协同处理（计算）模型及其程序设计问题；包括：基于GPU存储器、计算机内存储器和网络分布式存储器的全局协同知识表示模型和并发模型；支持协同并行计算的局部和远程通信模型；协同网络环境管理模型；最后扩展CUDA以便使其支持大规模网络环境下的并行协同处理。目标是使该协同并行处理模型具有近线性的可扩展性，通过多级分组协同机制，其可支持大规模网络环境下的并行协同处理工作。该项目的研究成果，对利用现有的分布在各地的流处理器计算资源，构建高可靠的、低成本的、自主知识产权的、大众化的、高性能网络并行协同处理装置有着基础理论支撑作用，对我国经济建设的发展将有重要的经济价值和社会意义。其可以应用到大规模网络环境下海量信息处理问题。
高性能拓扑感知应用层组播模型构建及性能优化问题的研究	网关级拓扑发现;网络模型;性能优化;拓扑感知应用层组播;负载均衡	应用层组播在部署、可定制性等方面有着IP组播无法比拟的优势，但其传输效率却逊于IP组播。拓扑感知应用层组播由于采用事先探测端节点的拓扑信息方法，充分利用所获取的底层网络拓扑信息来构造覆盖网络，使覆盖网络组播树可以尽可能地与真实网络吻合，减小了因覆盖网络与真实网络不符而导致的最短路径计算误差，从而获得较高的传输效率，成为目前应用层组播研究的一个热点。但是目前这种基于拓扑感知的应用层组播还未很好地应用于实际网络中，主要原因是拓扑感知应用层组播在拓扑发现、覆盖网络构造及转发、组播生成树负载均衡等方面还存在很多基础性研究问题需要解决。本项目拟主要针对上述基础性问题展开创新性研究，着重探究高性能拓扑感知应用层组播网络模型的构建及其优化问题，使在理想环境下能够获得良好性能的拓扑感知应用层组播方案，在真实网络环境下也能为用户提供较高的服务质量，为高性能应用层组播能够尽快应用于真实网络打下坚实的理论基础。
CPU Cache的功耗驱动设计方法及工具研究	功耗驱动;高速缓存;快速蒙特卡罗仿真;量子修正功耗模型	随着高性能处理器及多核处理器的普遍应用，高速缓存（Cache）技术变得越来越重要，高速缓存的功耗问题研究的也越来越多。本课题针对纳米尺度大规模高速缓存技术的架构和电路特点，提出了功耗驱动（Power Driven）的设计方法，并建立相应的工具软件及设计流程：根据纳米尺度电路及器件的特点，建立器件电路的量子修正功耗模型；研究自适应重点采样方法，建立快速蒙特卡罗（Fast Monte Carlo）仿真的计算模型和计算方法，并完成相应软件工具；应用此工具，对全Cache进行计算，找出电路的功耗约束条件；通过调整功耗关键路径MOSFET尺寸及相关电路设计，完成全Cache的功耗优化设计。进而探索在保证Cache性能和良率的情况下，达到功耗最优的目标。在方法学研究的基础上，试验设计一款大规模CPU Cache, 优化其动态及静态功耗，通过测试片验证方式，比较文献及实验数据，分析功耗驱动设计方法效果。
基于图原语线性代数表达的并行非规则归约计算	图算法;并行算法;非规则归约;线性代数;大数据	开发高效的非规则应用并行程序是计算流体力学、粒子模拟等许多传统科学与工程计算领域的难点问题。随着新兴大数据领域的发展，在数据挖掘、社会网络分析等研究中也涌现出来许多非规则应用。由于具有不可预料的数据访问模式，这类计算的空间与时间局部性差，不能很好地匹配当前处理器的体系结构特征，使得这类应用难以获得高性能。许多非规则应用的核心是非规则归约问题，它是一组与非规则内存访问相关的归约操作，且是影响非规则应用的关键性能因素。因此，研究高效的并行非规则归约算法和技术，从而支撑多个领域的非规则应用的高效能开发具有重要的科学意义。.        本项目面向新型硬件体系结构，研究基于图原语线性代数表达的并行非规则归约算法和优化技术，进而研制相应的并行非规则归约计算框架，支撑传统科学与工程计算以及大数据领域的若干非规则应用的快速开发和大规模计算模拟，具有很强的理论和实际应用价值。
分布式流数据近似计算在线质量保证机制与策略研究	分布式流处理;流数据;误差分析;质量保证;近似计算	近年来，面向流数据提供实时在线处理能力的流式计算成为大数据研究和应用的热点。随着数据中心和云计算成为大数据处理的主流模式，分布式流数据处理模型(DSPS)得到广泛关注。然而数据的指数级增长仍为流处理技术带来了极大挑战，因此近似计算作为减少资源消耗、提高处理效率、满足实时性需求的方法，将成为流数据处理中不可或缺的关键技术。但流数据近似计算的相关研究工作目前还很不充分。申请课题将面向流数据实时近似处理相关技术，研究分布式流数据处理系统中关键环节的误差产生、传播和关联模型；研究输入/输出数据和近似计算过程的质量评价方法和质量监控机制；研究可定制的质量保证策略等。本项研究将为流数据近似计算技术在数据中心和云计算平台的深入应用，提供理论参考模型和实验验证，具有重要的理论和应用价值。
基于异构平台的高复杂度生物序列分析算法并行化研究	细粒度并行;异构体系结构;高性能计算;可重构计算;算法加速器	生物信息学属于多学科前沿交叉科学，应用广泛，意义重大。生物序列分析是生物信息学乃至现代生命科学领域重要的基础性研究工作，该领域的应用具有程序特征多样化、数据相关多维度、访存行为不规则等特点。通用结构计算机虽然能够提供很强的峰值计算能力，但是不能完全适应该领域复杂计算特性的特殊要求，计算效率不高。.课题以序列分析领域中的复杂结构预测和分析算法对高性能计算的需求为背景，基于通用微处理器结合硬件算法加速器（FPGA和GPU）的异构体系结构，从提取典型方法的动态计算特征入手，研究对复杂数据相关和不规则存储访问的优化方法，对典型算法实现细粒度并行，达到高效加速计算的目的；并在此基础上提取普适的并行优化方法，为特定领域的算法群提供一种基本的硬件结构模板和并行程序设计框架，为有效降低算法加速器设计复杂度、实现加速器快速生成奠定基础，为提高对生物信息的处理能力提供技术参考。
面向人脸检测的大规模异构并行Adaboost机器学习算法研究	并行化;人脸检测;机器学习;负载均衡	本项目以人脸检测为应用背景，将Adaboost机器学习算法针对大规模异构并行系统进行并行化。经过对Adaboost机器学习算法的深入分析，本项目拟从四个粒度开发Adaboost机器学习算法的并行性，从而与大规模异构并行计算系统丰富的并行计算资源相匹配。通过研究支持闭环反馈的动态任务划分算法、异构并行Adaboost算法实时性能监测技术以及异构并行Adaboost算法性能分析模型，不断对异构并行Adaboost算法进行优化，充分利用大规模异构并行计算系统强大的计算能力加速Adaboost机器学习过程。
基于非易失内存设备的数据读写性能优化方法研究	动态随机存取内存;存储级内存;存储性能	传统计算机体系结构下，非易失数据存放在诸如磁盘等访问速度较慢的外存设备之中，不仅引起了应用程序与非易失数据之间的交互性能瓶颈，也限制了其潜在的性能优化空间。通过借助新型内存、也即存储级内存（SCM, Storage Class Memory）的非易失特性和高效写入性，本课题拟在基于动态随机存取内存（DRAM, Dynamic Random Access Memory）和SCM的混合内存架构下，提出一套非易失数据读写性能优化技术，以突破传统方法的性能瓶颈，具体工作是：（1）研究具备可继承性的SCM设备编程环境，实现应用程序访问SCM设备的高效数据通道，同时保留用户以往对非易失数据的编程习惯；（2）研究SCM内存的资源共享策略，缓解由于多进程对SCM资源竞争所带来的读写性能干扰；（3）研究混合存储架构下的非易失数据迁移策略，进一步改进应用程序与非易失数据交互的时空效率。
相变内存耐久性研究	相变内存;多核;耐久性;恶意攻击;理论模型	内存系统已成为计算机系统发展的瓶颈，研制高速、大容量、低功耗的新型存储系统是计算机系统设计的一个重要挑战。相变内存的出现为该问题提供了一种解决方案，但是相变内存写次数受限的问题限制了它的使用寿命，严重阻碍了它的实用化。本项目以提高相变内存使用寿命为目标，首先拟建立相变内存使用寿命的理论模型，其次基于理论模型分析抗磨损机理，最后根据抗磨损机理从多核处理器体系结构设计的角度进行优化，提高相变内存使用寿命。本项目拟提出延迟替换频繁写数据的Cache替换策略、基于目的节点脏数据量评估的复制和迁移技术，通过优化多核处理器架构减少对相变内存的写操作；拟提出一种基于数据预取技术的恶意攻击检测机制，利用数据预取技术中重复Cache缺失预测机制来检测恶意攻击程序，避免相变内存受到频繁写攻击。本项目研究对于提高基于相变内存的计算机系统使用寿命和性能具有重要的理论和实践意义。
云计算平台中大规模交互式服务长尾延迟消减关键技术研究	长尾延迟;云计算;组件;交互式服务;性能干扰	大规模交互式服务通常将请求切分到多个组件上并行执行，因此请求延迟取决于组件长尾延迟（即最慢组件的延迟）。在云计算平台中，组件与并发批处理作业共享和竞争节点资源如高速缓存和I/O带宽而受到性能干扰，是造成组件性能差异性和变化性及高长尾延迟的主要因素。随着服务规模与复杂度的增加，高长尾延迟已成为制约其性能和收益提高的瓶颈，是云平台应用管理的关键技术难题。本课题将围绕云计算混合负载运行环境下大规模交互式服务长尾延迟问题，从性能干扰定量刻画和预测、组件层次细粒度延迟消减技术、及面向重点领域应用长尾延迟消减验证原型系统三个方面，开展系统的研究工作；重点研究长尾延迟消减的可预测性、精确控制性、高可用性三个科学问题。课题研究成果将有助于解决云平台中大规模分布式系统管理的关键技术挑战，提升我国云计算自主创新能力，并促进我国云计算快速发展。
面向嵌入式系统的STT-RAM/SRAM混合Cache的优化技术研究	低功耗高性能优化;非易失性存储器;数据流模型;混合Cache;自旋矩传输磁存储器	新型非易失性存储器STT-RAM具有静态功耗低、尺寸小、非易失性、抗电磁辐射等突出优势，是替代SRAM用作片上Cache的最优选择。但是STT-RAM的写操作存在动态功耗高和访问延迟大的问题，严重阻碍了其直接替代SRAM的进程。采用STT-RAM和SRAM混合的Cache结构是有效解决STT-RAM写操作问题的主要技术手段之一。本项目针对嵌入式系统在功耗和实时性方面的需求，研究该混合结构的功耗和性能优化技术。本项目：1）将结合STT-RAM读写非对称的特点改进现有的数据流模型；2）将提取嵌入式系统中软件控制Cache、Cache锁定和VLIW等结构特征；3）在上述典型的结构框架下，将对程序的数据流模型与系统的功耗和执行时间进行量化关联分析。依据量化分析结论，将提出一系列数据分配和指令调度策略，实现混合Cache的功耗和性能优化，为STT-RAM在嵌入式系统中的应用提供有效的解决方案。
基于异构众核平台的三维水声传播高效并行算法研究	水声传播;并行算法;异构众核;三维模型;性能优化	复杂海洋环境下的三维水声传播计算难以为实时应用提供快速的声场数据支持，并行计算是提高其计算效率的有力手段。本项目以声纳探测效能预测、水声信号处理和水声环境仿真等水声应用问题为背景，结合国产高性能异构众核计算平台的体系结构和编程模型特点，采用理论分析、算法设计与数值试验相结合的方法，着力发展考虑辐角耦合影响的三维水声传播多层次可扩展众核并行算法、众核并行性能优化、异构众核协同并行等核心技术，重点解决三维水声传播模型计算速度慢、内存需求大的问题，大幅提升复杂海洋环境下的三维水声传播的计算效率，为目标检测、匹配场定位、宽带声传播、声场反演等问题的深入研究提供技术支撑。
大规模定制服务系统的Petri网语义模型与关键技术研究	服务;建模;语义;大规模定制;Petri网	在满足用户个性化需求的同时而又不牺牲效益和成本，达到规模效应，提供大规模定制服务是SOC理论和技术面临的新挑战。针对上述挑战，面向大规模定制服务系统的设计与实施，本项目围绕"多态"服务的语义和表达、系统的重组与优化两个关键科学问题，从形式化方法入手，构建基于可重构Petri网的系统语义模型，把握大规模定制服务系统的内在机理；进而研究支持大规模定制的"多态"服务形式语义，给出其过程语义和连接语义，制定服务描述规范，实现大规模定制服务的精准表达；在此基础上，提供基于Petri网语义模型和形式语义的系统构造、装配和优选技术，指导大规模定制服务系统的规范设计，支持大规模定制服务系统的动态组合、快速重组和个性服务；最后研制大规模定制交通信息服务应用验证系统。项目的研究，将为大规模定制服务系统的设计和实施提供理论保障，丰富和发展服务计算科学理论，为网络环境下的信息服务更好应用提供技术支撑。
煤矿井下物联网感知层感控异构融合理论与技术基础研究	煤矿井下;嵌入式系统;物联网;感知层	通过对煤矿井下现有各监控子系统及其关联性质的分析、解耦，异构有线和无线网络，以构建集智慧感知、智能决策、自动控制于一体的具有煤矿特色的物联网感知层系统为目的，优化物联网感知层解析理论，建立煤矿井下感知层表示模型和多模态异构信息的融合计算模型；采用三维空间对称斜三角形分区覆盖、分层不均匀折线型巷道无线通讯拓扑和动态流量调整思想，建立复杂环境下感知层无线感测网络的无缝融合和多源信息的动态主动获取及高效传输机制，实现煤矿井下各类感控信息在物联网感知层的智能汇聚；设计煤矿井下物联网感知层系统优化方案，科学设置各类感测与控制信息的处理时间和保真度，探索煤矿井下感知层感控分层和控制分级处理机制，采取硬判决策略，保证关键信息及时、可靠地传输与控制；进而在数值仿真、实验、理论分析和平台验证基础上，揭示煤矿井下物联网感知层感控异构融合机制，为物联网大规模在煤矿井下应用提供理论支持。
异构多核体系结构的能效优化关键技术研究	多核处理器;运行时;性能优化;异构;功耗模型	与同构架构相比，异构架构中集成的计算单元数量更多、类型更丰富，计算任务能够被更加合理地分配到适合的计算设备上，获得更好的性能或性能功耗比。然而，现有的异构多核体系结构还不能很好地支持能效优化，如没有为能效优化提供必要的软硬件支持、一些关键模块或机制对能效优化的影响还不清楚，等等。为此，本项目将深入、系统地研究异构多核体系结构的能效优化关键技术，包括：异构多核体系结构的能效模型、异构多核结构中CPU与GPU的协同能效优化、面向能效优化的异构平台任务分配策略、异构体系结构中的数据通信机制及其对能效优化的影响。本项目的研究成果既能够直接用于实现高效能的单个计算节点，也可以用于指导未来高效能异构多核体系结构设计。
超高密度磁记录用分立存储介质中软磁/硬磁交换耦合的复合记录单元的结构设计及性能研究	软磁/硬磁交换耦合;软磁层覆盖方式;分立存储介质	分立存储介质（Bit Patterned Media）是磁记录领域的一个新的发展方向，由于其具有热稳定性好，信噪比高等优点受到了研究者们的广泛关注。然而随着记录密度的提高，分离存储介质也面临着如何在保持热稳定性的同时降低矫顽力的问题。将软磁/硬磁交换耦合这一概念引入分立介质中是解决这一问题的一种简单有效的方法。通常在软磁/硬磁交换耦合复合介质的研究中人们只重视软磁部分的体积和饱和磁化强度，而对于软磁部分的各向异性带来的影响没有深入研究。本项目中我们提出通过改变软磁层对硬磁部分的覆盖方式，即软磁层的形状来改变软磁层的磁各向异性能，并研究其对于记录介质整体性能的影响。同时对于软磁层的材料、厚度、用于调节交换耦合强度的中间层的厚度带来的影响均进行系统而深入的研究，期望能够寻找到适合分离存储介质的最佳覆盖方式以及与之相匹配的软磁层材料、厚度、和中间层厚度。
众核处理器中能效驱动的存储和互连网络系统研究	存储;众核处理器;互连;能效;网络	功耗已成为制约高性能计算系统性能提升的主要壁垒，提高微处理器的能效是设计高性能计算系统的关键。随着芯片规模增大，数据在片上存储和互连网络中的移动能耗显著增加，成为众核处理器设计遇到的新障碍。本项目面向下一代64~128核心的众核处理器，提出一种能效驱动的紧耦合可扩展片上存储和互连网络体系结构（ecsMNA），实现处理器核与存储和互连系统的平衡、协同数据移动的高能效。主要研究内容有：（1）有限能量预算的CPU芯片总体架构，如何对计算、存储和互连网络资源进行最优分配。（2）ecsMNA总体结构及关键技术，包括能效驱动的片上数据移动机制、数据局部化优化策略、可扩展的多级互连网络等。（3）性能和功耗模拟器设计，构造整合片上存储、网络和一致性协议的性能和功耗模型，支持多指标模拟和自动优化。本项目的研究成果将为面向高性能计算系统的下一代CPU的研制做准备，对提高国产CPU体系结构设计水平具有重要意义。
资源虚拟化环境中面向I/O密集型负载的能效优化策略研究	能效优化;云计算;虚拟化技术;性能计数器;功耗模型	随着云计算概念的推广，虚拟化技术为数据中心的能效优化提供了若干新的解决思路。但近期研究显示，在执行I/O密集型负载时，虚拟化平台的各类能效指标将显著降低，其主要原因在于：（1）I/O密集与CPU密集型任务共存时，虚拟机调度器难以兼顾效率和公平；（2）大量"中间数据"的I/O访问与管理显著增加了虚拟化平台的能耗开销。针对以上问题，本课题首先以研究虚拟机功耗模型为起点，提出采用"性能计数器比例模型"来描述虚拟机的功耗状态，以此提高虚拟机功耗度量的准确性；然后，通过高精度功耗模型来量化分析I/O密集型负载对资源虚拟层能效的负面影响，并研究如何通过"I/O补偿机制"来优化虚拟机调度器的能效表现；最后，研究如何通过两阶段虚拟机部署策略来降低"中间数据"所导致的I/O能耗开销。本课题的研究结论和相关理论将有望应用于采用虚拟化技术部署资源的云平台，以期降低其数据中心的能耗成本。
基于异构体系结构的稀疏矩阵分解算法并行化研究	异构体系结构;高性能计算;可重构计算;稀疏矩阵分解	稀疏矩阵分解是科学与工程计算领域求解大规模稀疏线性方程组的核心算法，也是求解过程中最耗时的部分。近年来，一系列稀疏矩阵算法通过异构体系结构平台获得了显著的加速比，然而，由于任务间大量数据依赖关系以及访存的不规则，面向异构体系结构的稀疏矩阵分解算法研究存在计算效率低、并行性能低等问题。本项目以稀疏矩阵分解为研究对象，从并行算法设计和体系结构设计两方面出发，在研究CPU-GPU和CPU-MIC两类通用异构体系结构计算平台上的算法并行化设计的基础上，设计并实现基于FPGA的可重构算法加速器，进而构建包含GPU、MIC和FPGA三种不同特性的算法加速器的混合异构体系结构计算平台，实现不同计算单元的体系结构特征与稀疏矩阵分解不规则计算特征的适配，从而有效提高稀疏矩阵分解算法的计算性能和适应性。
多维时序模式指导下的软件保护评估方法研究	网;时序模式;Petri;手机软件保护评估;软件保护;软件度量	为了确保手机软件安全运行，软件保护是应对攻击者恶意分析的重要手段。在这一背景下，准确全面地评估目标软件的保护性能对于提升手机整体安全水平具有重要意义。为了解决现有评估方法对手机软件特殊性考虑的不足，本申请将动态时序模式和多维软件度量结合引入手机软件保护评估中，提出一种多维时序模式指导下的软件保护评估方法。该方法改进并使用局部线性嵌入方法对目标软件的多维度量数据进行降维分析，接着对其结果采用时序模式和多维聚类相结合的方法来分析手机软件在持续更新过程中软件保护性能上的变化规律，在此基础上利用Petri 网模型对攻击过程进行建模和分析，同时结合成本效益分析，进而获得准确全面的手机软件保护评估效果。一方面，本方法充分考虑了手机软件持续更新在软件保护上的影响以提高评估的准确性；另一方面，本方法的评估结果体现了保护效果在软件持续更新中的变化规律，可以用于发现安全薄弱环节并支持预测，丰富评估的全面性。
基于"余数系统"的DSP系统高效数值缩放方法研究	余数系统;低功耗;余数基;符合检测;高效缩放技术	随着通信、信息处理、多媒体技术和网络技术的发展，对数字信号处理VLSI实现的要求日益提高。利用"余数系统"的并行数值表征及计算，即利用并行的独立的简单运算代替传统的单次复杂运算，使运算复杂度和功耗上得到简化和降低，从而使"余数系统"成为解决数字信号处理过程中高速、大动态范围与低功耗、低复杂度矛盾的有效途径之一。本课题将以基于RNS的DSP系统所面临的基本问题- - 数据缩放问题为切入点，以余数系统高效数值缩放方法、适合构建DSP系统的余数基构造方法、余数系统符号检测方法、余数基动态范围扩展方法等作为核心研究问题，讨论其算法基础、构造理论和实现方法。本项目研究的创新点在于为余数系统的缩放、基的选择和符合检测提供高效、易于实现的方法。通过本项目研究工作将为低功耗、低复杂度的数字信号处理芯片设计提供新方法和技术准备。
基于光线追踪机制的三维集成图形处理器体系结构研究	光线追踪;体系结构;三维集成;众核;图形处理器	图形显示是人机交互的核心手段，目前几乎所有个人计算机、平板电脑和智能手机都装备图形处理单元(Graphics Processing Unit, GPU)来实现显示功能。光栅化和光线追踪是实现图形显示的两类主要算法，前者被主流商业GPU所广泛使用，而后者允许高度逼真的光影效果、并在高度复杂场景下具有较低的算法复杂度。由于人类对于显示质量的要求日益提高和显示场景复杂度快速提升，光线追踪算法在满足新一代图形应用方面具有巨大潜力，有望成为下一代主流图形显示方法。本课题在深入分析光线追踪算法的基础上，探索基于光线追踪算法的众核图形处理器体系结构(微架构，microarchitecture)。通过发展具备良好扩展性的众核体系结构和兼顾不同计算模式的组织与调度方式、并利用三维集成电路消除传统GPU体系结构的性能瓶颈，本课题的设计目标为在1280x1024分辨率下实现接近实时性能(～15帧/秒)的显示速率
基于全数据的云存储系统实时性能建模理论及方法研究	性能预测;系统建模;性能优化	云存储是IaaS的重要组成部分，是大量云计算应用的基础。这些应用需要低成本、高性能且稳定的数据存储服务。由于云存储系统是一个多用户、分布式的大规模动态系统，现有的评测及建模方法不适用于云存储系统的性能分析。传统建模方式仅能反映系统的统计规律，无法获得真实系统的特性。随着大数据相关研究的发展，云存储系统中数据的采集、存储和处理都有了突破。对复杂系统进行全面数据采集并建模已成为一种新的建模途径。本项目拟提出一种基于全数据的性能预测模型，该模型对云存储系统进行持续数据采集，结合云存储服务性能指标，分析并预测云存储系统级的性能和瓶颈。研究内容包括:1.利用集成的全数据建立自适应的反馈性能分析及预测模型。2.数据缺失和采集延迟情况下，劣质数据的检测与修复方法，以及延时数据的利用方法。
片上网络演算模型及性能分析研究	网络演算;系统建模;延迟上界;性能分析;片上网络	随着集成电路技术的发展，片上网络（Networks-on-Chip，NoC）替代传统的总线或点到点互连成为多核芯片内新的通信架构。性能分析是片上网络一个重要的研究方向，对于构建性能可预测的系统、提供端到端的QoS保证和加速NoC设计空间搜索意义重大。本课题基于网络演算对片上网络进行系统建模和性能分析，重点推导业务流的端到端延迟上界，研究提供尽力服务的分组交换NoC中，网络冲突、拓扑结构、流量控制、交换策略及缓冲区大小对通信性能的影响。提出NoC中多业务流竞争网络资源的冲突树演算模型，NoC二维和三维拓扑通信性能的对比分析模型，NoC基于信约的链路级流量控制演算模型，NoC虚通道虫孔交换演算模型。本课题为NoC建立一套完备的确定性性能分析方法，为网络演算这一新兴的数学理论开辟一个新的应用领域，具有很高的研究价值和实际意义。
异构系统上基于任务窃取的负载平衡研究	任务窃取;负载平衡;异构系统	当今世界主流的超级计算系统往往不是单一结构的，很多都配有GPU等加速器。在大规模计算当中，负载平衡对计算的性能、功耗至关重要。而传统的任务调度方法往往很难直接用于异构系统，使之实现负载平衡。 本项目将基于广泛使用的任务窃取方法，针对能够充分利用GPU等加速器的程序进行负载平衡研究。这类程序包括一些化学计算方法（TCE、SCF等）以及n-body问题等。主要研究内容包括：（1）改进任务窃取方案，通过使用不同大小任务、不同处理器任务队列调配等方法，优化窃取机制。（2）对加速器上的任务进行流水化和批处理，以充分利用计算资源，更好地实现负载平衡。（3）根据数据局部性对任务调度进行优化，使共享数据的访问更加高效。本项目的研究充分挖掘异构系统的计算潜力，为未来E级系统上的负载平衡研究提供基础。
基于系统层次结构的大图并行处理框架研究	图划分;图结构;社交网络分析;大图处理	随着社交网络的兴起，基于大规模图结构上的计算、分析与挖掘，成为具有重要价值的研究热点。数据的持续增加和关联关系的日益复杂，对并行系统结构设计和图划分算法的设计都提出了挑战。本项目拟针对大规模图并行处理中高性能、高效率和易编程的需求，通过将大图划分算法与系统层次化结构有机结合，提出一种新颖的大图处理框架。该框架利用分布式系统中各层级的结构特性，建立划分代价分析模型，设计适应各自层级需求的高效划分方法。在此基础上，建立统一的层次化大图划分框架。为了进一步提升效率，通过分析大图数据存储与访问的模式，优化数据分布、数据复制、节点通信、I/O访问和任务调度等机制。针对自然图及动态图等多种类型图结构，提出自适应、自优化的处理机制。最后，构建开源框架和原型系统，并在社交网络及电信网络等应用领域中进行评估验证。
面向智能环境的软件定义智能的研究及其在老人看护上的应用	智能环境;转移学习;软件定义智能;上下文感知;活动识别	研究提升智能环境部署效率所需解决的智能层模型设计、基于软件定义计算及上下文感知算法研究、迁移学习等相关理论及技术。针对智能环境中的嵌入式节点智能判定、智能决策所涉及到的数据交换、计算、推理操作，构建能够重构的智能层，完成能承载以人为中心的智能层的理论抽象，同时包含了面向脚本配置的上下文感知算法的研究，以支撑普适计算基于用户活动场景、环境当前状态来对未来决策进行判定的需要。通过面向智能需求的任务分解，采用分布式RETE网络分配方法，从而实现最小代价的任务分配。以能有效完成已有活动模式知识迁移到新环境的方法解决智能环境中活动模式知识学习和挖掘开销大、操作效率低的问题。基于已嵌入智能层中以脚本驱动的通讯、数据处理、计算、推理引擎，最终实现智能的软件定义。将以老人看护为目标，在老人公寓实现该智能环境，完成上述方案的实际测试验证，为智能环境能够高效率、低成本部署奠定理论和关键技术的基础。
大规模可持续传感器网络的能量同步研究	斯坦科尔伯格博弈;拓扑控制;分布式能量同步;可持续传感器网络;线性规划	本项目针对可持续传感器网络的能效优化问题，提出一种分布式、层次化的网络能量同步机制。1）研究基于无线能量传输技术的传感器网络能量模型，设计极小开销的分布式拓扑控制机制和层次化的网络能量同步架构，实现大规模网络的分散化、区域化。2）将执行器的路径规划转换为空间域的线性规划问题，采用多项式时间近似算法求解，得到其最优巡航轨迹，实现网络区域间的能量同步。3）研究具有能量供应能力的无线传感器网络的充电特性，设计单主多从斯坦科尔伯格博弈的数据交互及无线能量传输算法，保证局部网络中节点间能量的动态同步。通过本项目的研究，将为实现大规模无线传感器网络的可持续工作提出一种有效的分布式多目标联合优化方法，得到强可扩展的分布式计算架构，对基于无线能量传输的可持续传感器网络技术的发展起到一定的促进作用。
基于扩展控制数据流图的异构通用计算建模与性能评估	评估;建模;扩展控制数据流图;异构计算	随着移动互联网应用与服务的演进，巨大的计算需求开始释放。传统面向低延时的处理器设计已经无法满足应用对处理器计算性能的需求。CPU+计算阵列的异构通用计算模型通过对应用中控制密集型和数据密集型计算的切割与映射，可以提供50倍以上的加速比。然而目前异构通用计算由于缺乏统一的硬件高层模型与软件编程模型，难以开展大规模的设计空间探索。本研究拟通过改造传统的控制数据流图，加入异构计算引擎微结构信息，从而获得扩展了微架构信息的控制数据流图（ECDFG）。然后借助GPGPU方式对该图进行切割和优化，间接实现对异构通用计算资源的分配与设计空间探索，最终得到优化后的系统硬件微结构，从而完成面向某一类（或某几类）应用领域的软硬件协同设计与性能评估。
海量存储系统的可用性模型及度量算法研究	可用性度量;可用性模型;海量存储系统	随着"云存储"及在线应用服务为主的大数据时代到来，对于海量存储系统中未预料的故障或停机等造成的损失及服务的有效性越来越受到重视，使得海量存储系统可用性研究成为了新的热点,如何评价海量存储系统的可用性即海量存储系统可用性度量问题又成为一个新的研究课题。本课题通过对海量存储系统的可用性模型的研究，建立海量存储系统可用性度量的理论架构。通过海量存储系统软件及硬件故障模型、故障分布、系统可维修度分布，进行可用性度量的研究，建立新的海量存储系统故障传播模型；结合实际海量存储系统运行日志及经典海量存储系统日志，研究海量存储系统部件故障覆盖率模型和故障分布模型；研究系统维护优化模型，提出新的系统维护策略机制；研究高可用海量存储系统在稳态及降级状态的系统性能影响度，提出平衡优化海量存储系统可用性及高性能之间的平衡配置策略，为在海量存储系统研制全过程及系统验收运行中,保障系统的持续服务能力提供理论技术支撑。
基于固态硬盘的虚存cache管理技术研究	预读;固态磁盘;虚存管理;写回;热数据跟踪	存储墙问题是高性能计算研究的核心技术问题，固态硬盘作为一种新型存储器，具有访问速度快，容量大，功耗低等优点，被认为是解决存储墙问题的一个有效途径而得到广泛应用。然而，固态硬盘也有读写开销不对称，写放大等缺点，其性能的发挥依赖于有效的固件和操作系统调度算法的性能。目前国际国内的相关研究，主要面向固件层的调度算法，在操作系统层面，尤其面向高性能计算应用的研究还不够，算法的性能还不够高，还有很多待研究的问题。因此，基于固态硬盘加速的虚存cache管理技术研究，具有重要的理论和实际应用价值。基于此，我们拟在如下方面开展研究：(1) 面向固态硬盘加速的热数据跟踪，能够为调度算法提供更充分的调度信息；(2) 基于固态硬盘的虚存管理优化，对数据预读、写回进行优化，能够捕获更多的数据复用，提高应用性能；(3) 面向分布式文件系统基于固态硬盘加速的优化，能够提高数据写、元数据服务等的性能。
大数据环境下基于新型存储设备的I/O系统数据布局优化研究	磁盘存储系统;集群存储;混合存储系统;数据布局;大规模存储系统	基于新型存储设备的I/O系统是解决大数据访问的一种有效方案。然而，面对大数据环境下"异构存储设备"和"复杂I/O访问模式"等特征，目前数据布局方法面临着严重挑战。本项目从基本理论和实现方法两个角度研究数据布局的四个基础科学问题：研究并行I/O数据访问成本模型的准确性，确保其在异构存储设备、不同访问模式、多种数据布局下能很好评价数据访问性能；研究分层结构下cache系统数据管理的高效性，提出选择性cache数据进入策略和布局感知的cache数据放置策略，提高cache系统访问性能；研究平面结构下数据布局方法的适应性，提出基于线性优化的分条大小优化方法和区间级分条大小优化算法，以满足多种异构服务器和复杂访问模式的需要；研究I/O体系结构的动态自适应性，通过将分层存储和平面存储两种结构结合起来，进一步优化系统资源配置。本项目对促进大数据应用的发展和高性价比大数据平台构建具有重要价值。
针对多核系统存储层次增强数据并行性能的软件支撑技术研究	多核系统;数据局部性;LLC;存储层次;数据并行编程模型	SMP、NUMA结构的多核系统复杂的存储层次特征严重影响着并行应用的性能，其症结在于如何充分发挥LLC和Memory效率，而以数据局部性为中心的软件优化方法成为研究趋势。本项目针对数据中心最有代表性的数据并行编程模型OpenMP和MapReduce，从中挖掘数据局部性特征，并结合多核系统存储层次特征，以提高LLC、Memory效率为目标，多层次一体化地研究OpenMP运行时系统自适应调度技术、Hadoop运行时系统作业任务协同调度技术、操作系统并行线程集分层聚集映射算法、同步调度执行算法以及软件Cache分区技术，为增强多核系统数据并行类应用的并行性能提供共性的软件支撑技术。创新之处在于通过具体编程模型挖掘应用负载的数据局部性特征以及建立三个模型（存储层次模型、局部性关系图模型以及多核效率模型）研究上述关键技术。其研究成果将有助于增强数据中心并行应用在多核系统上的并行性能和可扩展性。
计算系统安全性保持与增长的自律机理研究	自评估;自优化;自律计算;系统安全性增长	自律计算能够克服计算系统的异构性和复杂性，被认为是实现系统自治、解决系统安全性能下降问题的新的有效途径。本项目以自律计算在系统安全中的应用为基本背景，以增加系统安全的自律特性为切入点，探索融合自律计算的系统自主实现安全性保持与增长的自律机理，为系统整体安全性的渐进演化式自主增长提供理论依据和技术支撑。基于生物启发原理，将自律计算的主要属性引入非线性不确定的计算系统，给出系统安全演变一般规律的形式化描述，以此为基础分层次建立并验证系统安全性的自律计算模型；考察影响系统安全性能的属性依赖关系，通过将智能决策理论融于系统不同的安全时序阶段和安全策略，提出系统安全性的自评估方法；引入多目标极值优化方法，刻画系统安全单元的行为自我更替、资源自主调配和状态动态变换规律，解决带约束的多目标系统安全性自优化问题。
众核处理器容错性设计之研究	容错性设计;众核处理器;硬性错误;软性错误	随着集成电路制造工艺的不断发展，高集成度、低电压、高频率等特性使得芯片的硬错及软错增多，容错设计成为了芯片设计的一个关键。另一方面，单核处理器在性能、功耗等方面受到了严峻的挑战，众核处理器成为了最具前景的计算平台。本课题将研究众核处理器硬错和软错的解决方案。针对硬错，本课题拟采用分而治之的策略对众核处理器进行区域划分，同时把出错情形细分为处理器核本身出错、路由电路出错、输入链路出错、及输出链路出错，这些方法将提升众核处理器的容错能力及扩展性，并用较低的硬件开销最大程度的提高处理器核的利用率。针对软性错误，本课题将基于全局异步局部同步的概念来研究众核处理器软性错误的检测及恢复策略，在检测到软性错误时对时钟进行细粒度的频率调节，与传统多倍冗余的方式相比具有高性能、低功耗的优点。本课题具有极强的学术价值及应用前景，并将从容错性设计这一侧面提升我国在众核处理器领域的研究。
面向版本授权的快照数据安全存储技术研究	云存储;数据加密;版本授权;快照	如何对数据进行安全存储是基于互联网的存储系统迫切需要解决的关键问题。存储系统中快照版本的数据依赖（映射）关系破坏了按版本授权的访问控制机制，对现有技术提出了新的挑战和机遇。本项目首次提出了快照数据的版本访问控制问题，并针对版本数据存储特征和授权访问特征设计适合该问题的CCA安全模型及可证安全的公钥加密方案，保证版本数据的私密性。同时针对引入的安全机制，基于现有快照技术改进和发展版本数据的存储机制，保证数据的访问性能。本项目预期研究成果对提高面向互联网的数据服务的安全性和可用性具有重要的理论意义和实际应用价值。
片上统一虚拟存储系统结构研究	多核处理器;存储体系结构;平衡预取;多级Cache;主动推送	存储体系结构对多核处理器系统性能提升起着关键作用。以往面向特定的存储体系结构优化程序，才能在特定多核处理器上达到最佳性能。否则，多核间对有限容量的Cache竞争访问会导致访存冲突加剧和存储系统的颠簸，进而影响到片上多核处理器性能。.因此本项目提出研究片上统一虚拟存储体系结构，旨在通过对不同层次的存储系统进行统一的管理，从而将有限的片上存储资源在不同处理器核间共享使用和动态划分，最大效率的利用所有存储资源，提高多核处理器在不同计算负载下的存储系统适应能力，从而充分发挥多核处理器的并行处理能力。.本项目将研究片上统一存储体系结构支持核间存储资源的动态划分，研究专用数据通信网络支持分布式共享存储资源的互连互通，研究不同层次间的数据推送和核间平衡预取策略。其研究成果将有助于简化并行程序的存储器编程模型，提高多核处理器存储系统的资源利用率和吞吐率。
面向异构多核千万亿次并行机的辐射流体力学并行算法研究	性能评估;辐射流体力学;千万亿次;并行迭代方法;异构多核	针对多介质辐射流体力学问题，结合异构多核千万亿次并行机体系结构多核处理器缓存命中率、指令级并行效率和系统通讯带宽、通讯延迟、问题规模和问题结构等因素，研究异构多核下的并行性能评估数学模型；结合辐射流体力学问题的计算与通讯特征，及异构多核并行机多核之间共享缓存、单核内对同步多线程的支持及单节点内处理核数量多的特点，研究并行性能优化技术；充分发挥多核对数据密集型处理快的特点，研究新的区域分解并行预处理技术；基于降低全局通讯次数，研制适合于异构多核并行机的并行Krylov迭代方法，解决迭代方法并行计算的瓶颈问题；基于GPU-CPU混合编程模式，研制适合于异构多核千万亿次并行机的并行线性代数解法器，使得所研制的方法可扩展到上万个核，并行效率达到30%以上；研究适合于异构多核并行机的混合精度Newton-Krylov子空间迭代方法；将所得成果应用于辐射流体力学实际问题的数值模拟，提高模拟的整体效率。
云存储的数据自主访问控制与数据完整性盲审计方法研究	云存储;云系统;分布式系统;访问控制;存储审计	云存储面临的主要问题是：⑴数据机密性问题，数据所有者担心云服务器让未获许可的用户使用其数据；⑵数据完整性问题，数据所有者担心存储在云服务器中的数据被破坏或删除。本项目对此进行研究，主要内容是：⑴研究数据自主访问控制机制，以保护数据不被乱用。⑵研究高效的第三方存储审计方法，以保护数据不被破坏。基本思想是利用表达能力强的多层次多维属性代替传统的简单字符串，表示多授权机构条件下的用户身份及其权限和访问策略，同时支持属性级、前向安全和后向安全属性撤销；以双线性映射函数为基础设计访问策略隐藏方法，实现细粒度自主访问控制，同时避免共谋等权限漏洞；以同态哈希函数为基础，设计盲审计方法，高效支持数据动态更新和批量审计，避免大的计算和通信开销。所提出的新方法，具有实用性，有助于建立具有公信力的云存储系统，实现云存储服务提供商、数据所有者和用户的互信、共赢。
SEU故障的系统级容错加固技术研究	BCH;纳米工艺;微处理器;流水线;SEU容错	研究采用纳米级工艺制造面向空间应用的高性能微处理器具有重要的战略意义和现实需求。空间环境下，单粒子事件会引起微处理器电路的数据与状态翻转（SEU）故障，需要在系统级和电路级采用容错加固措施才能保证微处理器的工作可靠性。随着特征尺寸减小和工作电压降低，纳米级电路中SEU故障的发生机率大大增加，且不再仅仅引起单个数据位翻转，而是最多可同时引起8位翻转。以EDAC、TMR、奇偶校验为代表的容错方法已无法满足要求，研究探索新的能够对多位错误进行快速高效容错的算法已成为纳米级微处理器发展必须解决的重要科学问题之一。本项目以流水线和存储部件为研究对象，提出一种自修复双冗余流水线结构和一种快速BCH混合纠检错方法，研究高性能微处理器多位SEU故障的系统级容错加固方法，与器件级加固措施结合，不仅可以消除空间环境下SEU故障对微处理器的危害，提高可靠性，还可降低资源需求，提高处理器工作速度。
片上多处理器共享Cache优化关键技术研究	片上多处理器;共享缓存;资源划分;替换策略;数据预取	片上多处理器通常采用共享最后一级Cache的方式。由于多核对共享Cache的竞争问题，同时兼顾多核对Cache访问的高性能和公平性具有较大挑战性。针对此，从多个角度进行Cache优化，并有机整合为完整的Cache优化框架。.在硬件结构方面，借助理论分析与实验验证手段研究共享Cache大小、数量、组织方式、互连结构对线延迟的影响，通过物理路径优化和逻辑距离优化，缩短线延迟；在资源管理方面，研究基于各个核的负载分布情况进行公平性度量的共享Cache资源动态公平划分机制；在数据调度方面，研究充分挖掘核间数据访问模式协同性的数据预取策略；在失效替换策略方面，研究结合数据访问频率信息、实现机制更简单的替换策略。.通过有机集成以上优化策略，能够降低硬件结构复杂性，减小访问延迟，动态保证资源划分公平性，增强Cache与处理器核的协同性，提高命中率和资源利用率，实现多核性能的均衡提升。
面向闪存特性的文件系统数据组织优化技术	闪存存储系统;闪存文件系统	闪存存储设备正越来越广泛地被应用实际系统。闪存的数据操作与传统磁盘的操作存在较大差异，如闪存具有擦除操作、随机读取性能高等特点。而传统文件系统多针对磁盘设计，不能充分发挥闪存优势。针对该问题，本项目研究面向闪存特性的文件系统数据组织优化技术，具体包括：1）内存与外存多层次互补式数据组织方式，结合不同层次的存储介质特性，设计两个层次上特性相互补充的数据格式；2）并发感知的文件系统级数据分布技术，均衡考虑数据并发、数据冷热及地址连续性等多种制约因素。本项目将基于真实的闪存存储平台验证所研究内容，为闪存文件系统的设计提供数据组织方面的理论与实验基础。
基于云存储服务的高性能计算作业开放云服务关键技术研究	作业管理;工作流;云存储;云服务;高性能计算	科学研究和工程领域产生大量数值模拟和数据分析处理的计算需求，既需要海量的计算资源以完成繁重的计算任务，也需要高效可靠的工具快速传输大规模的输入和输出数据，更需要简单易用的交互平台管理这些复杂计算任务及其包括的大量批处理作业。因此，本项目充分利用广泛分布的云存储服务和快速发展的高速网络，以用户需求和应用为中心，从整体上研究云存储服务和高性能计算服务作业管理的融合问题，重点研究基于云存储服务的作业数据快速传输、多种数据来源的作业异步提交、轻量级和可扩展的高性能计算工作流管理模型，实现轻量级高性能计算开放云服务和计算服务原型系统, 提供高性能计算服务的开放能力和按需定制能力，从而为用户提供以应用为中心一站式计算服务，为多个领域的开发人员简单易用的开发平台，支持1-2个典型应用。
众核集群程序设计机制研究	编程语言机制;编程模型;众核集群	众核（GPU）集群是在普通集群的各节点上增加众核加速设备构成的异构集群，也是当前大规模超算系统的典型架构。本课题研究基于新数组类型的程序设计机制。其主要思想是将多维数组扩展为多层次的树状结构。这一扩展间接反映了众核集群树状存储器结构和不同处理器在集群架构中的异构性。数据的划分、分布、转置与变形均可以在新数组类型这一层次得以简洁表示和处理，而同类的线程也组成数组：多核并行、众核并行以及集群并行得以统一。这一程序设计机制在支持用户对系统的全面控制和深度性能优化的同时可以简化并行程序的设计、维护和修改。本课题也将提供在天河这样的大型GPU系统上稳定计算的技术与编程接口。
适应于大数据特性的智能存储技术研究	智能存储;4V特性;存储结构;大数据;数据组织模型	大数据技术对现代科技与经济的发展带来了深远的影响，同时也对传统的信息处理技术提出了严峻的技术挑战。目前大数据的存储和管理技术已经成为大数据分析和处理技术中首先必须解决的前期关键科学问题，由于现有的信息存储技术不适应大数据的体量大（Volume）、模式多（Variety）、变化快（Velocity）和价值密度低（Value）等4V特性方面的存储需求，所以开展适应大数据特性的智能存储技术研究有重要的现实意义。本项目拟研究的主要内容包括：（1）大数据"低能耗"分层存储和数据备份方法（2）大数据存储设备安全预警和数据主动迁移"智能"存储管理方法。本项目希望通过上述两个方面的研究，期望在大数据的智能化存储管理技术方面取得一系列研究成果.
面向信息物理融合的可重塑异元嵌入式组件协同建模与验证方法	信息物理融合系统;嵌入式系统;组件;协同验证;协同建模	嵌入式组件是构建信息物理融合系统的基石，涉及多种计算模型的集成和协同工作，面临设计方法不统一、重塑性差、复杂性高、难以实现信息物理协同设计和验证等问题。项目针对异元环境下嵌入式、传感、控制、通信和物理组件协同设计，提出一种可重塑、结构化、可描述行为的且嵌入式软/硬件和其它异元组件通用的开放性组件模型，用统一方法进行建模，解决模型的不开放问题；提出可扩展一致描述方法，用统一规范描述各类组件，解决不同计算模型描述语言不一致和不可扩展问题；提出多级开放组件模型的协同验证方法，确保模型真实地反映设计者的建模意图，尽早检测出一些会导致建模失败的设计错误，解决验证的不可协同问题。这种模型驱动建立可重塑异元嵌入式组件并确认其设计正确性的过程，支持信息物理协同设计和边构建边纠正，可避免在系统实现过程中发现问题时再进行反复修改的弊端，为构建以嵌入式系统为核心的信息物理融合系统提供必要的理论和可行的技术支撑。
基于时段逻辑和时间Petri网的RFID复杂事件描述与检测模型及验证研究	RFID;复杂事件描述;复杂事件检测;Petri网;时段逻辑	随着现代计算技术和传感网络的飞速发展，以无线射频识别技术（Radio Frequency Identification，RFID）为核心的软件系统已成为当前国际上研究的热点和前沿领域。RFID复杂事件处理的基础理论模型与分析验证技术是RFID系统研究中最基础、最核心、最有挑战性的问题之一。本项目立足深入分析RFID系统及其数据的本质属性，充分运用离散时段逻辑描述方法和时间Petri网的建模方法，研究基于时段逻辑和时间Petri 网的RFID复杂事件描述语言与检测模型及其验证分析技术，着力在RFID复杂事件处理的基础理论领域开展创新性探索，为RFID系统研究与推广应用提供理论模型支持和分析手段。本项研究的开展对于攻克RFID共性基础及前瞻性、产业化关键技术，建立起我国RFID技术自主创新体系，具有重要的意义。
基于进程相似性的大规模并行程序在线可扩展分析方法研究	并行程序;通信Traces压缩;进程相似性;可扩展性分析	高性能计算机的硬件已经进入千万亿次计算时代，但是，大量重要的科学应用程序尚未扩展到相应的计算平台。并行程序可扩展分析技术能够帮助应用开发人员定位并行程序性能瓶颈，改进程序的可扩展性。然而，随着系统规模的逐渐增大，现有可扩展分析方法产生的性能数据已经远超出当前的I/O处理能力，使其无法帮助程序应用到拥有数十万个核的高性能计算平台，这较大地限制了高性能计算领域的发展。针对上述问题，本课题研究工作包括：首先，研究大规模并行程序进程间计算和通信模式的相似性，提出基于通信类型序列和函数调用图的轻量级方式对进程分组，选取代表进程进行性能分析；其次，提出静动态结合的大规模并行程序通信Traces压缩算法，改进现有压缩技术完全动态时猜测通信结构较高的处理开销；最后，提出基于虚拟重叠网络和进程间删冗的在线性能数据采集和分析方法，定位程序的可扩展性瓶颈，改进现有方法较大的离线存储和处理开销。
基于非IT设备虚拟化的主机托管式数据中心能耗管理	博弈理论;资源利用率;数据中心;能耗管理;非IT设备虚拟化	随着云计算产业的发展壮大，其所依托的主要计算系统——数据中心的数量与规模在近年间迅速增长，而其大量能耗也成为颇受关注的话题。作为一种常见的数据中心模式，主机托管式数据中心在能耗优化方面存在其固有的缺陷，包括难以对服务器和机房内其他设备采用提高能效的专用化设计，以及数据中心运营者不直接控制服务器能耗等等，因而面临能效偏低又不易改进的困局。为此，本项目提出非IT设备虚拟化的思路，让托管数据中心的运营者向客户提供虚拟化的供能、制冷和储能等非IT设备，并为设计与之相适应的能耗管理方法而展开以下研究：基于非IT设备虚拟化的资源分配机制；面向托管数据中心客户的功耗控制系统；面向托管数据中心运营者的能效协调管理技术。本项目的成果可为改善主机托管式数据中心的能耗管理提供一个有效的系统性方案。
嵌入式多媒体流计算的自适应机制与跨层优化	嵌入式系统;流计算;多媒体系统;跨层优化;自适应	目前常用的嵌入式多媒体流计算系统缺乏动态优化机制，虽然新型流处理器增加了硬件可伸缩功能，但是由于系统仍不能根据需求定制所需计算资源，导致资源利用效率不高。本项目研究嵌入式多媒体流计算的自适应优化问题，内容包括用于联合优化的系统分析模型，支持双向信息传递的跨层结构和资源受限条件下的系统优化方法。创新性地结合转移函数法和状态变量法建立系统分析模型；并根据多媒体流计算的软实时性，提出以双向跨层信息传递结构替代传统的单向信息传递结构，实现计算资源和服务质量的联合优化。通过预测和调度主动利用流计算任务的统计特性，均衡计算负载；以博弈方式使系统组件通过相互竞争与合作，实现对共享资源的合理分配与高效利用。这种计算模式具有适应外在需求动态优化自身组织结构的能力，可节省系统能耗。研究结果为新一代节能型嵌入式多媒体系统的优化设计提供了新思路，对丰富和发展嵌入式流计算的学科理论有重要意义，具有广泛的应用前景。
多核系统中基于新型存储器工艺的高能效缓存设计研究	缓存体系;高能效;新型存储器工艺;多核系统;非对称访问	新型存储器工艺（如MRAM和PCM等）作为SRAM的替代工艺受到越来越多的关注和研究。这些存储工艺具单位面积存储密度高，静态功耗低等优点。在多核系统的缓存体系中使用这些存储工艺，有助于提高系统性能并降低功耗。然而，对基于新型工艺缓存的访问操作在性能和功耗等方面存在"非对称性"，并且在工艺扰动的影响下显得更加突出。这一特性已成为采用这些新存储工艺的一个主要障碍。本课题研究如何在多核系统中，利用这些新型存储工艺设计高能效的缓存。主要研究内容包括：（1）考虑工艺扰动的影响对新型存储工艺在器件和电路级建模；（2）基于模型开发能够根据不同系统规格要求优化缓存电路设计的仿真器（3）针对新型工艺缓存的非对称性提出结构改进和管理策略的优化。通过以上研究，本项目可以帮助缓存设计者根据多核系统的设计规格快速的选择合适的存储工艺和设计参数，同时利用结构级优化技术进一步提高缓存的性能并降低功耗。
GPU通用计算系统检查点方法研究	GPU通用计算;检查点;虚拟机;程序分析	本项目以GPU图形处理器在通用计算系统中的应用为背景，以提高GPU通用计算系统的可靠性为切入点，探索GPU程序高效检查点技术的实现方法与理论基础，使之满足GPU通用计算系统在高性能计算和超级计算中的理论和应用需要。以鲁棒性、高性能、透明性、灵活性为设计原则，将传统CPU检查点技术与GPU的体系结构特征结合起来，系统性地研究GPU核内检查点机制中的主要过程和关键问题，将增量存储、代码静态分析等技术融入到GPU核内状态的读取、保存和恢复过程中；对GPU硬件状态进行分析和建模，提取主要的特征参数，基于现有GPU通用计算软件开发框架，分析GPU程序内部语义，构建用户透明的检查点技术；研究GPU检查点技术在不同应用场景的应用，如虚拟机环境中的GPU计算任务在线迁移、GPU程序调试支持和自动错误诊断。
面向云计算多媒体应用的体系结构研究	多媒体应用;测试程序集;体系结构;云计算	目前，除传统文本数据外，多媒体数据已成为云计算环境处理的主要数据类型,各种新型多媒体应用层出不穷。与文本应用相比，云计算环境的多媒体应用，除具有数据密集的特征外，还具有计算密集的特点。因此，如何基于已有硬件加速这些应用的处理速度并且在未来云计算的体系结构设计中体现它们的特性变得日益迫切。 针对云计算多媒体应用对体系结构研究和设计造成的挑战，本课题将通过分析当前云计算环境中典型多媒体应用的特点，进行适合云计算体系结构评估的多媒体测试程序集设计；通过分析这些应用面临的性能挑战，设计基于已有并行硬件平台的加速算法并分析现有硬件在处理这些应用时在速度等方面存在的瓶颈；在此基础上，开展面向云计算多媒体应用的新型体系结构研究。研究成果有望解决云计算多媒体应用处理体系结构相关的基础问题，对提升云计算环境体系结构评估的有效性和加速相关体系结构研究的进程将具有普遍的适用意义。
基于多核平台的实时嵌入式混合关键性系统的设计，分析与系统支持	实时嵌入式系统;多核;实时调度算法;混合关键性系统	现代高度复杂的实时嵌入式系统，例如汽车电子系统，近年来有一个提高系统集成度的硬件平台整合趋势，即将多个不同关键性级别的应用整合到运算能力较为强大的共享硬件平台上，成为一个混合关键性系统。多核处理器的性能与功耗优势使其在混合关键性系统中的应用日益广泛，但是多核平台上的各种共享硬件资源，包括处理器，缓存, 内存等，使得基于多核平台的混合关键性系统的设计与实现具有较高的挑战性。我们计划为基于高性能嵌入式多核平台的混合关键性系统设计提供设计算法与运行时系统支持，包括混合关键性系统的实时调度与设计优化算法，为从软件模型到最后实现的整个设计流程提供有效的算法支持；与基于AUTOSAR的多核操作系统，为应用在多核平台上提供有效的运行时系统支持。
基于NIC的Exascale级计算机聚合通信卸载关键技术研究	非阻塞;聚合通信;NIC体系结构	基于网络接口控制器（NIC）的聚合通信卸载技术是解决并行应用程序通信瓶颈的重要途径，在Exascale级计算机系统背景下，下一代基于NIC的聚合通信卸载技术面临着众核处理器、系统规模爆炸性增长、互联网络复杂等方面的挑战，迫切需要开展NIC新体系结构的研究。本课题拟提出新的聚合通信卸载软硬件构架，通过软件生成算法框架，硬件提供可编程原语支持的技术途径，降低硬件实现的复杂度，并解决有效支持众核处理器、上十万个节点的可扩展性需求等问题。课题还拟在新的架构下，研究支持互联网拓扑、非阻塞、近邻模式等聚合通信新特性的关键技术。本课题进行的研究着眼于突破软硬件接口、算法框架提取、硬件原语设计、NIC体系结构等一系列关键问题，将为下一代高性能计算机NIC的设计实现提供有效的理论和技术支持。
基于访问特征分析的流媒体存储系统节能方法研究	存储;能耗;节能;特征分析;流媒体	流媒体存储系统的高能耗问题逐渐成为制约其发展的一项重要挑战，虽然针对通用存储系统节能方法的研究已经成为研究热点，但这些方法很难适应流媒体应用的多种特征，流媒体存储系统的节能问题还有待于进一步深入研究。申请者在前期的调研和实验分析中发现，在流媒体环境中达到优异节能效果的关键，一是降低节能方法的数据迁移开销，二是充分利用流媒体用户访问规律的潜在信息。因此本项目拟基于大量流媒体用户访问记录，抽取和分析与节能密切相关的流媒体应用的用户访问特征，并建立准确的流媒体存储系统的能耗模型；然后在此基础上，提出低数据迁移开销的动态节能数据分布方法，以及基于流媒体用户访问规律的静态节能数据分布方法；并建立原型系统进行验证。总的来说，本项目能够为流媒体存储系统的节能减排提供理论与实验依据，促进流媒体应用的进一步发展，同时也有助于环境保护，具有重要的研究意义。
面向通用计算集群的全局GPU虚拟化理论与方法研究	虚拟化;通用计算;集群;图形处理器	当前GPU集群已成为超级计算领域的研究热点，在国内和国际领域出现大量高效率、低能耗的超级GPU集群。但GPU集群作为异构集群，仍然无法摆脱编程难和应用少这两个传统问题，特别是大量商用软件不能直接使用GPU集群提供的加速能力，影响了GPU集群的实用性。本项目借鉴分布式存储领域的虚拟化理论和实践，以动态库拦截和块采样、虚拟GPU中断为主要研究工具，力图通过将GPU集群虚拟为单一的GPU镜像，从而简化编程复杂度，透明地支持商用软件和不开源软件，最终提升GPU集群的可用性和实用性。
大规模众核处理器光互连网络关键技术研究	存储系统;片上网络;众核处理器;芯片光互连;存储墙	高性能众核处理器是未来高性能计算的支撑技术。目前，众核系统设计挑战中，互连通信逐渐成为制约系统性能提升的瓶颈。新兴的3D集成技术和硅基光子器件在芯片功能、集成密度和功耗方面有独特优势，为解决众核系统互连瓶颈带来新的机会。本课题以研究众核系统互连瓶颈为出发点，探索众核微处理器光互连网络的创新型系统结构，并利用网络演算理论对众核互连网络进行建模与分析。本课题的研究内容跨越计算机系统结构、微电子学与光电子学，具有较强的创新性和很高的研究价值；同时，理论与实际结合紧密，将为众核处理器互连瓶颈问题提供新的解决方案，对推动高性能处理器技术发展做出积极的贡献。
面向大规模计算系统的分布式I/O资源池化研究	大规模计算系统结构;分布式资源池化;弹性资源分配;融合互连网络;I/O共享	大规模计算系统资源效率问题突出，集中式I/O资源池化架构能够实现跨节点资源共享，提高资源利用率，但对硬件部署方式改动较大，且存在性能和扩展性瓶颈，整体可用性不足。本课题旨在提出一种高可用的分布式I/O资源池化架构，研究以下三方面内容：一、在保持现行主流硬件部署方式的同时，在池化机制上利用单边通信代理避免长距资源共享路径上请求响应回路的产生，解除集中式资源池化的规模和性能限制；二、通过节点内节点间协同优化的互连网络设计提高池化数据的传输效率和服务质量；三、通过位置感知的资源分配和空闲资源碎片在线迁移提高应用与底层架构的耦合度，进一步提高资源利用率。课题将使用FPGA原型系统验证架构的正确性和有效性，以及实际部署的可行性。
基于多块结构网格的并行自适应算法研究	并行算法;自适应算法;多块结构网格	在采用多块结构网格的大规模数值模拟中,存在局部计算区域精细模拟需求与全局网格加密造成计算资源浪费之间的矛盾。多块结构网格大规模并行自适应算法是解决此矛盾的有效方法，但至今未得到广泛应用。其根本原因在于：不同网格块的索引空间不统一，使得针对多块情形构建统一的块间通信模板异常困难。文献显示，现有研究工作均未能很好地解决这一困难，由此造成算法实现复杂，处理器核同步时间过长，无法扩展至上千处理器核。此外，在非结构拼接处难以实现高精度插值算法，也影响了其应用。 针对上述困难，本项目针对块间关系描述算法和层间块间通信调度策略均提出了新的思路，旨在构建统一的块间通信模板，并将设计新的负载平衡策略，以解决该算法难以扩展的问题。同时还将研究插值算子，在不影响性能的基础上，保证算法精度。最后集成上述工作，实现并优化面向上千个处理器核的多块结构网格并行自适应软件，解决该算法的实用化问题，支撑相关应用领域的发展。
混凝土细观数值模拟中的高效并行预条件技术研究	并行算法;混凝土细观数值模拟;预条件;稀疏线性方程组	对混凝土试件动态性能进行细观数值模拟对指导高拱坝的抗震设计具有重要意义，但由于离散网格规模巨大，导致稀疏线性方程组求解所需时间在整体细观数值模拟时间中占95%以上，研究高效并行预条件方法减少线性方程组求解时间是加快数值模拟的必由之路。.本项目针对非结构化网格下混凝土细观数值模拟程序中的对称正定稀疏线性方程组，基于图分割算法，研究将子图扩展为重叠子图时的最佳重叠方式，采用对子图间与子图内节点进行综合排序的方法，控制非零元素离对角元的距离，并在分析加性Schwarz并行预条件不能趋近对应串行预条件机理的基础上，对局部分解因子研究最佳的舍弃策略与组装方法，构筑对称正定且随重叠度增加趋近于对应串行预条件的并行不完全分解预条件。同时，基于混凝土细观数值模拟中稀疏线性方程组的特点，分析系数矩阵中元素之间的相互关系，研究粗网格构造方法，设计粗网格校正算子作用到向量上的并行算法，进一步改进并行预条件。
面向CFD并行应用开发框架的高效容错方法研究	检查点;计算流体力学;并行应用;容错;开发框架	利用并行计算机对CFD应用进行模拟，已经得到学术界和工业界的广泛认可。然而，并行计算系统日益严重的可靠性问题，却严重制约了CFD方法的进一步发展。传统容错方法在应用于CFD应用时，存在易用性与效率之间的矛盾：一方面，为了便于使用，系统级容错方法引入大量容错开销，这是大规模并行CFD应用不可接受的；另一方面，为了降低容错开销，应用级容错对程序员提出更高要求，CFD应用领域的程序员难以胜任。本课题首次提出将容错方法嵌入面向CFD并行应用开发框架以设计高效容错方法的思想。借助框架高度抽象的组织结构，让CFD应用研发人员以类自然语言的方式配置各种容错方法；同时，利用框架提供的程序信息，指导高效容错方法的设计。我们将对面向CFD并行应用开发框架容错的组织结构、机制方法以及优化技术展开研究，最终设计实现一个切实可用的嵌入容错功能的CFD并行应用开发框架，从而解决或缓解CFD并行应用模拟的可靠性问题。
绿色认知无线传感器网络的分布式多目标联合能效优化	绿色认知无线传感器网络;随机博弈;多目标联合优化;分布式能效优化;次梯度学习	本项目针对绿色认知无线传感器网络的能效优化问题，提出一种分布式多目标联合优化方法，均衡频谱检测和信息感知的能量消耗，提高网络的频谱利用率和面向应用的信息感知性能。1）研究能量受限的认知传感器网络节点的分布式协作频谱检测技术，设计分布式次梯度学习算法，提出保证频谱检测性能的节点休眠调度机制；2）对认知节点获得的不确定信道状态信息，设计部分可观测随机博弈算法对各个可用频段的发送功率进行控制，实现最佳信道分配和功率控制；3）研究认知无线传感器网络的应用需求，根据环境信号源分布特性，将节点休眠调度机制和认知场景下的干扰约束有机融合，设计分布式认知注水算法，实现保障应用数据准确率的频谱认知与数据感知的能效联合优化。通过本项目的研究，将为绿色认知传感器网络的能量均衡使用提出一种有效的并行分布式多目标联合优化方法,得到可扩展性强的分布式计算架构，对认知无线传感器网络的绿色通信技术发展起到一定的促进作用。
众核系统动态能耗管理关键技术研究	高性能计算;众核;能耗管理;绿色计算	随着高性能计算系统规模不断增长、性能不断提高，系统功耗问题也日益凸显，高能效设计也成为高性能计算系统的首要设计需求之一。另一方面，众核体系结构在高性能计算领域扮演着越来越重要的角色，不仅为高性能计算系统提供了强大的计算能力，同时也消耗了大部分系统功耗。因此，研究众核系统的动态能耗管理技术，降低众核芯片的功耗、提高其能效，对于提高整个计算系统的能效具有重要作用。针对众核体系结构的新特点及其能耗管理的迫切需求，本项目拟通过建立应用程序透明的动态能耗管理软件支撑系统，对众核系统的功耗和性能实现有效的调节和管理，提高系统的能效。以此为出发点，深入研究与之相关的众核体系结构的功耗模型；结合功耗模型及性能模型，探索可扩展众核系统能耗管理技术；研究针对众核的高效动态电压频率调节技术。
基于网络编码的分布式存储容错机制研究	网络编码;数据容错;分布式存储	随着网络上的信息量、磁盘存储量、网络带宽和计算资源的增长，分布式存储系统及其相关应用正在快速发展。网络编码作为信息论的一个新兴领域，越来越多的研究领域因网络编码理论而迅速拓展。本项目拟开展基于网络编码的分布式存储容错机制研究，具体研究内容包括:(1)根据分布式存储系统中的性能参数和成本参数构建分析模型，设计用于数据容错的网络编、解码算法以及相应的数据存储方法，降低数据冗余度，提高数据可靠性；(2)针对基于网络编码的存储方法，研究用户数据访问机制与故障节点的数据恢复机制，降低用户访问延时、数据恢复过程中的网络负载与恢复延时；(3)设计出错编码块的检测机制，防止出错数据块在存储系统中蔓延；(4)研究动态分布式网络存储中自适应的网络编码策略与相应的数据存储策略，以提高动态分布式网络存储的系统性能；(5)实现一个基于网络编码的分布式存储的原型系统，实验验证相关网络编码策略与数据存储策略的性能。
基于三维场计算和多值逻辑的模拟计算研究	群智能;三维模拟场;涌现性;多值逻辑;模拟计算	本课题的研究是由模拟场计算、多值逻辑和群智能方法三者共同构成的新型计算系统。以基于模拟场计算和多值逻辑的新型模拟计算模型uEAC为研究对象。在uEAC二维模拟场计算假设的先期研究基础上，本课题通过建立基于三维模拟场的uEAC的计算模型，分别对三维模拟场的物理和数学模型、导电介质各异性和多值逻辑等内容展开研究，突破了原有二维假设的局限性，为uEAC的研究和开发供了重要的基础。进一步开发了在模拟场分别为二维和三维情况下的模拟计算机的原型电路和仿真模型，以及辅助开发的软件环境，以便于在采用uEAC求解典型问题的过程中结合群智能等优化方法，完成对uEAC应用的优化设计。通过对"模拟场计算+多值逻辑"模拟计算机结构的认知模型研究，结合复杂科学和认知科学的理论框架，更深层次的探索模拟计算的演化规律和基于群智能认知模型的智能本质，从而对模拟计算机和群智能方法涌现性行为进行分析和解释。
时空专题语义Web建设方法与技术研究	语义处理;本体方法;描述逻辑;语义Web;时空信息	语义Web的建设是当前国内外研究的热点之一，其目标是建立机器可"理解"的Web，让计算机能够辅助人类完成对信息语义的识别和处理。语义Web的研究和建设将改善传统Web应用，解决Web信息爆炸带来的系列问题。目前语义Web的研究已经进行到了关键阶段，相关的国际标准和技术规范纷纷出台，为建立专题语义Web提供了必要基础。时间和空间信息与人们的工作生活密切相关，在Web上大量存在，其语义处理是未来语义Web系统中不可缺少的组成部分。本课题研究建立时空专题语义Web的方法和技术，包括：Web时空数据表示模型和时空本体、Web时空语法处理和语义获取技术、时空描述逻辑表示和推理技术，在上述研究的基础上建立时空专题语义Web实验系统，对研究方法和技术进行检验和评估。对时空信息的处理是未来语义Web建设的核心内容，项目组将以前期预研的时空本体和标注技术为基础，实现面向时空信息的专题语义Web。
面向I/O密集型云虚拟机的多级协同虚拟磁盘服务关键技术研究	云虚拟机;虚拟磁盘服务;I/O密集型;分布式块存储;多级协同	虚拟磁盘服务是在云计算环境下实现高效I/O密集型虚拟机的关键。除了传统云储存面临的可靠性、可用性、一致性和可扩展性等需求外，虚拟磁盘服务还必须满足虚拟机应用对IOPS、延迟和吞吐率等I/O性能的需求。虚拟磁盘服务通常采用基于多副本（较高I/O性能）或纠删码（较高存储效率）的分布式块存储实现。针对云虚拟机I/O模式的特点，本项目将对"内存-固态硬盘-机械硬盘"多级协同虚拟磁盘服务开展研究。1．针对固态硬盘和机械硬盘的巨大性能/成本差异，研究基于日志缓冲的异构协同多副本块存储技术，以接近机械硬盘的成本实现接近固态硬盘的I/O性能。2．针对纠删码随机写性能较差的问题，研究高效低成本的纠删码块存储技术，在保持纠删码存储效率的同时实现高性能I/O。3．针对传统虚拟机换页磁盘性能较差的问题，研究基于P2P（对等模式）共享网络内存的换页磁盘机制，以对云虚拟机透明的方式实现基于内存的大规模高速换页服务。
考虑测试成本和版图成本的低功耗BIST的研究	测试数据;测试功耗;扭换计数器（TRC）;内建自测试（BIST）;测试时间	不断增加的测试成本（测试时间和测试数据）和测试功耗给芯片的测试问题带来了巨大挑战。本项目提出一种新颖的2-bit TRC向量产生器结构，并在考虑测试成本和版图成本的条件下，对基于2-bit TRC BIST的功耗进行优化研究。具体包括：通过分析传统TRC序列的特性，对由多个2-bit TRC构成的向量产生器所产生序列的特性进行研究，以便为硬件和软件设计奠定基础；采用数据抽象的方法对芯片版图信息进行提取，研究在高层次建立计算版图成本的方法，并在版图成本限制的条件下对输入单元排序分段算法研究；从分析测试集的相容特性入手，对基于2-bit TRC的测试数据压缩算法进行研究；采用序列分段思想，对基于2-bit TRC的低功耗BIST的冗余向量删除方案进行研究。通过本项目的研究，拟将得到在测试成本、测试功耗以及版图成本方面具有较大优势的BIST方案，对推动BIST实用化具有较好的实际意义。
基于深度神经网络稀疏性的异构计算访存优化方法研究	计算机系统体系结构;加速器设计;异构计算;计算机系统设计方法;类脑计算	基于神经网络的深度学习技术在大数据智能信息处理领域中占有至关重要的地位。然而，随着数据规模的急剧增长，基于通用处理器的计算机系统上运行大规模深度学习算法的效率较低，难以满足不同场景下各种应用在功耗和性能等方面的需求。本项目基于深度神经网络的稀疏化特征，重点研究其在异构计算系统上的高效能访存优化方法，以缓解所面临的内存瓶颈问题。首先，利用深度神经网络模型拓扑连接和特征映射的稀疏性，探究针对异构硬件优化实现的网络模型稀疏化原理，探索基于硬件约束的稀疏化网络模型学习机制；然后，采用划分和聚类的方法将稀疏化模型参数形成局部稠密块，从而降低数据存储访问时带宽限制的影响；最后，通过监测模型特征映射的稀疏性，动态调整任务调度及线程压缩，进一步提高数据访存效率，优化整个系统的性能。充分挖掘深度神经网络稀疏化特征，面向异构硬件提升系统访存效率，对深度学习加速器研究领域具有重要的科学意义。
大数据环境中利用闪存内存模组提高I/O性能的关键技术研究	NAND闪存;闪存内存模组;内存总线调度	I/O的高延时等待严重的影响了大数据应用程序的性能。基于闪存（NAND Flash Memory) 的非易失内存模组（简称"闪存内存模组"）可以有效地解决该问题。该技术利用闪存技术存储外存数据并通过高速内存总线传输外存数据，因此可以有效提高I/O性能，但同时引入一些新的问题。通过内存总线传输外存请求导致与传统的DRAM内存请求竞争总线资源，引起新的异构内存总线竞争问题。同时，将外存数据通过内存总线传输，传统外存数据访问特性被破坏。因此，如何高速有效地利用该结构并且减少对传统的DRAM数据请求带来的影响，是亟待解决的问题。针对上述问题，本项目考虑设计新的异构内存调度机制。同时，根据外存数据转换成内存请求后的数据特点设计新的外存调度策略，以提高闪存内存模组的响应速度。本项目的成果将为解决计算机系统中异构内存的总线竞争问题，以及如何利用内存总线的传输特点来匹配闪存内存模组的特性提供新思路。
大规模网络存储系统的容错技术研究	数据一致性;大规模网络存储系统;性能优化;容错技术;纠删码	近年来，随着国家和社会对信息存储需求的增长，大规模网络存储系统的应用需求越来越广泛。随着存储系统规模的不断扩大，用于存储数据的磁盘个数不断增多，磁盘出错变得越来越频繁。为防止磁盘出错引起的数据丢失，确保大规模网络存储系统中数据的可靠性，需要研究大规模网络存储系统的容错技术。本项目拟从大规模网络存储系统的磁盘出错特征模型及评价方法、大规模网络存储系统的高容错高利用率的纠删码技术、高容错纠删码技术的可靠性评价方法、大规模容错存储系统的性能优化方法、大规模容错存储系统的数据一致性等五个方面对大规模网络存储系统的容错技术进行全面研究。通过该项研究，将为大规模网络存储系统的设计建立起完整的容错方案，从而推动实际大规模网络存储系统的设计和应用。
大规模数据中心体系结构级仿真器加速技术研究	统计抽样仿真;数据中心;并行仿真;全系统仿真	仿真速度过慢是体系结构级仿真器面临的关键问题，研究人员通常采用并行化或统计抽样技术来加速仿真。然而在大规模数据中心全系统仿真中，已有工作在如何减少大规模并行仿真所导致的同步开销以及如何有机结合并行与统计仿真技术等方面考虑不足，致使仿真器的性能难以达到实用的要求。本课题针对数据中心并行仿真中同步开销随规模增大快速攀升这一核心问题，研究基于墙钟的同步协议，将全局同步转换为低开销的本地同步，从而有效降低全局同步的频率；同时为避免同步放松所带来的仿真精度大幅下降，研究基于分析模型的误差补偿机制，回滚状态以修正仿真器的微体系结构时序误差；针对并行统计抽样仿真中程序样本代表性低这一关键问题，研究并行感知的抽样算法，采用模式匹配与样本筛选的方法，在提高样本代表性的同时有效避免了对并行化的干扰。通过以上研究，将推动并行抽样仿真技术取得实质性进展，为大规模计算机系统的开发应用提供高效可靠的支撑工具。
基于柏拉图立体多级裂变模型的三维片上网络拓扑结构的研究	多级同步裂变模型;柏拉图立体;多级异步裂变模型;拓扑结构;三维片上网络	基尔比发明了世界上第一个集成电路，改变了整个世界，也因此获得了诺贝尔奖。随着半导体技术的发展，片上系统和片上网络相继问世，然而，受二维布局条件限制难以保证关键部件相邻以减小信号延迟，三维片上网络应运而生，已成为主流研究方向。目前三维片上网络拓扑结构主要是二维拓扑的堆叠，而直接从三维空间进行研究很有必要，具有代表性的是超立方体三维片上网络拓扑结构，但它不具有可扩展性且不易构成多维超立方。柏拉图立体历史悠久，应用广泛，当今又成为新的研究热点；研究基于柏拉图立体的三维片上网络拓扑结构具有探索性，而5个柏拉图立体的节点度不尽相同(最高5)，且可扩展性差。因此，本项目提出柏拉图立体多级裂变模型(同步/异步两种，且具普适性)，旨在用其研究三维片上网络拓扑结构，该拓扑结构具有规则性、低节点度(恒为3)、对剖宽度不变、空间旋转对称性等优点，并使网络扩展从"加法"转为"裂变"，选题具有创新性(见查新报告)。
面向E级计算的并行代数多重网格新型算法研究	稀疏线性解法器;代数多重网格算法（AMG）;激光聚变数值模拟;迭代方法;E级计算	代数多重网格（AMG）是高性能科学与工程计算中不可或缺的共性快速算法，在实际应用中发挥重要作用。然而，面向E级计算，受限于"实际应用和体系结构"双重复杂性，AMG计算效率面临"并行度和计算规模扩大1000倍带来的可扩展性问题"以及"复杂体系结构带来的性能优化问题"的新挑战。本项目面向未来E级系统，依托P级和将要出现的百P级系统，立足实际数值模拟应用，研究适应于复杂体系结构和复杂应用特征的AMG新型算法。主要内容包括：适应于体系结构特征和AMG算法特征的性能评价方法和模型；适应细粒度并行、具有良好局部性的AMG算法和结点内性能优化方法；适应复杂应用特征、具备良好并行可扩展的AMG算法；研制AMG解法器；基于典型数值模拟应用，在百P级系统上进行验证。项目成果将为E级系统的实际应用提供高效AMG算法支撑，同时对探索"体系结构和实际应用"驱动的算法设计新模式具有重要意义。
基于链置换的微流控DNA自组装系统研究	计算机系统结构;链置换;微流控;建模与仿真;DNA自组装	当前DNA自组装模型针对特定问题需设计特定实验，并且在实验过程中存在易错、耗时的人工操作。本项目拟在DNA自组装框架基础上，综合微流控技术、链置换技术设计并构建微流控DNA自组装系统。以揭示分子自组装的本质规律,实现分子操作的可控性研究。主要内容包括：(1)研究分子自组装的动力学行为及分子间的调控机制，结合链置换技术建立可操控的DNA自组装系统模型；(2)开发分子操作的高级语言，借助于微流控和计算机控制技术，通过软、硬件编程的相互协调来构建一个由应用层、模型层、实验层和芯片层组成的自动化、可编程的微流控DNA自组装系统；(3)设计系统的硬件基础、指令系统、处理系统和数据结构。并用该系统设计分子逻辑电路及解决图顶点着色问题以验证系统。本项目的研究旨在进一步提高DNA自组装的可靠性和可控性，研究成果不仅为DNA自组装提供新的模型和方法，还将为其应用于信息处理领域提供强有力的技术支撑。
基于物联网的雾霾重点污染源监测的传输可靠性与内容可信性基础理论与应用研究	传输可靠;物联网;雾霾污染源监测;随机Petri网	物联网具有价格便宜、部署灵活、组网独立、远程通信多样化、抗干扰抗破坏能力强等特点，使得物联网非常适合对雾霾重点污染源进行广覆盖、细粒度的可靠监测。项目围绕解决"目前雾霾重点污染源监测中的信息传输可靠与内容可信"问题，研究四个关键科学问题：①在拓扑可靠性方面，研究适合雾霾重点污染源监测的均匀分簇的三维立体物联网节点部署方案，突出监测网的拓扑可靠性；②在传输可靠性方面，研究本地和远程相结合的多目标优化的可靠传输和实时性保障机制，突出信息传输的可靠性和实时性；③在内容可信性方面，研究基于随机博弈网新理论的节点行为监管的信息内容可信保障机制，突出监测内容的可信性；④在实证方面，对现有传统污染源监测系统按前三个科学特性进行物联网优化升级，并利用贝叶斯网络对污染源排放情况进行反演与修正。由于企业利益可能与污染控制冲突，致使监测环境十分特殊，这使得提高监测的可靠可信性变得尤为重要，需进行专门创新研究。
云存储系统中海量时效数据的组织模式及关键技术研究	云存储;存储体系结构;存储系统	当前云存储系统面临的主要问题包括：海量数据的弱可用、存储系统的低效能和数据价值的强时效，使得存储系统的可扩展性和性能优化面临挑战。为解决这些问题、提高时效约束的存储服务质量，提出面向时效约束的近似处理的系统组织模式、方法论和性能优化的关键技术。基本思想是通过研究近似处理的系统内在机理，即隐含的语义关联特征，对数据集进行语义划分，使得对语义分组的数据管理操作在处理数据量、计算复杂度和结果准确性方面达到量效均衡，进而优化存储系统的整体性能。研究工作注重研究近似处理的系统内在机理和演化规律、以及动态命名规则，并通过构建语义感知的特征向量和支持近实时操作的存储系统组织模式来融合多源、异构、时效的数据，提供高质量的存储服务。面向内存级的存算联动模型能够进一步融合计算模式和存储结构的特点，支持关联特征的快速感知和识别。研究工作具有一定的前期基础，将在真实的云存储系统平台上进行系统实现和测试分析。
面向云计算实时服务的调度模型与算法研究	云计算;可靠性;调度算法;实时服务	随着云计算与各行业广泛而深入的结合，多样化的服务对于时效性要求越来越高。当前云计算实时研究多关注于响应速度而忽视截止期保障。由于云平台数据分布化、计算并行化、集群动态化等特点，传统实时调度理论无法应用于云计算场景，妨碍了云服务的拓展。本课题围绕云计算实时调度的核心问题，首先分析多种基于并行数据计算框架云平台的结构和存储特点，提炼其计算流程的共性，建立形式化、可分析的云计算实时调度理论模型。其次，研究解决实时调度可靠性中的三个关键问题：①硬实时作业的可调度性分析和在线接入控制测试方法；②软实时作业服务器的接纳能力和响应速度；③实时容错版本控制和冗余缩减等。最后，研究大规模异构系统中作业完成时间的估计方法，设计面向现实云计算系统具有避错容错功能的自适应多模实时调度器。本课题旨在增强现有云计算系统对实时服务的处理能力，创新并行数据计算框架的模型研究，从而为建立云计算实时调度理论体系奠定基础。
嵌入式系统中基于PCM/DRAM混合型内存的能耗和耐久性优化技术研究	能耗优化;耐久性优化;嵌入式系统;混合型内存;相变存储器	新型非易失性存储器的快速发展已得到广泛关注。其中，相变存储器（PCM）凭借其存储密度高、静态功耗低、存取速度快和非易失性等优点，与DRAM融合组成混合型内存正推动着存储技术的发展。然而，PCM写能耗高和耐久性低的问题，为混合型内存的性能及可靠性带来挑战。在面向特定应用及资源受限的嵌入式系统中，如何有效利用PCM/DRAM混合型内存及其优势，降低PCM对系统能耗和耐久性的负面影响，是亟待解决的重要问题。本项目结合能耗、耐久性等嵌入式系统关键指标，开展基于PCM/DRAM 混合型内存的可靠性和耐久性研究。通过建立能耗优化模型，设计面向嵌入式系统特定应用的能耗感知的数据分布优化技术，并在此基础上开展以低能耗为目标的元数据分布优化和基于区段的耐久性优化研究。本项目的成果将为嵌入式系统中PCM/DRAM混合型内存的能耗及耐久性优化提供新思路，为混合型内存在嵌入式系统中的高效应用提供技术支撑。
面向存储受限应用的GPU性能预测模型和通信优化关键技术研究	存储受限应用;性能预测模型;通信优化;图形处理器	GPU通用计算技术给高性能计算领域带来了前所未有的机遇和挑战。目前GPU已经在生物信息学、计算金融、机器学习、国防和医学成像等领域得到广泛的应用。基于GPU的异构计算是未来高性能计算发展的主流方向之一。然而，随着GPU的计算能力和峰值带宽之间的差距越来越大，程序的性能更多的由GPU各个存储层次之间的数据移动能力决定。然而，要开发高效的GPU程序还面临着诸多问题，其中以GPU程序的性能优化空间巨大、工作繁杂以及基于GPU的大规模并行计算可扩展性差两个方面最为突出。本课题提出面向存储受限应用的GPU性能分析预测模型和通信优化关键技术研究，从数据传输的角度建立GPU程序的性能预测模拟，通过模型驱动的方式简化GPU程序的优化，结合多流多线程并行机制、负载均衡、计算与通信重叠等技术隐藏节点之间通信开销，提高GPU之间的数据传输效率和大规模并行程序的可扩展性。
面向嵌入式闪存存储系统的系统纵向优化关键技术研究	嵌入式系统;闪存写放大;嵌入式闪存;闪存写减少;系统纵向优化	嵌入式闪存存储系统具有诸多优点，包括随机访问性能好、功耗低、无振动以及尺寸较小。然而此类系统普遍存在可靠性和性能问题，导致其进一步发展受到严重阻碍。首先，随着应用复杂度的增加，存储设计要求增加；其次，随着闪存发展，可靠性和性能不断变差，因此优化嵌入式闪存存储系统可靠性和性能成为其继续发展至关重要的研究课题。本项目突破传统思维，从嵌入式闪存存储系统纵向设计角度展开研究，研究通过设计纵向的写减少技术和纵向的写放大优化技术两个角度改善系统性能和可靠性。首先，根据系统纵向特征，构建以系统纵向访存行为为基础的写操作行为模型和写放大模型，为嵌入式闪存系统的纵向优化提供基础框架。然后在此基础上开展基于系统纵向的写减少优化技术研究和系统纵向的写放大优化技术研究，最终达到系统性能和可靠性的优化。本项目成果将为闪存存储设备在嵌入式系统中的性能和寿命优化提供全新思路，为各类闪存存储设备的进一步应用提供技术支撑。
汽车电子系统中软件组件分配与混合关键性调度研究	软件组件分配;任务调度;汽车电子系统	汽车电子系统复杂性急剧增长，成为了典型混合关键性实时嵌入式系统，在开发和性能保证上均面临许多挑战。项目将在开发前期中软件组件到ECU分配、运行期中多处理器混合关键性调度这两个关键问题开展研究，拟取得以下两方面成果：①采用问题分解思想，设计基于图划分的多步分配算法：减少通信为标准，基于图划分理论生成组件划分；根据外连节点出度对划分进行分支切割。实现较高资源利用的优化匹配，更好适应组件数目增长。②完成ECU配置后，对多处理器ECU，设计可靠的混合关键性任务划分调度算法：从功能子系统级计算任务优先性值；研究基于EDF混合关键性任务可调度性，生成处理器数等同的划分，遍历任务并进行划分检验，以均衡处理器负载为标准选择划分；根据子系统级优先值，调整任务执行顺序或进行迁移，提高调度性能和可靠性。最后进行仿真和原型系统验证。通过本项目研究，为汽车电子系统的研发提供一定理论支持，提高开发效率和系统性能。
面向长尾现象的数据缓存技术研究	访问模式;局部敏感哈希;数据缓存;大数据;数据重删	大数据访问模式由传统的Zipf分布变为扩展指数（SE）分布，传统数据缓存技术不再适用于大数据访问。造成SE分布的长尾现象及缓存效率远低于Zipf分布的根源在于大数据访问的局部性减弱且缓存空间不足。本项目提出面向长尾现象的大数据缓存结构，通过挖掘热文件中所包含的冷文件数据块，在保证热文件缓存访问命中率的前提下，提高冷文件缓存访问命中率；提出基于属性集和相似度检测的文件分类方法来实现分布式缓存管理，采用局部敏感哈希技术对文件进行分组，缩小重复数据的查找范围；进一步采用动态计数型布隆过滤器阵列技术加快重复数据的判断，提高缓存检索长尾全集冷数据的性能。本项目突破传统缓存研究只针对热数据的思维定势，聚焦规模及价值不断增长的SE分布下的冷数据，为大数据缓存设计提供新的思路。
面向高计算密集度应用的片上多处理器并行处理关键技术研究	计算密集;通用处理器;片上多处理器;数字信号处理器;并行	随着科学计算、多媒体、网络通讯及关系到我国国家安全保障的军事电子装备中雷达、电子对抗和声纳等高计算密集度应用领域的快速发展，对处理器的并行处理能力提出更高的要求。本项目面向高计算密集的科学计算、数字信号处理等领域应用，针对这类应用计算量大，数据访问规则，数据并行度高的特点，并进一步分析其线程级、数据级和指令级并行性特征及计算和数据访问模式，结合通用CPU和DSP融合的技术趋势，提出在多核架构上构建CPU与DSP融合的高度并行的新结构，及其相应的存储层次组织和编译自动并行化技术，从而挖掘程序多层次的并行性、提高计算的并行度、保持计算的局部性、增强计算部件的供数能力，提升系统对高计算密集度应用的处理效率。从而使处理器在保持DSP固有的高计算密集度性能的同时还具有与CPU一样的优良的通用性，为片上多处理器体系结构设计提供关键技术，为未来国产多核系列处理器的研制提供指导。
2.5D堆叠众核协处理器片上网络结构研究	2.5D堆叠;片上网络;通用图形处理器;众核协处理器	2.5D堆叠已成为众核协处理器的发展趋势。随着集成DRAM带宽的增加，2.5D堆叠众核协处理器对片上网络性能提出了更高的要求。然而，现有片上网络研究成果不能满足2.5D堆叠众核协处理器的通信需求，如没有利用硅中介层的闲置连线提升性能、设计不适合众核协处理器的many-to-few-to-many通信模式等。为此，本项目将充分挖掘2.5D堆叠众核协处理器的结构和通信特性，利用硅中介层连线资源实现立体片上网络，在硅中介层网络中传输低负载的请求报文，在GPGPU层网络中传输高负载的回复报文，基于通信特性对这两层网络进行优化设计，并在它们之间进行负载均衡，将在基于硅中介层连线资源的立体网络结构、高访存效率的无冲突请求网络、低延迟高带宽的异构环-链回复网络、跨层负载均衡等方面取得创新研究成果。这些成果可直接用于2.5D堆叠众核协处理器设计，具有重要的理论意义和实用价值。
云提供商可信性审计与验证研究	远程验证;可信计算;云计算;可信性;云提供商	云计算被广泛认为是信息技术发展的必然趋势。然而，由于在云计算模式下用户失去了对托管在云端的数据和应用的直接控制能力，云计算与云服务的推广和有效使用很大程度上取决于云提供商的可信性。本项目拟分析造成云提供商不可信的威胁模型以及典型的云计算平台体系结构，在此基础上，设计一种新的引入可信第三方TTP（Trusted Third Party）云提供商可信性审计和验证模型，并针对云平台可信证据收集、云提供商远程可信性验证、云提供商可信审计协议等关键技术进行深入研究。更进一步，为了防止TTP成为单点瓶颈或单点故障，通过云计算技术构建TTP平台，我们称为"小云审大云"。本项目拟实现原型系统，并进行定量分析、测试和评价，期望将其用于实际云计算平台中。
基于博弈论的云提供商及其用户效益协同优化服务机制和策略	纳什均衡;效益优化;博弈论;云计算环境	效益优化是云计算中一个重要的研究内容。当前，几乎所有的研究工作都集中于优化云提供商的效益。与传统效益优化视角不同，本项目从用户效益优化出发来考虑问题。设计优化多用户效益的服务机制从而吸引更多的用户使用云计算服务而提高云提供商的效益。提出采用非合作性博弈和合作性博弈方法对多用户之间的关系进行建模并分析得到各自合适的优化策略。在优化云用户效益的基础上，提出采用Stackelberg博弈模型刻画云提供商及其用户之间的关系。在该模型中，云提供商被当作一个领导者，众多用户被当作跟随着。云用户的优化策略受限于云提供商的策略。提出通过求取该模型下的均衡解来同时优化云提供商及其用户的效益，以期改善云计算中心的服务模型，促进云计算的可持续发展。
多核处理器容错关键技术研究	多核处理器;可重构;片上网络;容错	随着集成电路工艺技术不断进步，单芯片上集成数十亿晶体管成为可能。片上多核处理器为有效利用这些晶体管资源提供了一种高效、可扩展的方案。但随着集成度的不断提高，可靠性问题成为多核处理器研发和应用重要问题之一。本项目拟通过对处理器可靠性和故障分析，研究多核处理器各级Cache、片上网络（NoC）等关键模块故障时的容错技术。包括：基于组划分的Cache行级容错技术;基于网络地址映射的Cache模块级容错技术;基于最后级Cache（LLC）容量借用的NoC容错技术，以及支持容错的可重构NoC架构等关键技术。协同利用片上存储资源、维护片上可缓存地址空间完整、实现片上Cache、NoC容错是本项目研究的关键。缓存空间的网络映射技术、基于LLC的NoC重构与系统适度降级容错技术是项目研究的创新所在。因此，项目研究对于提高多核处理器的可靠性和多核应用系统的可用性具有重要的理论和实践意义。
高性能并行计算环境下膜蛋白分类模型设计与web服务构建	生物信息学;特征提取;蛋白质结构与功能;膜蛋白分类	膜蛋白分类问题作为蛋白质组学研究的一个分支，近年来受到研究者们越来越多的关注。该问题是全面掌握膜蛋白结构与功能的前提和基础，在分子生物学、细胞生物学、药理学和医学中扮演着非常重要的角色。本项目利用生物信息学的手段与工具，针对膜蛋白这一特殊的研究对象，研究膜蛋白分类问题中的特征提取算法，并在此基础上，建立大规模高通量的膜蛋白分类平台，发展相关的并行模型与并行算法。重点研究：（1）膜蛋白分类问题中的特征提取算法，（2）新型而高效的膜蛋白分类模型，（3）在高性能并行计算环境下实现膜蛋白分类的并行算法，（4）建立针对膜蛋白分类的大规模、高通量分类平台与环境。项目完成后，可望发展一套膜蛋白分类的特征提取算法与分类模型，并在高性能并行计算环境平台下实现提供分类服务的web服务器。
CPU/GPGPU紧耦合异构多核系统共享Last Level Cache优化研究	CPU/GPGPU异构多核;结构设计;共享LLC;性能优化	随着片上核心集成技术的发展，极大地推动了异构多核片上系统的发展，它们通常具有共享LLC，从而使整个系统的通讯和资源使用更加高效。本项目面向CPU+GPGPU的紧耦合异构多核片上系统，根据异构的核心对共享LLC访存模式的差异以及LLC上数据的相互共享和干扰，研究优化LLC的方法。具体内容包括：分析CPU+GPGPU紧耦合异构多核系统下共享LLC的架构对计算模型的影响，建立LLC访存性能模型；针对CPU+GPGPU紧耦合异构多核系统共享LLC数据访问模式，设计共享LLC动态划分，重配置LLC的组织结构，调整插入替换策略等，实现数据迁移，改进一致性以及提供数据过滤方法; 共享LLC访存与任务调度优化，对异构核上的访存进行重排，优化线程在异构核上的映射方式，运行时重配置任务的映射，探索LLC对CPU和GPU线程优先级的影响因素；共享LLC性能评估模型的建立，验证共享LLC对计算模型的影响。
云计算与对等计算混合环境下流媒体分发关键技术研究	对等计算;流媒体分发;云计算;资源分配;性能优化	基于对等计算(P2P)的流媒体分发技术由于动态性和传输控制方面的劣势，难以稳定地保证传输服务质量。云计算提供稳定和大量的存储空间和带宽访问能力，可弥补P2P模式的缺陷，两者结合分发流媒体是当前该领域的主要趋势和开放研究问题。本项目针对云计算和P2P环境下流媒体分发关键技术，从组织架构设计、传输性能优化和资源优化部署三个方面，进行系统性研究。项目组结合云计算和P2P模式的各自特征，提出分层混合组织架构，建立和优化"树网"结合的传输拓扑；通过构建最优化模型，提出基于网络编码的最优传输调度算法，设计云计算数据迁移机制，提高云计算资源利用率和系统传输服务质量；根据对云计算资源的动态需求，建立和解决云计算资源部署最优化问题，达到系统传输服务质量和经济开销的最佳平衡。本项目研究成果有望突破云计算和P2P混合环境下流媒体分发技术所面临的传输性能和运营部署的双重瓶颈，实现系统的大规模实际部署和应用。
面向云数据中心应用感知的参与式资源调度技术研究	应用感知;数据中心;资源调度	数据中心是云平台对外提供各类云服务的物理承载者。数据中心资源调度应高效利用底层物理资源、保证所部署的应用的稳定性能。已有的技术框架在适应异构动态的用户资源需求方面提供了支持，但没有较好地解决用户的动态资源需求对云平台资源调度的挑战。本课题拟面向基于虚拟化技术的云数据中心，以系统化构建应用感知的动态资源调度机制为目标，以云平台引入用户参与调度为切入点，实现云平台与用户协同的参与式资源调度。具体而言，基于良好定义的模型和接口，用户可主动询问数据中心物理机及网络的运行细节；根据应用的运行状态，用户可主动向云平台提供其资源需求可能的变更信息，便于云平台应用感知的资源分配、预留和迁移；云平台亦可基于激励机制向用户反馈物理资源的动态定价信息，鼓励或迫使用户调整资源需求曲线。基于课题组自主研发的云计算实验床，系统化测试与验证上述研究成果。
具有随机执行时间的批任务工作流云资源弹性租赁与调度方法	调度问题;工作流;批任务;云计算;随机调度	批任务工作流应用广泛存在于科学计算、商业数据分析等领域，云计算资源动态租赁与工作流调度是其核心问题。任务预测执行时间与实际执行时间的偏差（随机性）使得实际执行结果与调度产生较大偏差，降低了调度算法鲁棒性；云资源按区间计费方式和执行时间随机性使得调度更加复杂，提出对任务执行时间随机性具有良好鲁棒性的批任务工作流资源租赁与调度方法具有一定挑战性。针对任务执行时间随机性，本课题拟提出预测偏差概率分布建模方法和生成多任务总执行时间概率分布的蒙特卡罗方法；针对批任务工作流执行时间和资源租赁成本最小化问题，分别提出基于随机任务执行时间建模的动态调度方法；对于不同约束，分别提出基于加配成本和执行时间降低效率的截止期划分和预算分配方法；针对批任务调度问题，分别提出基于新增计费区间成本期望、满足截止期概率值、等待队列总执行时间期望的资源租赁与任务调度方法；提升云用户的数据处理能力，降低数据处理成本。
分布式文件系统通用化性能评测与缓存调度方法研究	分布式文件系统;性能测试;分布式系统;大数据;缓存调度优化	分布式文件系统构成了分布式计算的基础，是如今日益发展的大数据应用的重要支撑。充分了解和优化分布式文件系统的性能对上层用户、分布式文件系统的研究者和开发者都非常重要。在性能评测方面，现有的分布式文件系统性能评测工具大都侧重于特定功能测试，覆盖面有限，缺乏一个可扩展易用的通用化性能评测框架；在缓存调度方面，现有分布式文件系统难以根据应用数据访问模式的变化合理选择缓存策略。为此，本课题将首先研究分布式文件系统通用化性能评测的关键技术方法，拟从分布式文件系统的通用化抽象接口模式、抽象评测用例模型设计、评测用例设计与自动化生成、大规模并发评测框架四个层面，着力研究解决上述问题，最终构建一个高度可扩展的、易用的通用化分布式文件系统评测框架；然后，本课题还将针对现有分布式文件系统缓存策略调度对数据访问变化感应不足，研究提出基于机器学习模型和滑动窗口技术的自适应调度方法。
多核处理器中面向对象Cache体系结构技术研究	多核处理器;多核编译器;面向对象Cache;Cache体系结构	片上Cache体系结构对多核处理器的性能和可扩展性有着重要的影响，我们认为传统Cache的问题在于没有考虑程序中数据对象的访存特点，单纯由硬件管理，为此，我们提出一种面向对象Cache。面向对象Cache由软硬件合作管理，并能根据程序中数据对象的访存特点，按照其访存需要分配Cache资源，并配置适当的Cache参数（Cache容量、相联度、Cache Line大小等），从而更合理地利用多核处理器上的Cache资源。本课题计划研究面向对象的Cache体系结构及相关的编译支持技术，以解决多核处理器的可扩展性问题。
片上多核处理器硅后验证关键技术研究	片上多核处理器;覆盖率;调试;硅后验证	以片上多核处理器为代表的现代大规模复杂设计都面临着因各种问题导致的芯片多次流片问题，对处理器硅后验证提出了巨大的挑战。当前大规模集成电路硅后验证问题在国内外都是一个令人困扰的难题。本项目就片上多核处理器硅后验证中的科学问题及其技术方法开展深入研究，通过硅后验证可调试性设计技术、支持CMP的硅后错误重放技术、支持CMP的硅后测试向量生成技术、支持CMP的正确性检测技术、支持CMP的测试向量精简技术、硅后随机验证技术、硅后覆盖率度量技术、硅后验证提高芯片耐受性技术、以及验证状态空间裁剪技术等一系列关键技术和方法的研究，解决目前片上多核处理器硅后验证中存在的困难及面临的科学问题，并将研究成果融入和实现在验证工具平台上，所实现的工具和平台将在验证科学领域达到国际先进水平，在实践上可以满足国内低成本设计验证环境下片上多核处理器硅后验证的需求，并直接应用于国产片上多核处理器硅后验证的实践。
基于随机Petri网的大规模网络服务系统行为适配模型及方法	系统性能分析;工作流;系统建模;随机Petri网;Petri网	在资源受限和异常扰动时，如何柔性适配系统行为，提升系统应变力，保持系统稳定的良好服务状态是大规模网络系统可用性技术和理论面临的严峻挑战。本项目围绕群体行为变异原子感知和负载动态均衡的行为自适应重构两个关键科学问题：提出基于随机Petri网行为势能平衡理论与方法，探讨系统受事件扰动下群体行为变异诱发机理；以行为原子性的观点研究群体行为变异的感知方法，构建基于原子行为势能的敏捷感知随机Petri网模型与算法；以群体行为势能与系统动态负载自适应均衡分配为核心，研究系统行为与用户群体行为自适应重构方法，构建系统行为适配的随机Petri网模型及算法；最后在交通信息服务系统中进行验证与应用。项目的研究，将为大规模网络服务系统行为适配提供理论和技术支持，丰富大规模网络服务系统高可用性以及Petri网的理论与方法，为面向民生的大规模网络服务系统提供变异行为敏捷检测、工作流性能适配等应用支撑。
面向异构众核平台的交通图像车型检索并行计算模式研究	语义拓扑图;异构众核;车型检索;并行计算;交通图像	交通图像数量巨大，借助高性价比的异构众核并行计算提升车型检索性能成为亟需。项目针对交通图像的车型检索语义特点和众核计算的异构特性，研究：（1）基于交通图像混合底层特征关注度的学习策略，构建显著对象检测方法，将原始图像分解为变尺度层次化语义区域；（2）基于层次加权图的交通图像内容表示机制，分析各尺度语义区域的相对关系，构造语义拓扑图，蕴涵更丰富的可贡献车型信息量；（3）交通时空约束下的语义拓扑图的相似性评价方法，体现子图顶点属性和拓扑结构的贡献，挖掘车型信息的时空线索；（4）基于异构众核平台的大规模交通图像车型检索的并行计算方法，综合考虑交通图像中语义区域的计算依赖性、层次拓扑图访存特点、众核平台的计算/存储异构性等，建立数据分层和资源分层的合理映射；（5）面向领域用户的易用编程环境，设计众核计算的性能自适应策略，降低开发难度。
基于固态硬盘的存储系统性能分析模型研究	固态硬盘;性能分析;可靠性;耐久性	固态硬盘是一种正在改变存储系统体系结构的新型存储设备，由于固态硬盘独特的物理特点，从而导致固态硬盘的内部结构与实现算法，以及存储系统的体系结构与工作负载对固态硬盘的读写性能、使用寿命与可靠性均形成独特的影响。但是，目前仍然缺乏适用于固态硬盘的理论模型来揭示不同的系统结构对固态硬盘性能的影响。本项目拟建模分析固态硬盘内部的垃圾回收算法与损耗均衡机制对固态硬盘读写性能与使用寿命的影响，并揭示两者之间的权衡关系，旨在准确掌握固态硬盘的性能与内部结构的关系，从而指导系统设计提升固态硬盘的性能。其次，本项目拟构建适用于固态硬盘的可靠性分析模型，从而为设计高可靠的存储系统提供理论分析工具与设计指导。最后，本项目拟建模分析固态硬盘在大规模存储系统中的可适用性，揭示不同的系统结构与工作负载对系统性能的影响，从而为基于固态硬盘的存储系统提供理论分析工具，并指导设计能有效利用固态硬盘性能的大规模存储系统。
容错并行程序设计模型的研究与实现	工作窃取调度;任务并行;容错;并行程序设计	本课题在计算机体系结构并行化的发展趋势和日益突出的容错需求两方面背景下提出，研究支持容错的并行程序设计模型。该模型以任务为基本单元进行调度、执行、错误检测和恢复，通过充分发掘并行性提高系统性能和降低容错开销。主要研究内容包括：1）任务粒度的错误检测和恢复机制，拟采用一种Buffer-Commit的计算模型支持瞬时错误的容忍，采用应用级无盘检查点实现永久错误的容忍，并研究对错误频发的计算单元的弃用算法；2）分层可扩展的任务调度框架，对多核集群系统，节点内采用容错的工作窃取调度策略，节点间采用工作窃取和工作共享相结合的自适应调度策略；3）任务划分，针对不同并行模式研究不同的初始划分方法，研究并行循环和分治应用在运行时的动态划分策略，以获得最佳的负载均衡，另外，对出错任务研究一种动态分割算法。总之，本课题在并行程序设计中融入对错误容忍的支持，兼顾系统性能和可靠性两个方面。
基于动态差异化策略的高效大规模图计算系统研究	图划分;大数据处理;大规模图计算;图并行模型	许多大数据应用如社交计算、模式识别、推荐系统和自然语言处理等均可以通过大规模基于图数据结构的计算进行处理，这也对如何构建大规模的图计算系统从划分算法、编程模型和计算引擎等方面的设计与实现提出了全新的挑战。然而，现有图计算系统大多采用单一性策略，缺乏对图计算中数据、算法和负载的多样性的支持，因而对目前具有种类多和变化快等特征的图数据未能充分挖掘系统效能。本项目将研究基于动态差异化策略的高效图计算系统，拟结合离线剖视和联机抽样等手段，以及神经网络和效能建模等技术，研究高效的动态特征感知方法；同时针对不同特征的图数据、应用和负载，设计并融合不同划分算法、编程模型和计算引擎的策略，充分挖掘图计算过程中的数据局部性和计算并行性，并兼容于现有面向单一策略的优化方法。项目的研究成果将为构建高效的大规模图计算系统提供有力支持，对大数据时代的复杂关联数据的分析研究起到积极的推动作用。
面向多核处理器的硬软件协作Transactional Memory系统结构	可编程性;I/O操作;Transactional;memory;事务嵌套	随着多核处理器的迅速发展，传统并发程序难于编写和调试的问题将变得越来越突出。做为一种能有效改善可编程性的技术，Transactional memory已引起越来越多的关注，但目前尚有若干挑战性问题有待解决，如事务内I/O操作处理、事务嵌套和硬件资源限制导致的问题等。项目计划在Transactional memory中采用硬软件协作的方法，提出事务内I/O操作处理方案和更为灵活的事务嵌套模型；在此基础上形成面向通用型程序的Transactional memory系统结构；通过扩展现有系统结构模拟器建立Transactional memory模拟实验环境；在实验环境中对所提出系统结构进行评价测试。
汽车电子中分布式嵌入式系统的优化算法与设计工具	AUTOSAR;实时嵌入式系统;实时调度;Flexray;汽车电子	汽车产业在我国国民经济中占有举足轻重的地位。当今汽车中的车载电子控制系统日趋复杂，成为现代汽车的大脑和中枢神经。为了迎接汽车电子设计的挑战，本项目围绕汽车电子领域的现代行业标准AUTOSAR和FlexRay来开发一套有效实用的技术和工具，在从软件建模到最终实现的开发过程中，帮助设计者解决从应用软件模型到分布式硬件平台的映射过程中的一系列设计优化问题，包括基于FlexRay总线的分布式系统的分析与优化，任务到多处理器平台的映射优化, AUTOSAR模型的实现优化等，最终目标是在保障系统的实时性和可靠性的前提下提高开发效率与产品质量。
基于时空上下文数据的关联关系挖掘与推理技术研究	时空数据挖掘;多维度建模;事件演进;概率图模型;上下文推理	时空数据关联挖掘是对时空数据中非显性知识、时空关系等模式的自动提取。其在交通、生物、公共安全、气候、人口普查等领域等有广泛的用途。基于地理位置的时空上下文数据，是生活中最普通也是最重要的基础数据，同时也是一种复杂的异构环境应用。本项目基于多种时空轨迹数据，采用语义化的上下文描述模型，研究时空位置信息的关联关系推理方法与挖掘技术。拟从四个方面展开：1）多源时空数据的多维度表示理论与模型研究，建立时空数据及时空变化的统一语义描述模型；2）基于概率图模型的关联关系挖掘，从统计学角度阐明时空数据与时空变化的模式特征与关联关系；3）基于行为语义的上下文推理技术研究，根据挖掘出的行为模式特征，进行未知事件的推理预测；4）面向群体用户的空间关联模型研究，挖掘群体活动的行为模式以及对事件的演进路径进行分析。
基于三维集成电路的神经形态芯片互联架构研究	低功耗;片上网络;类脑芯片;三维集成;脉冲神经元	神经形态芯片由于其低能耗，可以进行异步、实时、并行和分布式数据处理的特性，以及具备自主感知、识别和学习的能力，已经引起了越来越多的关注和研究。本项目针对大规模神经形态芯片的高并发度、互联高复杂度以及低可扩展性的问题，结合三维集成电路建模、三维集成电路设计、以及三维片上网络技术，开展基于三维集成的神经形态芯片架构研究，使得神经形态芯片在达到高带宽的同时保持低功耗。本项目的主要研究内容包括：(1)建立硅穿孔的电特性模型，考虑利用硅穿孔大电容作为脉冲神经元的可能性；(2)利用三维路由构建三维片上网络，为大规模神经形态芯片通信提供方案；(3)针对一到两个典型神经网络应用提出应用到芯片的映射机制并提供相应的性能功能仿真验证方案。通过以上三个方面的研究，本项目的预期成果是提出基于三维集成的神经形态芯片中片上网络架构，为后续的高性能低功耗的大规模神经形态系统奠定良好的基础。
针对视频分析的高能效异构硬件计算系统研究	高能效;可重构计算;视频分析;异构硬件计算;忆阻器	视频数据是大数据的主要来源，当今计算系统并不理想的能效水平是视频数据分析的根本瓶颈。在半导体行业的摩尔定律已无法有效降低数字系统的能耗水平的背景下，根据应用和算法特点，在体系结构和电路层面，结合新器件，提出优化相应计算系统能效的通用方法是亟待解决的科学问题。本项目将以跨层的思路，对"算法-硬件结构-电路-器件模型"四个层面进行联合优化，针对视频数据分析应用领域的共性，探索使用新器件及硬件定制等方法，设计出在视频分析领域具有一定通用性的高能效异构硬件计算系统，并突破基于忆阻器的高能效多层神经网络构建、神经网络训练的硬件加速、模拟计算单元的高效连接方式、针对视频分析算法的高能效结构映射、数模混合系统的仿真测试等一系列关键技术，将完整系统的能效水平提高2个数量级，解决视频大数据处理的能效难题。
大规模文件系统中元数据高效访问方法研究	元数据访问;文件查找;文件搜索;大规模文件系统;文件创建	大规模文件系统是应对大数据存储需求的重要技术途径之一，其元数据的访问方法是当前国内外研究热点。然而，现有研究难以满足大规模文件系统在大规模目录树结构、大规模文件数量、密集型元数据访问、并发多类型负载等方面的新挑战：文件查找、搜索、创建等典型操作中因文件系统规模扩展导致的缓存空间利用率降低、文件搜索空间增大、元数据访问局部性缺失、并发访问资源竞争加剧等元数据访问性能问题。本项目以提高大规模文件系统中元数据访问性能为目标，以元数据操作的I/O路径为主线，研究元数据高效访问的新方法：基于目录子树分区和反向链表缓存结构的文件查找方法、基于目录级多维可伸缩Bloom Filter的文件并行搜索方法、基于多阶段提交协议和元数据聚合机制的文件并行创建方法以及支持元数据服务质量的元数据访问性能保障方法。项目研究工作对提高大规模文件系统整体性能和可扩展能力、促进大数据领域技术创新具有重要的理论意义和应用价值
超高清视频处理加速器研究	视频处理加速器;面向应用的定制处理器;并行计算;众核体系结构;超高清视频	近年来，超高清视频已逐渐成为数字电影、数字家庭、云平台、虚拟现实、多媒体舞台/展馆等多媒体应用的主流。超高清视频处理对硬件平台性能和能效需求非常高，必须通过有效的并行来满足。各种各样的超高清视频处理应用内在的并行性呈现出显著的多层次、多元化、多变性特点，而现有的处理器（如CPU、DSP和GPGPU）提供的硬件并行度是固化的。多变的应用并行度和固化的硬件并行度之间存在巨大的鸿沟，导致各种现有处理器处理超高清视频时效率低下。.综合对体系结构和应用发展趋势的分析，我们认为，硬件并行度动态可重构的加速器才能有效地跨越多变的应用并行度和固化的硬件并行度之间存在的巨大鸿沟。因此，本项目将深入探索多层次并行度动态可变的加速器结构，以及可变并行度下的计算重用和访存重用的原理和方法。基于这一系列研究，我们将提供一种能应对多种超高清视频应用场景的高性能低能耗的加速器参考结构，为国内外同行提供借鉴。
基于嵌入式3D堆叠闪存存储系统的可靠性关键技术研究	嵌入式系统;闪存;存储系统;性能优化	随着计算机对存储设备容量和密度需求的不断提高，闪存作为最为广泛使用的存储器件之一，目前已经进入3D堆叠的发展阶段。然而，3D闪存却存在多种数据可靠性问题，阻碍了它的进一步发展。不同于2D闪存可靠性特征，3D闪存采用纵向堆叠和基于电荷陷阱的绝缘体构建，导致其在读写和数据保存过程中出现严重的电子干扰和电子泄露问题。随着尺寸和堆叠层次的不断发展，其可靠性也将进一步恶化。改善嵌入式3D闪存可靠性成为其发展过程中至关重要的研究课题。本项目将从3D闪存的操作和结构特征等方面展开可靠性优化研究。首先，根据3D闪存的可靠性影响因子特征分析，构建3D闪存可靠性模型，为其可靠性优化提供基础框架。然后在此基础上从两个方面展开可靠性优化：精确数据的可靠优化技术和近似数据的存储优化技术，最终达到存储的性能和可靠性优化的目标。本项目的成果将为3D闪存的进一步发展提供关键技术，为各类闪存设备的广泛应用提供技术支撑。
物联网应用系统控制安全技术研究	广义污点传播;程序级信息流控制;形式化验证;最小权限原则	保障物联网应用系统的控制安全是物联网得以大范围推广应用的关键问题之一。物联网中存储着大量的隐私信息，很多应用通过预设命令直接对物品进行智能控制。控制安全一旦失效，将会导致隐私信息泄密、应用行为失控、甚至整个应用被破坏。物联网控制安全风险的根源在于缺乏有效的以数据安全为中心的保护技术。已有的方法基本上都是以加密为主要手段开展研究，投入量大但效率低。主要原因是单个RFID标签能力无法支持复杂的密码学计算，而且加密的隐私信息难以适用于多种应用。本项目的总体研究目标是：研发有效且实用的物联网数据的隐私性和完整性保护技术，实现物联网应用系统的控制安全。拟重点研究如下内容：①研究以数据安全为中心的程序级信息流控制技术，实现分布式多种不同程序间的信息的细粒度传播控制；②研究支持最小权限的广义污点传播控制模型和技术，消除隐蔽通道；③研究系统级信息流控制模型的形式化验证方法，从理论上保障控制安全的有效性。
引入蒙特卡罗模型的闪存转化层管理算法研究	固态存储器;固态存储转换层;闪存管理	固态存储，具有非挥发性、快速读/写、低功耗及易携带等优点，已经成为一种重要的存储媒质。在固态存储核心技术中，闪存转化层管理算法是影响固态盘性能高低的关键所在。随着闪存芯片技术的发展，现有的闪存转化层管理算法存在：有限缓存空间下，固态盘读写性能差；损耗均衡时读写频繁的数据被频繁交换；系统开销影响垃圾回收效率等问题。本课题根据闪存颗粒的特征属性，通过深入研究闪存转化层管理算法的映射方式，均衡和垃圾回收过程，引入蒙特卡罗法进行短时数据块活跃度参数建模和垃圾块产生概率的计算，拟采用描述地址映射效率的多目标优化函数及数值求解方法，建立基于数据块活跃度模型的新型磨损均衡算法，设计依据I/O处理能力的动态垃圾回收策略，完善闪存转化层管理算法，提高固态存储盘的读写性能，延长使用寿命，提升我国在固态存储领域的核心竞争力，为固态硬盘国产化做好基础理论研究。
P2P存储系统中可用性、可靠性和安全性问题研究	安全性;可用性;可靠性;持久性存储;P2P存储	P2P技术给互联网带来了无限的生机和活力，P2P共享和P2P视频点播直播系统的成功，展示了P2P应用的光明前景。但是由于P2P系统动态性高的特点，使得存储应用的可用性、可靠性受到了制约，同时由于P2P系统的松散的管理机制，也凸显出存储系统的安全问题。我们认为，可用性、可靠性和安全性是构建P2P存储系统的关键问题，具有挑战性，但不是不可解决。问题的根本在要针对动态性，提出相应的解决方案，使P2P存储系统既能充分发挥P2P技术高可扩展性、高容错性以及自组织、自适应的优势，又能实现高可靠、高可用的存储保证。本申请将从研究动态性的规律入手，研究新的保证高可用性和高可靠性的方法。包括研究节点上下线规律，更准确计算可用性；利用这个规律，构成相关用户组，采用同组发策略，提高可用性；研究采用时间函数代替平均值的冗余度计算方法，更准确的计算副本数，降低成本，提高效率。
面向非易失存储器的一站式存储访问模型及技术研究	存储架构;对象存储;非易失性存储器;访问模型	新型非易失性存储器因其既具有接近内存的读写性能又具有类似磁盘的持久存储特性而被视为解决"存储墙"问题的一大利器，但传统的基于慢速磁盘和基于非持久化内存的存储访问、管理模式均不适合新型非易失性存储器。前者存储访问路径过于冗长，软件开销巨大，后者不支持存储持久化，而且不易屏蔽当前非易失性存储器的某些缺陷。本项目针对新型非易失性存储器，研究构建面向新型非易失性存储器的基于对象的一站式存储访问模型与技术。项目采用基于对象的存储访问与管理接口，实现存储访问与存储管理路径分离；通过软硬件结合的方式实现对非易失性存储器一站式存储访问；并通过对象存储接口屏蔽非易失性存储器的缺陷，在对象内部实现诸如磨损均衡、读写不对称屏蔽、并行调度等基于硬件的优化策略。这些方法和技术可缩短存储路径，同时掩盖了非易失性存储器的缺陷，充分发挥其性能优势，实现对其快速高效地存储访问，以期打破"存储墙"，释放系统潜能
层次式集成电路知识产权（IP）保护指纹方法研究	集成电路设计;约束传播;指纹;知识产权保护;跨层次检测	集成电路的发展受知识产权（IP）盗版的制约使得IP保护技术成为当今研究热点。目前的IP保护指纹方法只集中在VLSI设计的单一层面，且存在跨层次检测困难、安全性不高等问题。本课题在IP保护的前期研究基础上，将最优化理论、IP设计求解、复杂网络理论结合在一起，提出基于约束传播的层次式IP保护指纹技术方法。主要研究内容包括：（1）研究VLSI设计的最优化模型，建立指纹保护的约束层次；（2）建立"约束综合-最优化"的指纹设计求解模型，以及抗共谋攻击能力的指纹设计分发模型；（3）基于IP设计的复杂网络特性，提出指纹的跨层次检测方法;(4）研究提高VLSI设计层次间的指纹传播方法，实现IP核的层次式指纹保护。提出的层次式指纹方法既能保证IP设计的性能,又能提高IP保护的安全性。层次式的指纹方法可透明地应用于VLSI的EDA设计流程，成果将为最优化理论及复杂网络理论在VLSI设计的应用提供借鉴。
匿名性变换基础问题及其轻量级应用系统设计	隐私性;选择密文攻击;轻量级设计;混合机制;匿名性	计算机通信系统中，匿名性探讨在一组对象中不被识别的状态，为通信实体的个人隐私和匿名通信提供"无记名"的匿名保护。针对学界关于匿名性研究普遍基于具有匿名性的特定方案具体构造的现状，本课题研究匿名性转换基础问题：（1）研究通信用户匿名性与通信数据不可区分性两个安全属性的内在机理，揭示特定可验证的条件或性质，具有密文不可区分性的算法如满足该性质则必定满足匿名性，反之类似；（2）将该基础问题研究拓展至签密的匿名性以匹配现实系统的实际应用，细化涵盖不同通信环境的匿名性安全模型(密钥不可区分性、密钥不可见性等)，设计具有匿名性的轻量级签密及签密混合体制以适用于资源受限的应用场景。结合课题组在安全算法与基础理论的前期研究积累，本课题可取得关于匿名性转换基础问题实质性研究成果，进一步揭示匿名性与作为安全性评估"黄金标准"的不可区分性间的内在关系和设计规律，组织符合特定计算环境特定安全需求的计算系统。
异构众核平台CFD高效预条件JFNK并行求解算法及应用	非线性求解方法;并行算法;Newton-Krylov方法;预条件子;异构众核体系结构	航空航天等CFD数值模拟重要应用领域经常遇到复杂非线性流动问题，迫切需要发展高效非线性求解方法。JFNK(Jacobian-Free Newton-Krylov)方法是近年来发展迅速的非线性求解算法，其计算、存储、通信特点尤其适合于高效大规模并行，在CFD中已得到了初步应用和广泛关注。本项目结合气动CFD数值模拟和新型高性能众核并行架构特点，开展JFNK算法及其大规模并行计算研究，重点围绕三维可压缩湍流模拟的JFNK数值模型、高效预条件技术、新型众核架构的JFNK并行算法及其在高精度数值模拟中的应用等问题开展研究。本项目研究不仅能够提升我国众核高性能计算机上的气动CFD应用水平，对于促进流体力学与计算机科学的深度交叉融合也具有重要意义。
可扩展到Exaflops的超级计算机的算法容错技术研究	算法容错;容错计算;超级计算机;可扩展性;科学计算	为了达到更高的性能，超级计算机的规模越来越大，而整个系统的MTTF越来越短。传统的超级计算机容错主要基于checkpoint的模式。这种容错模式的效率和系统的规模是相关的，在现有的技术趋势下会随着处理器个数的增加而降低。为了扩展到Exaflops的规模，系统中处理器的个数将可能达到百万的量级，系统级的容错方法将很难获得高效率，在应用层级进行容错已经成为一种切实的需求。.算法容错是一种应用层级的容错技术，虽然其概念提出来的较早，但是由于实际需求不足，在高性能计算机系统中的应用还缺少深入的研究。将算法容错应用于可扩展到Exaflops规模的高性能计算，还有以下关键的问题需要解决：有效的冗余编码，并行算法流程的重新组织及设计，实现效率及可扩展性等。本项目研究算法容错技术在高性能科学计算领域的应用及实现，项目研究的成果能够为构建面向Exaflops的高性能计算机提供理论基础及关键技术。
HDFS读、写性能概率建模与模型迁移方法研究	概率建模;Hadoop分布式文件系统;性能建模;迁移学习	HDFS性能建模是云计算领域研究热点之一。HDFS性能具有独特的概率分布特征，蕴含着有用的信息。但当前尚未开展HDFS性能概率建模研究；并且HDFS性能建模主要使用单一建模方法，性能模型迁移方法的研究处在起步阶段。为此，本项目研究一种"实验建模、分析建模与迁移学习结合"的HDFS读、写性能概率建模方法。.首先，基于HDFS工作机理，对文件大小域(0, BS]范围的文件大小（BS代表块长），采用实验建模，提出基于"特征指标值预测-概率密度还原"的性能概率密度函数估计方法；对文件大小域(BS, +∞)的文件大小，采用分析建模，提出基于概率密度叠加的性能建模方法。其次，对新平台的HDFS性能概率建模，提出基于实例迁移的HDFS性能概率建模方法。.以上方法能够建立不同平台HDFS读、写性能在文件大小域的概率模型，减少建模成本、提高建模效率，对其它数据密集型文件系统性能建模具有借鉴意义。
以汽车为例的CPS若干问题研究	嵌入式虚拟机;任务调度;CPS;实时模型;实时分析	新一代汽车系统是一个复杂的CPS（Automotive Cyber-Physical Systems，简称ACPS）。以ACPS的观点研究汽车，研发新一代汽车电子系统，是当前嵌入式计算领域研究的热点之一。本项目旨在研究ACPS中的若干关键理论和技术，以实时性和安全可靠性为主线，从模型、设计技术、分析与验证方法等层面开展相应研究，主要内容包括面向ACPS的实时模型、实时分析理论和方法，面向ACPS的嵌入式虚拟机，基于嵌入式虚拟机结构的动态任务分配和任务调度方法，以ACPS为例的大规模CPS构建技术等。通过本项目的研究，将解决以电动汽车为例的一类时间和性能关键的CPS系统中若干理论和关键技术问题，发展嵌入式计算理论与方法；研发新一代汽车电子系统原型，对推动我国汽车电子工业的发展，提升汽车电子自主创新能力，提高我国在汽车电子领域的技术实力和核心竞争力具有重要意义。
宽带频谱压缩感知与自适应分配算法	压缩感知;频谱分配;循环平稳;认知无线电;频谱感知	频谱感知与分配是认知无线电技术中关键问题。压缩感知是一种稀疏或可压缩信号的信号重建技术。由于认知无线电所分析是宽带，稀疏的频谱信号，因此本项目拟在压缩感知框架下，研究认知无线电中的宽带频谱感知与频谱分配问题，希望在频谱感知与频谱分配方面有所创新。具体研究内容与目标是：研究单用户宽带频谱感知技术，拟提出对噪声鲁棒、考虑信号稀疏特性的压缩宽带频谱检测算法；研究两种稀疏信号模型下的分布式合作宽带频谱感知技术，拟提出共同稀疏信号模型下的基于交替方向乘子法与Consensus技术的分布式合作频谱感知算法及考虑网络中主用户与认知用户共同传输时的分布式合作频谱感知算法，有望提高频谱感知算法对网络失效的鲁棒性并降低通信与计算代价；研究基于压缩感知与循环平稳的宽带频谱感知算法，有望降低频谱感知的计算开销；研究波形生成技术以及宽带频谱分配问题，拟提出灵活快速的自适应波形生成及基于压缩感知的动态频谱分配算法。
面向众核处理器的片上高速光互连网络体系结构关键技术研究	光互连;低功耗设计;片上网络;众核处理器;cache一致性协议	众核处理器内部互连规模不断增加给传统片上互连网络带来了严峻挑战，片上光互连技术有效克服了传统金属互连的诸多不足，给更大规模片内互连通信结构提供了一条崭新技术途径。但如何结合片上光互连技术优势，设计适应更多核心互连通信需求的片上光互连网络，仍然面临着高性能、可扩展、低功耗、高适用等方面的严峻挑战。本项目将提出一种面向众核处理器的片上光互连网络体系结构，并深入研究其设计实现关键技术，主要包括：基于全光互连、光电混合互连的高性能可扩展片上光互连网络体系结构；互连结构、网络协议、任务调度、模块单元设计等多个层面的片上光互连网络低功耗设计技术；面向监听协议与目录协议的片上光互连网络缓冲一致性协议优化技术；片上光互连网络性能功耗分析模型及硬件设计资源优化方法等关键技术。本项目研究可为未来我国众核处理器片内互连结构设计和实现提供坚实的理论和技术基础，具有重要的理论与应用价值。
基于综合信任的边缘计算资源管理与协同优化研究	边缘计算;物联网;分布式计算;信任管理;体验质量	边缘计算旨在共享利用边缘设备的计算、通信资源，满足人们对服务的实时响应、隐私与安全、计算自主性等需求，随物联网的发展将应用前景广阔。项目以用户为中心，以用户体验质量（QoE）为目标，以信任评估为保障对边缘计算资源管理与协同优化问题展开研究。针对终端的动态性、边缘设施能力受限、边缘与终端的邻近性、云中心功能强和距离远的特征，项目研究融合云计算、P2P计算、C/S与网格计算模式，构建多层自适应的统一计算模型，实现对应用场景动态匹配；研究以用户体验质量为目标的综合资源/用户信任评估体系与模型，实现资源QoS向QoE的指标映射，构建资源和用户的身份信任、行为信任评价机制，形成综合信任评估体系与模型；根据应用需求，研究面向计算能力、移动性与可用服务时间、剩余能量、带宽等多重约束的边缘计算的任务卸载、资源调度算法和优化方案，实现资源在终端、边缘、云中心三层级可信共享和优化利用，更好满足用户QoE需求。
实时嵌入式系统能耗有效性分析与调度技术研究	实时嵌入式系统;能耗有效性;系统芯片;最坏执行时间	实时嵌入式系统的应用日益广泛；由于依靠电池驱动、严格的时效性限制和保守设计原则，其能耗有效性研究同时具有必要性、挑战性和比其它嵌入式系统更高的收益潜能。目前的研究集中于实时调度方面，依赖运行时刻信息，以某种回收技术发掘降低能耗的机会；这些方法在复杂性和有效性上的缺点使它们缺乏良好的实用价值。本项目从影响实时调度的关键信息――任务的最坏执行时间WCET出发，提出一种新型WCET分析技术，将传统静态分析和任务就绪时刻动态分析相结合，为实时调度提供更准确的时间估计。在此基础上，我们研究一些复杂度低、有效性高的调度算法。另外，目前的研究侧重于处理器能耗方面，本项目将关注系统芯片中其他部件的能耗；首先建立部件级和系统级能耗模型，然后将WCET分析范围从运行时间拓展到部件实时行为特征，以这两点为基础，我们研究系统级能耗有效性调度。本项目将采用真实任务集和实测方法，以进一步提高研究的实用性。
并行文件系统中大规模I/O服务器间负载平衡方法研究	大规模I/O服务器;负载平衡;并行文件系统	大规模I/O服务器间的并行访问是并行文件系统重要的性能瓶颈之一,其负载平衡是当前国内外高效能计算机领域的重点研究课题。现有相关研究主要是从负载建模和负载平衡算法等方面来开展。针对现有研究存在的难以适应高效能计算机的"I/O服务器数量剧增"和"复杂的I/O访问模式"等新需求、平衡算法的适应性等问题，通过深入分析典型高性能计算应用和高效能计算机系统的体系结构，基于负载平衡基本处理流程，研究并行文件系统中I/O服务器间负载平衡新方法：提炼服务器间I/O访问模式，建立面向大规模I/O服务器间的负载模型、分布式平衡算法、负载预测算法和文件动态迁移算法并研制原型系统。从并行I/O性能优化理论入手、理论与实践互相促进、以实践为目标，探讨了增强、改进和创新并行文件系统性能优化的新理论和新方法。对并行I/O瓶颈缓解、高效能计算机整机性能和企业高效能计算能力提高都具有重要理论研究和应用价值。
片上网络虚拟化关键技术研究	虚拟化;片上网络;众核处理器;单芯片云计算机	芯片特征尺寸的不断减小使得在单芯片内构建集群成为可能。然而，由于不同应用程序在片上网络内的相互干扰，导致无法保证用户的服务质量。片上网络虚拟化技术通过将应用分配到相互隔离的子网内，可以部分地解决这个问题。然而，目前该技术还并不完善。首先，其不支持正交凸互补子网,从而导致系统资源利用率低。其次，每个子网内使用相同的、针对最坏情况设计的路由算法,从而导致系统始终运行在次优状态。造成以上两个问题的原因是缺少一个针对正交凸互补网络的路由死锁避免机制。这个机制不但可以保证正交凸互补网络的路由无死锁，并且可以使得路由具有重构能力。可重构路由可以利用应用的流量特性，动态调整子网内的路由规则，提高网络性能。另外,可重构路由算法依赖于一种高效的路由重构机制,以保证路由重构不会降低网络性能。为了达到以上目标，本课题拟针对正交凸互补网络的路由死锁避免机制，相应的可重构路由算法和高效的路由重构算法展开研究。
面向E级计算的纠删码机群文件系统研究	E级计算;机群文件系统;纠删码	随着E级超级计算机的到来，高性能计算机的存储系统在可靠性和可扩展性等方面面临巨大的挑战。机群文件系统在提供高可靠的数据存储的同时还需提高数据的读写性能。采用纠删码的数据容错方法具备高效的存储空间利用率，能够有效降低海量数据存储的维护成本。然而数据在编码和解码过程的高计算开销和高磁盘访问开销限制了纠删码技术在高性能计算领域的运用。为了提高基于纠删码的机群文件系统的数据读写性能，本项目将在降低纠删码存储的恢复开销、优化基于处理器SIMD指集令的数据编码方法以及降低数据更新等方面进行深入研究。为评价本项目研究工作的价值，我们会将研究结果实现于纠删码机群文件系统上，并通过测试机群文件系统在数据存储和数据恢复等方面的性能作为评价该项目研究价值的重要依据。
面向多源空间相关大数据的数据清洗系统关键技术研究	清洗算法并行化;大数据清洗系统;缺失数据填充;任务合并与调度;函数依赖发现	大数据已成为信息社会的重要财富，以环境大数据为代表的多源空间相关大数据的研究能支撑民生改善和推进可持续发展，但数据质量问题极大降低了多源空间相关大数据的可用性。因此，为清洗该类大数据构建数据清洗系统成为亟待解决的问题。本项目面向多源空间相关大数据，针对其特点：数据规模庞大、数据多源异构、数据间存在时空源间相关性，研究多源空间相关大数据清洗系统的关键技术，解决大数据清洗算法并行化设计以及清洗系统运行效率不高的问题。首先，通过分析多源空间相关大数据时间、空间以及数据源间特征，为后续研究提供基础；然后，研究基于依赖规则集迁移的大数据函数依赖发现方法和基于分类划分的缺失数据填充算法并行化，以此完成大数据清洗系统核心构建；最后，通过任务合并、任务调度与副本放置策略优化大数据清洗系统性能。本项目为多源空间相关大数据质量与可用性提升，以及后续大数据分析提供工具支撑；为专题大数据清洗系统的设计提供借鉴。
先进微处理器软错误易感性动态预测和免疫技术研究	动态预测;可靠性;软错误易感性;阶段特性;软错误免疫	现代先进微处理器中由于采用超深亚微米工艺技术，使得辐照和干扰引发的软错误日益增多，造成的可靠性降低问题已严重限制微处理器的发展和应用。本项目针对微处理器的软错误易感性和免疫性进行研究，提出了预测技术、免疫技术和保护技术相结合的微处理器可靠性设计的新思路和新方法。主要研究微处理器软错误易感性估算模型与评估框架，在多个层次感知和利用微处理器软错误易感性的阶段特性，研究微处理器软错误易感性动态预测算法和实现技术，提出微处理器软错误免疫技术和结构。通过本项目的研究，建立微处理器软错误易感性动态自适应调整机制和免疫结构，以较小的代价缓解超深亚微米工艺水平下先进微处理器的软错误问题。软错误是现代微处理器在超深亚微米工艺条件下变得严重的问题，是本领域国内外的前沿研究课题，结合高性能微处理器设计需求进行研究，我们一定可以取得较好的创新性成果。
面向物联网环境的大规模可扩展网络管理研究	物联网;报文压缩;网络管理;信息模型映射与融合;轻量级可编程协议	物联网是继PC、互联网、无线通信网之后第四次信息技术革命，有重大的科学意义和应用价值。对于物联网这样一个庞大而复杂的网络系统，必须有一个可靠、有效、灵活且便利的管理系统作为它正常运行的保障，但目前还没有形成一个统一、完善的物联网管理模式和框架。本项目研究适用于物联网环境的大规模可扩展网络管理架构，并基于此架构实现支持多协议的被管网元数据采集、异构网络管理协议之间的互操作、域间管理节点消息报文的压缩与解码。并对基于本体的网络管理信息模型映射与融合技术、网络管理策略一致性保障机制、多协议消息报文的转换方法进行深入研究，以实现基于本体的网络管理服务中间件。同时为管理物联网环境中的大量传感设备及无线网络设备，提出一种具有分布式数据融合机制的轻量级可编程网络管理协议。本项目研究的网络管理技术适应物联网的大规模性、协议多样性、设备异构性以及高动态特性，为物联网的运行和维护提供支撑和保障。
大规模异构计算系统高时效协同功耗管理方法研究	功耗均衡;协同功耗管理;可扩展性;异构计算;高效能	能耗问题已经成为限制现有计算系统性能发挥的主要因素之一。异构计算系统具备显著提高计算系统能效比1～2个数量级的潜力，但现有的功耗管理方法对这类计算系统显示出明显的时效性和协同性的局限，使得这类计算系统的能效潜力得不到充分发挥。鉴于此，我们提出基于均衡理论构建分布式协同功耗管理的方法来克服现有方法的局限性。本项目拟研究1）异构计算系统的低计算复杂度能效模型，2）基于局部均衡的博弈模型，3）全局均衡的分布式协同分配算法，4）基于片上网络的分布式管理信息传递方法及硬件支持，5）高响应速度的硬件支持等五方面的内容，分别从理论方法和硬件支撑两大方面来研究高时效性和协同性的功耗管理方法，最后落实到FPGA原型系统验证。我们期望在本项目的支持下，为未来可用功耗受限的大规模异构芯片的功耗优化提供新的参考依据。这些工作可望带来一系列计算机系统结构领域原创性的高水平研究成果。
网络安全应用中高性能特征匹配体系结构研究	特征匹配;专用体系结构;网络入侵检测系统;网络安全	基于深包检测技术的内容级网络安全解决方案是信息安全发展的重要方向之一。其中，特征匹配技术是深包检测技术的核心，也是这类安全方案的重要性能瓶颈。本研究拟采用计算机体系结构的方法，设计高性能特征匹配体系结构，满足千兆、甚至万兆网络中安全应用对特征匹配的计算需求。主要研究内容包括支持大规模特征集的字符串匹配算法和体系结构、支持大量正则表达式并行匹配的算法和体系结构、优化的系统调用方法等。我们拟从基础理论入手，设计用少量存储资源支持大规模特征集的算法，并根据优化算法设计多网络流并行工作的高吞吐率特征匹配结构。对于提出的体系结构，采用EDA工具、软件模拟器和FPGA平台等方式进行实验分析与评价，并结合建模分析方法探讨该问题解决方案的理论界限。
结合逻辑与物理I/O访问信息的存储系统优化策略的研究	动态数据优化分布;存储系统;I/O访问模式;数据预取;大数据	本项目拟研究主动式存储系统的I/O优化策略，以克服当前主动式存储系统模型的不足。通过捕获应用程序的逻辑I/O访问信息和其在存储服务器上的物理I/O访问信息，采用有向无环图（DAG, Directed Acyclic Graph）记录、合并优化和管理所有I/O事件；同时构建I/O事件之间的依赖关系。通过建立应用程序的逻辑I/O访问和物理I/O访问之间的映射联系，利用基于概率计算的预测模型预测并分析逻辑和物理I/O访问模式，结合I/O事件的依赖关系，为以并行文件系统为基础设施的主动式存储系统准确地执行数据预取和数据物理分布的优化操作提供基本决策信息，从而提高I/O子系统的吞吐率并减少应用程序本身的执行时间，最终达到提高系统整体性能的目标。
稀疏信标点无线传感网分布式免测距定位方法研究	稀疏信标点;传感器网络;分布式计算;分布式系统;免测距	分布式免测距定位技术凭借低成本和高鲁棒性，成为稀疏信标点无线传感器网络的首选定位方法。本项目针对传统免测距定位方法在定位精确度、复杂度和定位信息碰撞等方面的问题，拟探索一种适用于稀疏信标点无线传感网的分布式免测距定位方法。其核心思想是通过最大化利用节点之间网络连接信息，建立并求解最优化动态博弈模型，从而有效提高定位精确度；通过以网络连接和传输可靠性为依据的自适应优化方法，在兼顾精确度的同时降低算法复杂度；通过动态调整定位信息发送时间的分布式媒质接入方法，有效减少定位信息碰撞。研究内容主要包括可逼近免测距定位误差理论下界的分布式定位算法、计算复杂度和网络复杂度的优化方法以及定位信息碰撞避免方法的研究。本项目是对分布式定位理论与技术研究的进一步深化，为解决稀疏信标点无线传感网分布式定位问题探索一种新的研究手段。分布式免测距定位方法研究成果将在环保、消防、矿井等目标定位领域具有广泛的应用前景。
二元域上低密度系统阵列码的构造及其高效解码方法	存储系统;容错;纠删码;阵列码	纠删编码是一种能够兼顾存储效率和可靠性的数据保护策略，是计算机存储领域的重要研究课题之一。阵列码是一种编解码过程只需用到异或和循环移位运算的高效纠删码，已被广泛用于对抗存储系统中的硬盘/节点故障。然而，现有的各种阵列码在应用到存储系统中时均有各自的局限性，如容错能力较低、计算或I/O开销较大、对码长有严格的限制等。针对这些不足，本项目将研究二元域上的实用的低密度系统阵列码的构造及其高效解码方法。首先，在前期研究成果的基础上构造高容错的低密度系统阵列码；然后，结合使用水平校验与垂直校验，并利用阵列码与汉密尔顿拉丁方及完全图的完美1因子分解（P1F）之间的内在联系，对阵列码的码长进行无损扩展；最后针对不同的故障模式设计相应的解码算法以尽量降低解码过程中的计算和I/O开销。本项目的研究成果将有助于以最小的存储、计算和通信开销来提高存储系统的可靠性，同时将推动著名的完全图的P1F猜想的研究进展。
分布交换式互连系统的有界活性在线测试方法研究	实时系统;形式化方法;时间自动机;在线测试;交换式互连	实时系统的延迟界限可以被表达为时间自动机（timed automata）模型的有界活性（bounded liveness）。为了解决有界活性在线（on-line）测试对于分布式状态和并发行为的观测能力，使之具备硬件在回路（hardware in the loop）环境中实时性能指标的测试能力，本项目针对安全关键性或任务关键性分布交换式互连系统，研究基于测试自动机的分布式在线测试原理，提出适用于分布式系统的互操作性测试验证方法，提出适用于运行时间（run-time）并发时序的监视方法，制定并完善验证、测试和监视自动化的框架方案；预期在 时间自动机与性能分析联合建模、交互环境模拟与模型状态观测、形式化模型与工作状态模型的运行时间等价 等方面取得关键性技术突破和新进展，开拓分布交换式互连系统有界活性在线检测的应用基础，为飞机、舰船等尖端嵌入式平台内部的实时通信与控制的综合化提供性能保证支持。
条件BC网络上独立生成树及其性质的研究	互连网络;条件可扩点集划分;条件BC网络;独立生成树;顶点等价类	独立生成树（IST）在提高互连网络数据传输的可靠性和效率方面具有重要的作用，但在一般网络上独立生成树的存在性问题迄今仍是一个猜想。BC网络包含超立方体及其若干个性质优越的变型，但除超立方体和局部扭立方体之外，其它BC网络上IST的存在性和构造问题仍然没有得到解决，主要表现在：现有方法不适用于所有现存的超立方体的变型并且缺乏IST性质（高度与同构性）方面的研究；在超立方体及其变型上缺乏有关IST应用的研究。本项目的研究内容包括：研究几种特殊BC网络上的IST的存在性及构造问题；总结这些构造方法的共性，提出包含超立方体及其所有现存变型的一类BC网络- - 条件BC网络的定义，给出在条件BC网络上IST的存在性证明和相应的构造算法；改进上述算法，使所得到的IST的高度和结构得到优化；将由优化构造方法得到的特殊BC网络上的IST应用到结构化P2P系统中，并与文献中的相关P2P系统的运行性能进行比较。
多核/众核体系结构下的可靠性研究	计算机体系结构;可靠性;多核/众核处理器	计算机系统可靠性研究长期以来都是十分受到关注的研究课题。随着微处理器和嵌入式芯片的广泛应用，提高以这些处理器芯片为中心的计算机系统的可靠性成为越来越重要的问题。另一方面，随着芯片制造工艺的发展，高密度的晶体管集成也加速着芯片可靠性的降低。当处理器的体系结构从单核时代进入多核/众核时代，对于基于传统体系机构的可靠性研究已经无法解决新一代体系结构所面临的问题，因此针对多核/众核体系结构的可靠性研究已经成为一个非常迫切而重要的课题。本课题将通过对于多处理器系统的可靠性研究，建立一套评估多处理器系统可靠性的模型与标准，并基于提出的模型和标准对现有多处理系统的可靠性进行评估，进而提出新的针对于多核/众核体系结构的可靠性优化设计，以提高未来计算机系统的可靠性。
"非对称多通道"异质、异构内存系统架构及"启发式"混合内存资源管理机制的研究	混合内存;新型非易失系统内存	"大数据"时代对内存储系统的材质和架构提出了新的要求。STT-MRAM被业界所普遍看好。为了设计包含STT的高性能内存体系，本研究提出采用"非对称多通道"架构"水平"整合STT和DRAM，并拟研究支持该混合存储架构的操作系统"启发式"内存资源管理机制。本研究拟从存储架构、操作系统两个层次形成系统的解决方案，为未来研究异质、异构混合内存资源的有效管理提供一些探索和参考。研究内容包括：.（1）研究整合异质、异构内存资源的内存体系结构，包括资源组织、数据通路、带宽、地址映射等核心机制；.（2）研究支持混合存储的操作系统核心机制，包括地址空间、数据组织、数据映射、资源分配、回收、数据迁移等机制；.（3）针对混合存储的特殊内存架构，研究存储介质特征与访问特点敏感的"启发式"资源管理机制与分配策略决策机制，包括在线的内核级访存行为抽象的采样与学习机制、资源分配的决策机制等。
近似计算中基于概率图模型的软错误量化方法研究	软错误量化;概率图模型;边界模型;多位翻转;近似计算	随着工艺尺寸的缩小，更高的软错误率和复杂的多位翻转对处理器设计带来的挑战日益严峻。同时，近似计算作为节省能耗的有效手段，也成为处理器极具前瞻性的研究方向。但是，面向近似计算的软错误量化方法尚存在空白，因此针对当前处理器近似计算中固有精度可损特性和冗余执行的在软错误分析方法中识别难题，课题拟提出基于概率图的近似量化方法，包括：1）构建基于图参数快速评估的贝叶斯网络，在适应近似计算精度可损的同时大大提高量化速度；2）制定概率图中节点参数修正策略，以满足近似计算中冗余结构的精确评估；3）设计直方图的边界分析模型，近似扩展概率图模型方法的评估结果，解决多位翻转的软错误量化难题。同时，课题拟在开源的处理器平台实现近似的概率图量化方法，并对近似计算特有的测试程序进行验证和评估。总之，课题拟研究近似概率图模型方法及其完整的实现和验证,进而为近似计算的可靠性设计提供有力的技术、方法和工具支持。
大规模几何建模和自适应笛卡尔网格生成并行算法研究	几何建模;自适应网格生成;实体建模;并行计算	在计算电磁学、计算流体力学等诸多领域，随着实际工程计算中几何外形和物理特性越来越复杂，几何建模和网格生成这两个前处理组成部分花费的时间越来越长。前处理已经成为大规模实际工程计算中的主要性能瓶颈，而并行和自适应计算是最有可能的解决途径。本项目将针对笛卡尔网格应用的前处理部分，面向上万个处理器核，系统地研究以下内容：高效的三维实体建模并行算法、面向特征的复杂外形自适应网格生成并行算法；基于上述理论和技术，研制高效的三维并行前处理软件包，作为共性层面的软件模块服务于若干实际问题的大规模并行数值模拟。.    本项目的研究具有很强的应用牵引，是当前迫切需要解决的重要科学问题，并具有广泛的应用价值。
基于负载分类的实时嵌入式网络化NAND闪存存储系统及分析方法	实时;闪存;嵌入式;网络存储	随着嵌入式应用的不断发展，嵌入式系统对网络化实时存储的要求越来越迫切要求。大容量闪存存储和交换式实时网络技术的发展为实现这一目标奠定了重要基础。然而，现有闪存管理机制采用的单一管理算法模式不能满足异构负载类型对实时性的不同要求；复杂的网络存储体系结构与协议难以部署在能力差异极大的嵌入式节点上；由于调度单位不同，现有实时网络的实时调度与验证机制无法直接用来保障存储数据流的实时性要求。为解决上述问题，本课题申请提出构建轻量级网络存储架构，避免构造复杂存储协议和重量级架构；提出对闪存管理算法的实时性分析技术，设计可区分负载类型独立管理的闪存管理机制；以实时网络分析与调度技术为基础，设计适用于实时存储的分析方法和调度管理技术。基于上述技术，本课题将构造原型系统和仿真环境，对关键技术进行理论分析和仿真试验验证。
数据驱动的移动视频传输关键技术研究	数据驱动;传输调度;移动视频;性能优化;视频推荐	随着WiFi和3G/4G通信技术的普及，移动视频已经成为互联网最主要的传输流量。移动网络通信环境复杂，移动视频观看情境多样化，移动用户访问视频行为多变，这些特征导致现有移动视频传输技术难以提供良好的传输性能和用户体验。本项目以真实移动视频应用数据分析建模为基础，将数据驱动研究和实验方法贯穿项目的研究过程中。项目的主要研究内容包括：考虑用户观看移动视频的统计规律和移动视频流行度，研究基于机器学习算法的用户行为预测方法；针对移动网络的异构性，研究WiFi与3G/4G混合无线接入方式下的视频传输策略；面向异构移动设备，研究移动设备能耗和视频质量的联合优化模型；考虑移动用户请求视频的地理位置分布，研究移动边缘网络资源管理与优化方法；引入情境信息，研究移动视频推荐架构和基于进化多目标优化的推荐算法。本项目的研究成果有助于解决移动视频传输的性能瓶颈，并提高移动用户视频观看体验。
基于新型语言机制的异构系统通信自动优化及其应用研究	语言机制;通信优化;性能优化;异构系统	超级计算机的架构呈现出异构化发展的方向。提升异构系统应用性能的一个重要方面，在于提升应用中通信部分的性能。相对于传统集群，异构系统的数据通信不仅包括节点之间的网络通信，还包括节点内不同类型存储器之间的数据传输；同时，异构系统各个节点的健康状况、网络的使用情况都在时刻发生变化，对通信性能必须针对系统的动态性能模型进行优化。对于非计算机专业背景的应用程序员而言，完成此类优化工作可谓相当困难；而且，即使在一个系统上完成此类优化工作，到另一个体系结构不同的其他系统，往往需要重新考虑此类优化问题。本项目针对大型异构系统（包括GPU加速系统、MIC加速系统和国产众核系统）多进程/线程参与的复杂通信模式，设计基于新型程序语言机制的通信自动优化方案，解决应用程序员在其上进行编程时数据通信和转置的简化表示和自动优化问题，提升大型异构系统对于应用程序员的易用性，应用于直接法湍流模拟和蛋白质模拟示范应用。
面向广域云服务的计算、数据和网络的协同调度研究	数据密集型应用;调度算法;广域云服务;协同计算	云计算的发展正呈现出两个特征：数据中心建设和大量数据及副本存储的泛地域化。面向广域云的数据密集型任务处理受数据中心的计算存储能力、地理位置、数据及副本的存储位置和广域网带宽的影响变得更加复杂困难。针对该问题，为提高数据密集型任务的处理效率，本项目拟开展广域云环境下的资源调度研究。综合考虑虚拟机实例选择与安置、数据副本选择与安置和网络路由控制技术，提出以"计算、数据和网络的协同调度"为核心，解决跨数据中心的数据密集型任务的时间效率优化和收益效率优化两个基本问题。具体的，本项目研究将针对离线调度、在线调度以及分布式调度情境设计协同调度算法，并对算法作理论分析和仿真实验评估。研究成果将应用部署到广域天文数据分析中，有望提高天文数据分析的运行效率，为广域云计算产业的发展提供理论和实践支持。
基于三维SoP的高性能微处理器片上互连和存储研究	三维SoP;微处理器;片上存储;片上互连	高性能微处理器技术是国家安全和国防安全的重要基础技术。本项目针对未来10-15年高性能微处理器的下一代主流体系结构开展研究，研究基于SoP的高性能微处理器的片上三维光互连体系结构、基于网络演算的三维互连结构性能分析与优化和三维片上存储划分机制等。要解决的关键科学问题是高性能微处理器互连结构中功耗、带宽、延迟、面积、可靠性等综合性能的影响因子调整方法问题、基于网络演算的三维片上网络多个业务流竞争网络资源的复杂冲突建模问题和用户程序的访存行为模式的感知机制三个问题。项目的特色和创新包括片上三维光互连网络的光电并行体系结构、多业务流竞争网络资源的冲突树演算模型和应用对片上存储失效的高准确性预测模型。项目将提出片上光互连网络的光电并行双层体系结构，提出基于网络演算的三维片上网络性能分析和优化方法、提出基于访存行为模型和动态温度模型的三维共享存储划分机制和提出高可靠低开销的三维片上存储动态容错机制。
超级计算系统预算约束的可靠性与能耗动态融合任务调度策略	容错调度;能耗;超级计算机;作业调度;优化	近年来，超级计算系统在社会经济、科学技术和国防安全中的应用日益广泛。然而，超级计算机固有的可靠性墙、能耗墙、并行墙等问题一直制约着高性能计算的应用效率。项目首先针对超级计算机的大规模性与异构性，提出能耗视角的体系结构，采用DVFS、任务整合与资源休眠机制建立能耗计算模型；然后，以石油领域地震成像叠前逆时偏移处理为复杂高性能计算应用实例，采用结构化网格和非结构化网格复合技术对数据集进行动态剖分，提出适合超级计算机体系结构的多约束动态任务划分方法；其次，研究能耗、温度、可靠性与任务执行行为关系，提出应用程序执行可靠性和能耗动态融合理论；最后，采用经济学非合作博弈均衡理论计算应用程序执行成本，提出成本预算约束下的可靠性与能耗动态融合多目标帕累托优化调度理论。项目研究成果将丰富超级计算系统能耗管理、可靠性保障和任务调度等方面的基础理论，为提高超级计算机应用效率提供一种可行方法。
面向高能效和低延时的多核共享资源冲突约束方法	上界延时;缓存节能分区;Bank冲突延时最小化;能耗最小化;实时节能总线	本项目研究面向高能效和低延时的片上多核共享资源冲突约束优化模型。该模型包括三个主要优化目标：（1）基于缓存节能分区的Bank冲突延时最小化；（2）实时节能总线策略与支持机制；（3）片上共享资源的能耗最小化。本项目紧密结合高能效计算和WCET计算，重点研究一个Bank冲突延时最小化算法，在优选的缓存节能分区容量基础上使Bank冲突延时最小化；以及研究一个支持UBD优化值和时间距离的实时节能总线策略与支持机制，在减少的UBD改进值内保障硬实时任务及时得到总线服务，在时间距离值内提升非硬实时任务总线服务能力；同时在保障硬实时任务WCET满足定时要求的条件下，利用缓存节能机制和总线节能机制使片上共享资源的能耗最小化。研究成果可广泛应用于诸多电池供电的高端多核实时系统。本项目的可行性已在前期工作中得到了充分的验证。
面向车联网的协作机制及可靠通信方法研究	广播机制;可靠路由协议;分簇方法;协作机制;车联网	继互联网、物联网之后，车联网成为未来智慧城市的另一个标志，具有重大的科学意义和应用价值。与传统MANET不同，车联网具备节点高速移动、网络拓扑变化频繁、易受驾驶员行为及环境影响、无电源约束等特征，很多传统技术难以适用。本项目拟针对车联网环境下的协作机制和可靠通信方法进行创新性研究。首先研究融合RSU和3G/4G网络的协作方法，包括移动模型分析、动态自适应网关管理方法和基于多RSU的协同数据分发；然后研究车联网的分簇技术，包括基于移动模型、与场景相关以及引入情感因素的分簇方法；同时研究可靠路由协议，包括突发性链路质量评测方法，基于RSU的分段贪婪地理路由以及基于社会网络的协作路由；最后研究高效可靠的广播机制，包括高可靠低冗余的广播数据重传策略、支持预留广播通道的快速转发机制和面向车辆稀疏模型的广播域连通性保障方法。本项目的研究成果将为车联网的应用和普及提供理论支撑和技术保障。
多故障模式分布式计算系统可靠性分析方法研究	可靠性模型;可靠性分析;冗余容错计算机;动态可靠性;可靠性建模	针对现代分布式计算系统故障模式越来越多、失效行为越来越复杂这一发展趋势，研究高效的多故障模式分布式计算系统可靠性分析方法显得非常迫切，有着重要的现实意义。但已有分析方法在模型复杂性、计算效率、分析自动化、算法实现等诸多方面存在各种尚待解决的问题，不能很好的满足实际需求。本项目在已有前期研究的基础上提出了基于多值决策图(MDD) 的统一建模框架和全系统分析方法。重点研究：动态故障、共模故障和传播故障的多值变量编码方法和MDD 建模方法；多故障模式多值变量排序约束条件和系统级MDD 生成方法；高性能多值变量启发式排序策略和基于截断MDD 的近似可靠性分析方法。项目预期将获得高效的多故障模式分布式系统可靠性MDD 分析方法，对于完善分布式计算系统可靠性评估的理论研究具有重要意义，可以为冗余系统结构和底层容错机制等高可靠性设计决策的制定提供理论基础，也能够为系统高可靠运营机制的设计提供科学依据。
多云数据中心支撑之上的内容分发架构即服务关键技术研究	云计算;数据中心;负载调度;内容分发网络;流媒体	传统的流媒体视频提供商需要租用固定带宽的CDN服务器，造成较大的CDN租用费用，资源也不能动态扩展。而云计算的兴起倡导视频提供商可以按需使用云端资源，云数据中心服务器虚拟化和云存储技术的进步已经能为流媒体服务提供保障。多云数据中心的联盟正可以支持内容分发架构的部署。在这个背景下，我们提出多云数据中心支撑之上的内容分发架构即服务关键技术研究，简称内容分发云架构技术。研究新型的基于多云架构的内容分发服务松散耦合和资源高效调度技术。我们将展开研究内容分发云服务接口技术，研究跨多个数据中心的虚拟缓存资源分配模型，视频用户访问规模变化的高效预测算法，负载自适应的虚拟缓存资源调度和伸缩模型，和高性价比的内容分发云资源配置与容量规划经济模型。课题研究成果将有助于大力推动内容分发云技术的发展，为内容分发云资源的有效分配调度和使用，为各种异构内容服务系统的松散耦合提供理论基础、关键算法及基础模型的支撑.
面向应用领域的处理器软硬件协同设计关键技术研究	协同设计;编译优化;处理器设计;专用指令处理器	日益丰富和多样化的应用带来了不断增长的对计算能力的需求和功耗的约束，如何在多种约束条件下满足应用的性能、功耗等需求，同时缩短产品的设计周期，以适应市场的快速变化，是现代处理器设计的重要问题之一。从高级语言源程序开始的处理器协同设计技术可以在有效改善特定应用的性能和功耗的情况下保持通用处理器灵活的编程模式，拉近设计方法学的发展与生产工艺发展之间的差距，因此日渐得到学术界和工业界的重视，是近年来的研究热点之一。 本基金项目申请的研究工作将深入探索面向应用领域的处理器协同设计技术在实际应用中存在的不足，有针对性地将编译优化技术引入自动设计流程，建立和完善面向处理器软硬件协同设计的统一的编译支撑平台，并基于该平台开展面向特定应用的协同设计优化技术研究。这对于降低计算机系统的设计开发成本，改善设计效果，提高研发效率，具有重要理论意义；对加快国产自主CPU及配套系统软件的推广应用也具有实际推动作用。
通讯避免的若干数学库核心算法的设计和优化	计算机系统结构;高性能计算;并行计算	随着计算机系统的计算能力和通讯能力之间的差距越来越大，算法执行的总开销中通讯所带来的性能和能耗开销逐渐占主导地位，因此研究算法的通讯避免对于计算系统的性能提高和能耗降低都具有十分重要的意义。数学库中的核心算法对于高性能计算领域具有重要的研究意义，本项目重点针对高性能计算中几个重要的基准测试程序如Top500，Graph500，HPCG等，研究其中核心算法通讯避免的设计和优化。本项目拟从以下三个方向开展研究：1）构建实用的并行计算通讯模型，以反映最新的计算机体系结构特征，并确定在模型的框架下若干核心算法的并行通讯复杂度；2）设计通讯避免的算法，以达到或者尽量接近所确定的通讯复杂度下界；3）在现有主流的体系结构上，实现并优化所设计的通讯避免算法。通过本项目的研究，可以为数学库中的若干核心算法提供通讯避免的设计与优化方法，提高高性能计算实际应用的效率。
云环境下基于BSP模型的大规模不动点迭代计算研究	分布式计算框架;BSP;不动点迭代;云计算;大数据	不动点迭代广泛存在于数据挖掘和机器学习算法中，在社会网络分析、高性能计算、推荐系统、搜索引擎、模式识别等领域都有广泛应用。近年来，人们开始利用云环境进行大规模不动点迭代计算以适应大数据处理的需要，这也是当今云计算和大数据领域的研究热点，并且已经取得了一系列研究成果。本申请基于这些已有工作，以BSP（Bulk Synchronous Parallel）模型为基础，研究适合大规模不动点迭代计算的改进BSP模型。针对大数据新形势下的性能优化需求，研究基于多初始点的迭代过程优化、基于差别消息的异步迭代模型、基于数据依赖关系的增量处理技术，从多个方面提高大规模不动点迭代计算的处理速度。另外，为了便于验证和推广研究成果，本课题将基于研究内容，实现一个支持大规模不动点迭代计算的分布式计算框架原型系统。
嵌入式计算机控制系统可信化构造关键技术研究	进程代数;嵌入式计算机控制系统;层次化;等价关系	高可信是以离散变化和连续行为相融合为特征的嵌入式计算机控制系统分析与设计中的共性和关键性问题。本项目将对这类系统可信化构造中的一些关键技术进行研究。具体的研究内容是：（1）建立一套 数学模型，即基于进程代数的嵌入式计算机控制系统形式化模型；（2）抽象和分析的有力工具，如互模拟和模拟理论，的建立；（3）嵌入式计算机控制系统层次化方法的提出；（4）实例分析和辅助软件工具开发。.近年来，随着诸如嵌入式计算机控制系统等混合系统的应用日益广泛和深入，系统的高可信问题已经成为人们的研究热点和难点，也是关乎国民经济安全运行的迫切需要解决的问题。本项目将为嵌入式计算机控制系统的可信化构造提供统一的方法框架。本项目的研究成果不仅可以直接用于计算机、自动控制、通信等领域，而且对生命科学中的系统生物学领域有方法论的意义。因而本项目具有重要的理论意义和实际应用价值。
可信、可扩的复杂电网实时仿真算法研究	复杂电力系统;多尺度混合算法;实时数字仿真;并行计算	实时数字仿真是实施关键设备硬件闭环测试的核心技术，是电力等高可靠行业发展新技术和实施重大工程项目的关键。针对现代电力系统对实时仿真的迫切需求，及所面临的新计算挑战，本申请以提高算法的计算实时性、增强同步交互的可靠性和确保计算准确性为目标研究可信、可扩的复杂电网实时仿真算法。课题研究围绕三方面展开：一是开展适应多步长、多模型的多尺度混合仿真算法研究，在保证仿真准确度的基础上尽可能降低计算复杂度；二是面向多核集群系统平台开展可扩展的并行仿真算法研究，从算法构造、任务划分、算法映射和通信优化入手来提高计算实时性，增强算法的扩展性和灵活性；三是研究可信的仿真控制机制，降低因延迟导致仿真误差甚至仿真中断的可能，满足严格的实时交互要求，保障计算准确性和可靠性。本课题的成果将有助于促进新一代电网实时仿真系统的研制，为互联电网大功率电力电子实时数字仿真平台的建立提供坚实的理论和技术基础。
基于随机网络演算的数据中心QoS保障节能策略研究	节能策略;性能评价;服务质量;数据中心;随机网络演算	数据中心为众多的网络服务提供了后台支撑基础构架，同时也消耗了大量的电能，能耗成为数据中心总体成本的重要组成部分。尽管存在着大量降低数据中心能耗的研究工作，目前仍然缺乏有效的理论工具对数据中心节能策略进行QoS研究，十分需要采用理论模型方法从基础上对其进行认识和研究。随机网络演算是一种基于边界的性能评价方法，能够刻画多种类型的业务到达与服务过程，并对统计性能保障提供分析。本项目以随机网络演算作为理论工具，对数据中心的业务到达与服务能力进行建模；在此基础上构建数据中心成本优化模型，提出一类节能策略，在降低数据中心能耗的同时，为数据中心业务提供统计QoS保障；结合真实系统的日志记录文件进行数值实验，在随机网络演算模型框架下调整模型参数，量化分析数据中心节能策略的潜在效益；并将基于数值计算和理论分析的结果，对节能策略进行改进。
面向动态规划计算的并行编程模型和运行时系统研究	任务调度;编程模型;DAG;并行计算;动态规划	动态规划广泛应用于科学和工程计算，问题规模增加以及并行计算硬件技术发展使动态规划计算并行程序研发的难度随之增高，本课题通过面向动态规划的可复用并行编程模型和高效运行时系统来简化此类应用并行编程的难度和复杂度。基于DAG数据驱动编程模型，研究自适应多层次任务调度方法，通过任务粒度二次划分以及多层次任务调度的方法解决负载不平衡问题；研究适用于不同硬件平台以及应用类型的大规模中间结果数据的处理方法，以高效共享文件的方式解决中间结果数据的存储和访问问题，以按需数据传输的方式减少不必要的通信；研发插件式DAG模式库管理方法，支持模式的复用和系统扩展。研发过程结合计算生物信息学和计算实验金融领域的具体算法和应用进行，以实际应用反馈研究方案，并针对集群、MPP体系结构以及具体文件系统进行优化。课题预期成果可为基于动态规划计算的应用研发提供编程模型和运行时系统的支持，缩短科学及应用研发周期。
基于编译的PCM内存损耗均衡方法研究	损耗均衡;数据分配;编译技术;相变存储器	传统的DRAM技术由于存在存储密度低和功耗高的缺点，已经不能适应现代集成电路技术的发展趋势。PCM技术因其在存储密度和功耗方面具有显著优势，被视为最具潜力的DRAM替代技术。但是，PCM能够承受的写入次数非常有限。由于内存的访问通常是很不均衡的，PCM内存很可能由于局部过热而提早损坏，使用寿命和可用性面临巨大挑战。本课题组拟研究基于编译的损耗均衡方法，通过改善内存的访问均衡来避免局部过热和提早损耗，从而改善PCM内存的使用寿命和可用性。拟研究一种数据重命名方法来改善数据的访问均衡；拟研究一种损耗均衡感知的数据分配方法来改善数据的存储均衡；拟研究一种有机融合现有的硬件、操作系统和编译层次方法的层次化的损耗均衡方案。通过本课题的研究，将实现高性能、低功耗的PCM内存损耗均衡方法，以改善PCM内存的可用性，对于PCM技术的进一步推广应用具有重要意义，也为面向其他存储技术的编译方法提供新的思路。
嵌入式异构多核系统应用程序自动并行化过程关键技术研究	异构多核系统;并行编程;自动并行化;嵌入式系统	异构多核系统凭借其在处理能力、适应性和能耗有效性方面的优势，必将成为嵌入式系统的主流架构。如何有效利用嵌入式异构多核系统中的海量硬件资源，充分挖掘应用程序中的并行性，提升系统整体性能，已成为国际上的一个重要研究课题。本课题拟以多媒体处理领域为研究对象，结合数据密集型研究方法，围绕嵌入式异构多核系统的应用自动并行化过程开展研究，重点突破异构多核系统硬件资源的建模方法，考虑系统异构特性的并行任务的划分、映射和调度算法，以及提升应用运行时实时特性和并行效率的相关技术，并基于开源编译器实现所提出的算法和方法，构建应用自动并行化平台，大幅提升嵌入式异构多核系统的并行应用开发效率。
CPU/GPU异构系统下高光谱遥感影像降维多级协同并行计算方法及优化策略	并行算法;降维;GPU;高光谱影像;异构系统	如何对高光谱信息进行高效处理是遥感领域近年来的研究热点，高光谱影像降维是做好后续地物分析识别的一个关键步骤。由于降维计算是一个典型的计算密集和访存密集型过程，计算复杂度很高，采用传统的串行处理模式，已无法满足军事、农林等高端应用的实时性需求。通用CPU和专用GPU相互配合的异构系统可以满足应用对计算资源的不同需求，是未来高性能计算机体系结构最有前景的发展方向之一。本项目将针对高光谱遥感影像降维计算方法，以线性降维方法为牵引，重点研究基于流形学习的非线性高光谱影像降维算法，建立性能分析模型，研究提出基于新型CPU/GPU异构体系结构的多级协同并行算法，归纳此类应用面向该体系结构特征的一般优化方法，并反向指导降维方法在可并行度和可扩展性方面的创新，突破原有算法的"加速比墙"，出原创成果，有效提高高光谱遥感影像处理的业务水平，应用前景广阔。
虚拟化环境下面向新型存储系统的I/O资源调度方法	虚拟化;存储系统;资源调度	存储系统及其I/O性能一直是制约计算机整体系统性能的关键瓶颈之一，在日渐广泛的虚拟化系统中，I/O瓶颈问题因为虚拟化层的引入和虚拟机资源竞争而更加突出。现有研究工作主要基于磁盘存储环境，但是随着固态硬盘SSD的逐渐普及、PCM等新型非易失性存储器和存储级内存SCM的出现、以及存储系统结构的不断演进，传统针对磁盘环境的虚拟化I/O机制将难以适应日渐复杂的存储系统。本项目申请针对虚拟化环境下面向新型存储系统的I/O优化问题，从文件元数据访问、镜像文件数据分布、多虚拟机间I/O干扰等角度深入分析和总结虚拟化环境下I/O访问特征与规律，建立虚拟机I/O访问模型；基于此特征规律，结合SSD、PCM及网络存储系统的各自特点，研究一系列虚拟化环境下的I/O资源优化调度方法，包括面向SSD的多虚拟机并行I/O访问优化、面向PCM的虚拟机元数据访问加速、面向网络存储的虚拟机I/O服务质量保证等。
网络取证中的电子证据融合、推理及呈现技术研究	犯罪行为画像;电子证据;数据融合;行为推理;网络取证	网络取证技术的研究为调查计算机网络犯罪提供了一种新的有效方法和工具。然而当前相关研究在证据链融合、推理以及电子证据呈现等细节方面的局限性，致使网络取证技术难于实际应用和部署。本项目将重点考虑这些因素，以电子证据的融合、推理及呈现为核心科学问题，为构建一个实时、高效、准确的网络入侵取证系统，提供基础理论和关键功能单元技术与方法。并拟在以下方面取得创新性成果：疑似证据冲突消解技术与方法、有效性证据链推理模型与算法、证据图的图谱分析技术、犯罪行为画像的理论与方法等。项目力争在网络取证理论和方法两个方面取得突破，为我国打击计算机网络犯罪案件提供理论依据和技术支撑。
差错容忍的近阈值计算技术研究	暗硅;近阈值计算;能效性;差错容忍;近似计算	近阈值计算是提高计算机系统能效性的有效手段之一，但电压降低却导致了严峻的可靠性问题。提供精确可靠的计算往往开销较高，近似计算技术通过容忍差错为高可靠、低功耗的系统设计提供了可能。然而不同应用的近似计算质量存在差异性，近阈值电压又加剧了处理器核间的差异性。如何解决上述差异性是优化系统能效性、可靠性和用户体验等多方面的核心问题。因此，课题拟深入探索如下四个方面的内容：(1)研究基于NTC失效率的近似计算质量模型，为系统动态管理和优化提供理论支持；(2)针对近阈值电压引发的差异化问题，研究计算质量感知的功率管理方法；(3)针对热点影响可靠性的问题，研究温度感知的应用程序调度方案；(4)研究支持近似计算的近阈值系统软硬件协同设计技术。本课题将极大改善近阈值电压技术面临的可靠性问题，实现性能、功耗和可靠性的综合优化。课题预期在国内外期刊和国际会议上发表7-10篇高水平论文，并申请3-5项专利。
多核处理器结构级功耗评估与优化关键技术研究	低功耗设计;片上多核处理器;功耗分析;片上网络	功耗是导致片上多核处理器出现的重要诱因，也是片上多核处理器设计的重要制约因素。如何在结构级降低多核处理器的功耗并提高处理器能量效率，具有很大的研究意义与探索空间。本项目从提高多核处理器功耗模拟的准确性与灵活性的角度出发，研究多核处理器结构级功耗评估与优化的关键技术。通过对多核处理器电路的精确实现与分析，以及基于物理反馈的功耗建模机制，提出创新高效的结构级多核功耗评估方法和模拟平台，将基础性原理－算法－结构研究，和技术实现有效结合起来。并在此基础上，以片上网络的功耗有效性为研究对象，开展多核处理器的片上互连功耗优化，寻求提高多核处理器可扩展性与功耗有效性的多核互连结构，并希望将上述研究成果应用到未来的国产多核处理器如龙芯3号处理器的设计中。
面向众核体系结构空间特征感知的热管理研究	空间特征分析;全局协作的热管理;资源优化;机器学习;众核体系结构	随着计算机系统规模增长及"大数据"各类应用，大量异构应用程序和线程并发执行，使性能需求不断提升，能耗问题日益突出。传统体系结构分析方法，以单纯细节模拟和统计分析为主，不适应大规模异构众核体系结构的分析需求。为了更全面、更智能地分析和利用体系结构特征，我们提出将大规模众核系统复杂负载的热行为映射成2D空间特征，并采用机器学习等方法进行特征分类和预测，以便更直观地指导资源优化和热管理。主要包括三项内容：（1）研究能获取丰富信息、可扩展的空间特征挖掘和分析方法，以较低开销有效地获取并分类典型的热空间模式；（2）研究基于机器学习的空间特征智能预测模型，快速重构相应配置众核系统复杂负载的热足迹；（3）研究空间特征感知的资源优化和热管理方法，建立热冲突最小化模型，并利用反馈和学习机制，全局协作地调度任务和进行动态热管理。研究成果将对提升复杂众核系统分析和评估的有效性，节省大规模系统能耗有重要意义。
云游戏中资源管理关键问题研究	多服务器系统;资源利用率;任务调度模型;负载调度;能耗管理	云游戏是基于云计算技术的新型游戏模式。在云游戏中，游戏运行在云端服务器上，用户只需通过轻型客户端和游戏交互，免去了安装游戏和配置硬件等负担，让用户随时随地都可以体验最新的游戏。基于以上优点，云游戏的出现引起了工业界和学术界的广泛关注。随着用户数量不断增加，云游戏运营商通常需要提供大量的服务器等资源来满足用户需求，导致高昂的运营成本。因此，如何高效地对系统中的资源进行管理来降低运营成本、提高系统服务质量对云游戏运营商来讲具有重要意义。然而，云游戏平台中的资源管理面临很多挑战，包括用户请求结束时间未知、游戏不能在服务器之间迁移、多个资源管理问题相互关联以及系统状态不断变化等等。针对这些困难和挑战，本项目将对云游戏平台中的资源管理关键问题，包括用户请求调度、游戏布局以及动态资源管理等，进行深入研究，并提出高效的解决方案。研究对整个云游戏产业的发展具有重要的现实意义。
社交网络中软安全机制研究	演化博弈论;非合作;软安全;社交网络	本项目主要研究社交网络中软安全机制，它涉及到许多科学领域， 如：社会学、经济学、心理学、博弈论、计算机、软件工程方法论、图论等，因而具有很强的交叉性。社交网络是当前计算机领域的最热门研究课题。为了实现互联网资源的有效共享和综合利用，围绕建立高效、和谐、可信的大规模社交网络环境，本项目将研究基于社会准则的信誉表示机制；用户行为优化机制；用户社区管理机制；基于演化图理论的社交网络环境随机动态性研究；社交网络中下行间接互惠与信誉之间的内在联系、算法、模型等。 本项目从一个新的角度-软安全，重新审视和研究软安全技术在社交网络等中的应用，对推动该技术在社交网络和电子商务中的发展具有重要意义，也具有重要的理论价值和应用价值。
基于内容感知型去重Cache的磁盘能耗管理策略研究	缓存管理;系统建模;磁盘存储系统;对象存储;数据布局	磁盘上的板载Cache 能有效减少到达盘片的数据请求，延长其处于低功耗状态的时间。但受SRAM 的价格影响，其容量一般小于128MB。传统的基于地址的Cache 替换策略导致该Cache中存在较多来自不同地址但内容相同的数据块。本项目提出对磁盘Cache 进行重复数据删除以增加其有效存储空间，提高命中率并加大到达盘片的请求间的时间间隔，再结合磁盘功耗状态的切换以降低能耗。具体内容包括：（1）探明磁盘Cache 中的数据特点、分布规律和语义关联，明确其数据行为的形成机理，设计实时在线的方法来识别其数据行为；（2）研究并设计有效的去重磁盘Cache 结构，高效轻载的去重方法，能量感知的Cache 预取策略，以及协作性内存刷新机制；（3）理清影响该能耗管理策略的各种影响因素，了解它们之间的相互作用和影响趋势，构建其代价模型。相关研究成果，将进一步推动和完善磁盘存储系统在节能降耗方面的方法和理论。
面向数千个处理器的并行代数多重网格算法研究	代数多重网格算法（AMG）;稀疏线性代数方程组;辐射流体力学数值模拟;迭代方法;并行计算	具有良好算法和并行可扩展能力的线性解法器是在数千个处理器上完成精细数值模拟的重要支撑。代数多重网格（AMG）是当前科学与工程计算中线性解法器采用的最有效的迭代方法之一。然而，对于在数千个处理器上的复杂物理系统数值模拟应用，现有的AMG方法无法取得理想的可扩展能力，迫切需要一系列新的算法和技术改变现状。本项目围绕这一前沿问题，面向数千个处理器，重点研究新的并行AMG算法、理论和并行实现技术，研制高效并行AMG解法器软件包，期望改善并行AMG算法在数千个处理器上的可扩展能力。在此基础上，本项目将瞄准高能量密度物理领域中若干重要的实际应用问题，支撑该类应用的数值模拟扩展到数千个处理器，并取得理想的数值模拟性能。本项目具有很强的应用需求牵引，并将直接服务于实际物理系统的大规模数值模拟，具有很强的理论和实际应用价值。
车用实时异构网络调度模型及算法研究	实时;调度模型;车用网络;调度算法;异构	车用网络已经发展成为典型的嵌入式实时异构网络，如何调度这类网络使其为上层实时及安全攸关等应用提供有力支撑是车用网络进一步发展必须解决的关键问题。本课题旨在以信息物理融合系统的观点为指导，研究车用嵌入式实时异构网络调度的关键理论和技术。主要包括车用嵌入式实时异构网络的调度模型，消息和系统的可调度性理论，消息传递时的可预测性度量及其调度优化算法，系统可靠性调度优化算法等，最后开展实证研究，构建基于上述理论成果的原型系统。通过模型、算法、原型系统等的研究，将为混合时间触发和事件触发机制的车用实时异构网络的有效消息调度研究提供指导，为信息物理融合系统环境下车用实时异构网络的研究和实现奠定基础，并推动信息物理融合系统和车用网络技术之间的交叉应用研究。
数据中心服务器内存子系统关键技术研究	存储系统;数据中心;性能优化;资源管理;存储墙	数据中心面临的最大挑战之一就是降低成本和能耗，提高资源利用率。内存的成本和能耗在数据中心里占有着非常高的比例。申请人通过大量测试分析发现，数据中心典型应用与桌面应用和高性能应用具有完全不同的访存特征,使得现有数据中心服务器内存子系统效率非常低。因此，本课题提出一套数据中心服务器内存子系统的优化方法，用以降低内存使用量，提高内存带宽利用率。具体包含三方面研究工作：1）利用硬件优化"基于内容的页面共享"技术，提高单节点内存可用容量；2）充分考虑虚拟机应用负载特征，优化内存控制器的配置，提高带宽利用率；3）研究高效灵活的远程内存管理机制，提高节点间内存共享的效率。通过以上提出的节点内和节点间内存的优化方法，提高数据中心内存资源的利用率，进而降低成本和能耗。本项目预期将取得以下成果：1）在国内外知名的学术会议和刊物发表4 篇以上学术论文，其中3 篇以上EISCI 索引；2）申请5 项以上专利。
表面电极离子阱芯片可扩展设计研究	非谐阱;可扩展;离子串分离;囚禁离子系统;表面电极离子阱芯片	离子阱量子计算物理系统以其相干时间长、逻辑操作保真度高等优势，成为量子计算物理系统研究的热点。目前，可扩展离子阱芯片被认为是实现大规模离子量子计算的有效途径。然而，离子阱芯片可扩展研究正面临势阱深度锐减、非谐理论不适合非谐单阱设计、多阱连接处"结"区射频电极形状优化难、长串离子分离操作对芯片结构需求不明朗等挑战，特别是离子量子计算已经从小规模实验演示过渡到大规模可扩展的今天，上述挑战已经成为亟待解决的关键问题。为此，本课题立足问题的本源，提出离子阱芯片加盖的理论、非谐势离子阱芯片量化分析模型、灵活的"结"区射频电极优化设计方法以及面向长串离子分离的芯片电极优化设计方法，对关键瓶颈进行各个击破。在理论成果指导下制备表面电极离子阱芯片，验证所提模型和方法的正确性和实用性，具有重要的战略意义和应用价值。
可能量感知的块压缩磁盘Cache中的关键技术研究	能耗;压缩;Cache;磁盘	磁盘Cache除了用来提高磁盘性能之外，另一个重要的隐含功能是减缓磁盘盘片上的请求到达率，延长磁盘盘片可能处于低功耗或低转速状态的时间。然而，基于高速Cache成本方面的考虑，磁盘Cache一般较小（2MB-32MB）。本项目从能耗的角度，提出在磁盘Cache这个层次进行块一级数据压缩，来增加磁盘Cache的有效存储空间，并达到降低磁盘能耗的目的。研究从能耗、性能、压缩比三个角度同时来优化适合于磁盘Cache的压缩算法；设计基于磁盘属性的低能耗可压缩磁盘Cache的组织和管理策略；研究数据块之间的语义关联对压缩Cache的影响；从而减少对磁盘盘片的访问。再结合传统磁盘和多速磁盘的能耗管理模式，来构造一个可能量感知的、基于块压缩的磁盘Cache系统。
众核体系架构并行计算模型与算法自适应调优框架研究	OpenCV;算法自适应;数学库基础算法;并行计算模型;众核体系架构	众核架构已成为处理器体系结构发展新趋势，其多粒度并行性、复杂存储层次和有限存储带宽等都对并行算法的性能调优及性能可移植提出巨大挑战。FFT、稠密矩阵计算和OpenCV是三类不同的典型应用核心算法库，如何有效提高三类算法在众核上的运行和开发效率成为一个迫切需要解决的问题。本项目拟针对三类典型核心算法在众核上的性能可移植和高效实现开展研究,主要研究内容包括：1）新的众核并行计算模型研究。对众核架构的多粒度计算和多层次存储等关键特性进行建模，用以指导众核并行算法设计和分析；2）基于计算与访存模式分离指导语句的并行编程框架研究。利用并行计算模型和基于手工调优形成的优化方法链进行双向验证，提出一套基于计算和访存模式分离的制导语句的并行程序设计框架；3）算法自适应性能调优框架原型研究。将调优框架从过去的仅仅是平台自适应，扩展为平台自适应和算法自适应两个维度，进一步提升自适应优化的适用范围和灵活性。
基于Petri网与协同过滤的云上Web服务可信性量化分析与预测的研究	协同过滤;云计算;随机Petri网;可信性;Web服务	在云计算和服务计算的众多研究方向中，可信云服务一直是热点和难点。现有的相关工作重在形式化建模和性质验证，量化研究较少，在云资源调度管理策略量化分析、云服务组合细粒度控制流建模、运行时可信性趋势预测等方面存在诸多不足。本项目拟综合运用Petri网、随机过程与排队网络、协同过滤、时间序列分析等理论与方法，对云上Web服务可信性进行量化分析和预测。具体内容包括：1）运用随机Petri网对的云上Web服务组合进行细粒度定量建模，在不依赖状态分析前提下，提出基于结构等效约简的多指标可信性计算方法；2）对云系统进程迁移、动态速率拓展、主动重生、虚拟机-物理机动态映射等调度管理机制和系统行为进行随机建模，分析其对可信性的量化影响；3）对云服务和服务组合可信性历史数据进行时间序列建模和趋势预测；4）运用协同过滤方法对云服务缺失可信性数据进行分析。研究还将采集真实系统数据，对所提出的相关方法进行检验。
基于智能硬件系统的透明计算技术研究	闪存存储;智能硬件系统;分布式系统;透明计算	智能硬件已被广泛地使用于人们生活的各个方面。传统智能硬件的系统软件设计通常需要权衡设备的功耗和服务能力，其使用范围和场景受限于能耗技术和扩展能力等多种指标。本项目拟研究基于智能硬件系统的透明计算技术，研究基于智能硬件系统的流式执行方法，提出有效、完善的应用语义特征提取和描述方法，为提高智能硬件系统的可扩展能力和优化系统安全性，提供新的思路；从应用的行为特征出发，利用流式执行的分布式处理能力，优化系统的能耗，并将系统执行结束的软件代码立即抛弃以提高系统的安全性；基于闪存存储的特征，建立适合智能硬件的小容量闪存存储的描述模型和数据结构；在服务端的数据管理上，设计高效的数据分布机制和恢复机制；并在此基础之上，进行系统模型抽象和原型实现，力图将研究成果应用于不同的智能硬件系统中，对研究结果进行验证。
分布式虚拟机监控器时钟系统性能优化方法研究	时钟系统;分布式虚拟机监控器;优化	分布式虚拟机监控器（DVMM）的时钟系统是DVMM稳定运行的首要前提，现有的相关研究工作主要从时钟源、定时和同步方法等方面来开展。针对DVMM环境下的时钟系统较之单宿主机虚拟化环境面临着更严峻的定时和同步等挑战，但现有研究在中断改进和时钟同步等方面存在不足的问题，以及时钟中断是DVMM中时钟系统的性能瓶颈的现状，从时钟中断优化理论入手，基于时钟中断在DVMM时钟系统中的生命周期，以提高定时和同步精度为目标，研究一种基于时钟中断改进的DVMM时钟系统性能优化新方法（首先建立DVMM的时钟数据结构，然后优化虚拟时钟中断注入方法，最后建立跨宿主机的虚拟处理器同步算法，再建立原型系统验证）。本项目从时钟中断优化理论入手，理论与实践互相促进，以实践为目标，对DVMM的服务效率、系统稳定性和可扩展性等都具有重要的研究和工业实践意义。
簇内紧耦合的高可靠适应性众核处理器体系结构研究	高可靠性;紧耦合;体系结构;众核;适应性	众核处理器将成为未来电子系统中的核心，随着工艺尺寸减小、电压降低及工艺的不稳定性，器件的可靠性问题日益严重。传统的处理器可靠性只考虑软故障的容错策略已不能满足需求。本项目提出一种新型的簇内紧耦合的高可靠适应性众核体系结构，旨在从处理器的高可靠性出发，在高可靠执行模式下采用簇内的核通过共享L1 Cache进行紧耦合，核内采用流水线级的冗余执行进行错误检测，采用专用的备份核动态备份主执行核的执行现场，以便在发生故障时任务快速迁移到备份核执行，降低对系统性能的影响。同时通过对L1 Cache的管理，使高可靠众核处理器在普通场合具有较高性能。.本项目将研究综合考虑可靠性和高性能需求的紧耦合的粒度、主执行核和备份核的现场同步步长与性能和功耗等开销的表征、簇内紧耦合的适应性众核架构下统一L1 Cache调度机制以及容错众核处理器的执行核管理机制等问题。研究成果将有助于拓展高可靠众核处理器的应用范围。
基于软件可知分区和硬件原语的众核处理器运行支持技术	资源分区;众核处理器;运行支持;硬件原语	众核处理器的运行时支持面临一系列挑战，主要表现在：众核环境下多种应用相互干扰，导致系统性能下降；众多进/线程同时运行，导致进/线程间的同步/通信/调度成为系统瓶颈；以进程/线程为单位进行任务分配/调度，众核资源难以得到有效利用。因此，解决众核系统的运行支持问题，在系统结构层面支持操作系统及应用程序的高效运行，使众核资源得到充分利用，对众核处理器系统结构和新型操作系统的形态与设计实现有重要的参考意义。.项目重点研究软件可知的分区方法，实现分区之间的隔离以避免应用间的相互干扰；研究分区与操作系统和应用进/线程之间的灵活映射，为上层提供便于管理和调度的系统结构视图；研究支持分区控制和进/线程间高效同步与通信的硬件原语，在保证灵活性的前提下，获得更高的性能；研究在分区内多个处理器核上进行细粒度任务调度的方法和支持机制，以更有效地发掘应用的并行性，提高众核资源的使用效率，使应用获得应有的性能。
苛刻环境高可靠电子系统的电源健康状况预测机制研究	苛刻环境;健康状况预测与管理;电源;高可靠性	苛刻环境高可靠嵌入式系统蕴藏着巨大的潜在应用价值，而电子系统中34%的故障都是由电源系统的故障所造成，因此，电源健康状况预测机制与算法已成为众多应用的核心支撑技术。为了提高嵌入式系统在恶劣环境中的可靠性、可用性和可维护性，本研究拟通过对电源系统的FFP特征，DFP特征和愈合（缓解）特征进行分析，构建苛刻环境电源系统的故障、劣化与愈合模型；根据对电源系统特征参数和关键状态及其时变率进行瞬时的在线实时监控，捕获系统健康状态的特征信息，从而预测电源系统故障和剩余使用寿命，从根本上解决过去传统方法中依靠"统计方法"而采取的"定时维修"或"事后维修"所带来的灾难性事故等诸多问题。最后，我们将集成阶段性研究成果，设计、建立电源系统健康状况预测机制与测试实验平台，并对故障和剩余寿命预测机制与算法的性能进行实验分析与评价。
高效能事务存储系统研究	事务存储系统;软硬件协同;优化;并行	一直以来，并行编程领域存在着程序设计复杂、易产生死锁、调试困难、程序执行性能低等难题。事务存储是一种新兴的应用在并行计算机上的对共享资源的并行访问控制技术，与传统的锁机制相比，事务存储为程序员提供了一种简洁、一致的访问共享资源的编程界面，大大简化了并行编程的复杂度，并能有效提升并行应用的性能，是解决目前并行编程难题最有希望的技术之一。本课题紧密结合事务存储领域相关的科研成果，在深入分析典型并行应用对共享资源的访问特征基础上，展开该领域相关基础性研究；课题采用软硬件协同处理的技术路线，设计并实现高效能的事务存储系统，解决传统系统存在的设计复杂、效率低、资源开销大等问题；针对事务执行的特征，研究事务存储系统的性能优化方法。课题具有良好的理论创新价值和应用前景。
层次存储的访问分析与优化方法研究	相似性;数据访问;亲和性;重用性;存储墙	存储墙问题一直是制约计算机系统性能的重要瓶颈。层次存储的访问即数据访问，是连接存储墙问题中处理器和存储器的"桥梁"。因此，我们认为数据访问特征的分析理论与方法是解决存储墙问题的基础。我们归纳了数据访问的六种重要性质：依赖性、重用性、相似性、亲和性、一致性和生存性。这六种性质从时间与空间、地址与值等不同角度描述了数据访问的不同侧面，对程序的正确性和性能有着重要影响。研究这些性质的分析理论与优化方法对于解决和缓解存储墙问题，探索新的层次存储体系结构和优化技术有着非常重要的研究意义。因此，本课题计划基于课题组在计算机体系结构、编译技术等方面的研究成果和技术积累，研究数据访问特性的分析理论与优化技术，并运用软硬件验证平台对这些技术进行验证，为解决存储墙问题奠定理论基础和探索新的技术途径。
重复数据删除存储系统的数据重构性能和能效研究	重复数据删除;云存储;固态盘;数据重构性能;能效	重复数据删除技术虽然能够有效地减少存储容量和提高网络带宽利用率，但是降低了数据重构的性能和能效。这是因为一个文件或数据块经过重复数据删除后，可能分散存放在不同的磁盘上，导致数据读取操作需要多个磁盘进行寻道操作。当重复数据删除技术逐渐应用于虚拟机和邮件服务器以及云存储等一级存储系统时，数据重构的性能和能效问题变得更为重要。本项目从存储系统全局出发，在基于磁盘的重复数据删除存储系统中引入固态盘，并根据存储设备的性能和能效特征以及数据块的属性布局数据，以提高数据重构性能和能效。本项目拟：（1）研究采用重复数据删除技术后数据块在存储设备上的布局特点；（2）研究重复数据删除技术对数据重构性能和能效的影响情况；（3）研究提高数据重构性能和能效的数据布局策略。本项目的研究可以解决重复数据删除技术对现有存储系统中数据重构性能和能效的影响问题，进一步推动重复数据删除技术的更广泛应用。
面向服务的异构多核可重构片上系统任务自动并行化机制研究	异构多核片上系统;自动并行化;任务划分与调度;可重构计算;面向服务	将异构多核片上系统与可重构技术相结合是嵌入式计算系统的发展趋势之一。然而，编程墙问题和任务自动并行化问题给异构多核可重构片上系统的发展带来了巨大的挑战。为了解决这两个问题，我们首次将面向服务思想引入异构多核可重构片上系统，拟构建面向服务的中间件框架，将异构计算资源抽象为服务体，将软硬件计算功能定义为服务，为用户提供一致的编程模型，以此解决编程墙问题。在此基础上，为了提升任务的并行度，本项目重点研究任务自动化并行机制中的软硬件功能动态划分策略和支持乱序执行的动态调度算法，拟在任务划分算法设计中同时考虑热点任务、软硬件差异以及可重构特性等因素，并将指令级的重命名技术扩展到任务级，提出一种新颖的调度算法来支持任务的乱序执行。本课题的顺利开展对解决异构多核可重构片上系统的编程墙问题提供了新方法和新思路，同时对任务的自动并行化研究具有重要理论意义和应用价值。
空天应用中芯片自愈型可重构硬件结构与容错机制研究	胚胎细胞阵列;设计优化;可靠性;可重构计算系统;芯片自修复	机载电子系统的高可靠、强容错设计是空天应用研究需要解决的关键技术。基于仿生胚胎硬件理论的芯片自愈型可重构硬件，能实现芯片级在线自主故障诊断和自主修复，用于空天计算机系统和电子设备设计，可显著提高可靠性，但目前设计因设计冗余严重而无法实用。本项目面向空天应用从硬件系统体系结构、细胞电路模块组成和细胞阵列布局三个方面研究芯片自愈型可重构硬件的简化设计方法：⑴采用具有分子自修复能力的新四层结构模型，改善传统结构因修复层级高导致硬件利用率低的问题；⑵采用只保存三个细胞配置信息的配置存储器设计，大幅减小了细胞电路面积，且可同时容错瞬时故障和永久故障；⑶通过可靠性分析给出细胞阵列的布局优化准则，可减少电路的冗余空闲细胞数。研究成果可作为高可靠、自修复系统设计的理论基础。
海量数据处理中面向任务加速的数据调度策略研究	数据调度;数据传输;数据分发;海量数据处理;数据预取	目前，在互联网应用、金融电信、医疗健康等诸多领域，数据量正在急剧膨胀。为挖掘这些庞大数据潜在的科学或商业价值，需要依赖高效的海量数据处理系统。优化任务调度与数据调度成为提升海量数据处理系统性能的两个重要手段。传统数据调度关注于数据存放、迁移，复制及副本管理，用于提高存储资源利用率及数据访问服务质量。这类调度操作并非针对任务执行过程，因而对任务执行中的数据I/O优化存在响应度不足的局限性。本项目针对海量数据处理任务复杂的计算过程，以降低数据I/O开销、加快任务执行效率为目标，研究面向任务加速的数据调度策略，涵盖数据智能预取、数据协同传输、数据均衡分发等调度策略。面向任务加速的数据调度策略克服传统数据调度的局限性，充分降低任务执行中数据I/O开销，实现计算节点、存储节点间的高效数据调度，对海量数据处理性能提升有重大意义。
面向大规模动态服务环境的QoS评价方法研究	服务计算;性能评价;服务质量;随机过程;数学模型	随着服务计算技术的蓬勃发展，服务质量（QoS）成为重要的需求。伴随着互联网上各类服务数目不断增多，服务系统内部以及服务之间的相互关系愈发复杂，服务环境显示出大规模和动态性等特点，使得多维度QoS评价面临模型状态爆炸、测量成本过高、分析求解难度过大等问题。本项目拟研究面向大规模动态服务环境的QoS评价方法与技术，从随机建模和统计测量两个角度，提出同时保证评价效率和准确性的服务质量评价方法。研究内容包括：服务质量的随机模型建模理论；高效的随机模型求解方法与技术；大规模动态服务环境下的QoS测量理论与机制；有效的数据统计分析和动态模型更新方法。本项目的研究将丰富随机建模和求解理论，发展服务质量测量和分析等实用技术，并推进他们在服务计算中的应用，为服务计算系统和各类服务的设计、实现和优化提供理论参考。
分布式流处理系统实时容错关键技术研究	流处理系统;可靠性;实时性;分布式系统;容错	近年来，各行业应用中对大规模流数据进行实时处理与分析的需求激增，分布式流处理技术发展迅速。随着系统规模的扩展和处理流程的复杂化，系统鲁棒性问题日益得到关注，分布式流处理系统故障容错问题逐渐成为研究热点。现有工作缺乏面向处理实时性的容错理论模型，缺乏相应的容错策略优化方法，容错方法动态性不足难以适应流数据处理特点。本研究重点关注分布式流处理系统的实时性容错问题，从理论模型、优化策略及支撑机制等层面开展研究，主要研究内容包括：面向处理实时性的节点故障代价模型及系统可靠性模型、容错策略优化及动态调整问题、实时性容错支撑机制及协议等。与现有的工作相比，本研究主要特色包括：将故障对实时性的影响引入到可靠性理论模型中、基于拓扑感知思想根据节点故障代价优化实时容错策略、支持自动并行化和突发性流等场景的容错策略动态调整等。本研究具有重要的理论价值和广泛的应用需求。
多核混合关键度实时系统中同步感知的调度方法研究	多核处理器;资源共享;混合关键度系统;实时调度;任务同步	多核技术发展以及混合关键度(MC)实时系统性能需求不断提高,研究多核MC实时调度理论,对优化多核处理能力和提高实时应用服务质量有重要价值.项目从MC实时应用本质特征及多核资源共享系统结构出发,致力于同步感知的资源访问协议、任务映射策略、能效管理机制及其可调度分析、可持续判定等调度方法研究.针对多核共享资源独占访问而导致的优先权及关键度反转问题,提出新的无死锁资源访问协议,分析并降低不同类型同步开销的上限;理论探讨该协议下响应时间、系统利用率上限等可调度条件,挖掘同步开销与映射策略的内在联系;据此研究同步感知的任务映射策略,提高任务可调度比率,并推导可持续判定条件以改善映射策略的鲁棒性;基于这些研究,设计能效管理机制,合理分配在线空闲以有效节能并维护系统稳定可靠;最终在实时操作系统上验证并优化提出的调度方法.本研究将为多核MC实时应用领域提供新的方法途径,可进一步促进多核实时处理技术的发展.
面向大数据备份的重复数据删除关键技术研究	重复数据删除;数据存储	在大数据时代，受大数据自身特点的影响，大数据备份呈现出新的特征，包括备份方式的改变，大数据对实时备份和恢复性能要求的提高，大数据的数据类型和数据价值的多样化以及备份服务等级的多样化等，基于传统数据备份的重复数据删除方法受到了空前的挑战。本项目拟研究多项适用于大数据备份的重复数据删除关键技术，包括（1）基于语义感知的多粒度冗余鉴别方法，根据语义环境进行多粒度的冗余数据鉴别，提升重复数据删除吞吐率，满足大数据的高性能数据备份需求；（2）基于数据重复相关性的数据分布和放置策略，优化存储节点内和节点间的数据布局，提升实时恢复性能，满足大数据的高性能数据恢复需求；（3）数据冗余度估算模型和自适应的多性能目标优化模型，在数据去重前估算基于数据类型的数据冗余度，配置优化的数据去重方法，满足大数据所需的多种数据备份服务等级需求。本项目拟通过对上述内容的研究，最终构建适用于大数据备份的重复数据删除存储系统。
具重复数据删除大规模存储系统可靠性技术研究	重复数据删除;数据可靠性;ECC纠错码;大规模存储系统	随着数据的爆炸性增长和总拥有成本的不断上升，重复数据删除技术现已成为大规模存储系统中的重要组件。海量数据可靠性是数据组织管理和应用的基础。为了提高存储存储系统的利用率，目前普遍采用的带重复数据删除的存储系统中，其数据可靠性却大大降低了。本项目拟总结和分析当前主要的大规模存储数据高可靠性保证机制和算法，在此基础上，针对带重复数据删除的大规模存储系统的特殊特点，拟提出和设计一种适用和高效的数据高可靠性保证机制及算法-TH-ADMAD。TH-ADMAD综合应用纠错码ECC和分布式编码及恢复算法，在保证数据可靠性要求基础上，充分发挥存储效率。项目研究对于充分发挥海量数据存储组织效率，提高数据可靠性具有重要理论和实践意义。
可动态重构的高可靠嵌入式系统总线研究	嵌入式系统;系统总线;动态重构;容错;SoC	随着SoC技术的广泛应用，高可靠嵌入式系统功能集成化与小型化要求日益迫切。而用于系统内部互连的并行总线尺寸大，高速串行总线不支持多点直接连接，已成为嵌入式系统进一步发展的主要瓶颈之一。同时，由于缺乏有效的容错机制，内部总线也已成为高可靠嵌入式系统中一个主要的单点失效环节。项目组在研究嵌入式系统总线数据传输特征的基础上，提出一种面向嵌入式系统内部模块连接的高速高可靠总线。它支持多节点直接互连，能够实时监测总线通道故障和节点电路故障，通过动态重构实现总线容错。.项目主要从拓扑模型、动态重构方法、故障检测及差错控制等方面建立上述新型总线的基本模型与理论基础。为此，项目首先研究建立总线通道及节点电路的故障集及故障响应模型；然后研究总线故障实时检测方法和总线动态重构算法，设计总线通信协议规范；最后构建基于FPGA的试验验证平台，研究总线性能的综合评价方法，对总线的通信性能、容错能力进行评估。
LTE-A蜂窝网络中D2D通信资源分配机制的随机模型与性能评价研究	设备间直接通信;随机Petri网;高级长期演进	在LTE-A蜂窝网络中引入设备间直接通信（D2D）功能能够提高系统的频谱利用率并使得设备间的发现更加快速灵活。由于具有D2D功能的蜂窝网络拓扑既不同于传统蜂窝网络，也不同于传统多跳网络，因此需要建立新的性能评价模型对D2D通信所带来的性能增益进行评估并对资源分配机制的优化设计进行指导。目前关于LTE-A蜂窝网络中D2D链路资源分配及性能评价的研究刚刚起步，大都假设用户数目不变且具有无限积压业务。与上述研究不同，本项目基于业务随机动态到达的假设建立D2D通信的分组级和流级性能评价模型，通过排队论、随机Petri网、随机网络演算等理论工具，对支持D2D功能的LTE-A无线蜂窝网络的统计服务质量等进行性能分析，定量得出LTE-A无线蜂窝网络中引入D2D功能后在时延，吞吐量等方面取得的性能增益，并对各种考虑链路之间可能存在的相互干扰的资源分配机制进行性能分析和比较。
针对通信时滞与信道干扰的无线网络实时资源分配与性能优化	无线网络;通信时滞;跨层优化;资源分配;信道干扰	无线网络业务的增长及多样化对无线网络的服务质量提出了更高的要求。无线网络的资源有限、通信时滞大、信道动态变化等特点,使得网络资源的有效利用和公平分配非常关键，并且十分困难。本项目拟研究在信道随机干扰和时滞环境下无线网络资源的公平分配模型及实时分配方案。在随机信道干扰下研究基于效用函数理论公平的实时资源分配方案；并在此基础上建立起以时滞性能和用户效用相结合为目标的无线资源分配模型及优化方案；最后研究多业务混合的网络资源分配框架，并提出数学优化模型及解决方案。项目将采用相关的数学理论和仿真手段开展研究，力求使研究结果能应用于无线网络工程。
面向在线社会网络的信息传播机制、控制方法及关键技术研究	在线社会网络;计算机系统安全;级联失效;信息传播与爆发;信息预测与控制	随着Web2.0技术的诞生和迅速发展,互联网进入了在线社交网络的时代。由于信息发布、传播的开放性和自由性,在线社交网络中的信息传播活跃性达到前所未有的高度。研究在线社交网络的信息传播规律,不仅有助于加深对网络结构和用户群体行为的认识,也能有助于引导在线社会网络舆情,因而具有重要的理论价值和研究意义。本项目旨在深入研究开放环境下社会网络的信息传播机制的若干关键问题，解决目前日益受到广泛关注的互联网信息传播的信息安全问题。项目专注于社会网络的信息传播规律、社会网络的信息传播预测、社会网络的信息传播控制以及社会网络信息传播分析及控制系统四个方面的研究，以期发现社会网络的信息传播规律、有效地对社会网络的信息传播态势及爆发行为进行预测、精细化地对社会网络的信息传播进行控制。实现揭示并驾驭社会网络的信息传播规律！其研究成果在信息传播、商业营销、信息推荐、社会管理等多领域都具有很强的参考价值及借鉴意义。
云计算环境中面向数据密集型任务的能效优化策略研究	能效优化;资源虚拟化;云计算;数据密集型	随着以大数据为背景的数据密集型应用的兴起，传统面向云数据中心的能效优化技术和方法遇到了一系列具有挑战性的课题，其难点在于："数据密集"这一特性对云平台中多个服务层的性能和能效都有重要的影响。针对数据密集型任务的执行期特性，本课题从资源虚拟层、云存储层、数据管理层、应用管理层四个方面提出了一套完整的能效优化策略，以期综合地解决云计算平台在部署数据密集型任务时的能效优化问题。在理论研究方面，本课题提出的"虚拟层I/O缓冲机制和调度算法"、"二维副本活跃度评估模型"、"非结构化数据管理系统能耗模型与数据布局策略"以及"基于相对I/O能耗模型的延迟迁移机制"有望对云环境中数据密集型任务的能效优化研究提出新的思路和技术；在实际应用方面，本课题提出的以上方案均可以在现有的云计算平台中实现，并直接应用于各类成熟的云计算平台，以期降低云数据中心的能耗成本。
认知无线网络中的信道绑定/聚合算法	信道绑定/聚合;优先级队列;不完全感知;保护信道;认知无线网络	当认知无线网络中存在多个空闲信道时，认知用户可以选择绑定或者聚合其中的两个或两个以上的信道，这样使认知用户能共享更宽的信道，提高信道容量与利用率，同时有利于传输突发业务，缓解近邻信道干扰。而信道绑定/聚合的算法与策略对认知无线网络性能的改善至关重要，为此本项目将研究信道绑定/聚合算法与策略问题，拟提出一种最小时延与能量有效的信道绑定/聚合算法与一种近邻信道干扰情况下的信道绑定/聚合算法；拟引入跨层设计思想，综合考虑认知用户的业务特性，提出一种基于业务优先级队列的信道绑定/聚合策略；拟研究信道感知误差对信道绑定/聚合算法的影响，提出不完美信道感知条件下的信道绑定/聚合算法。本项目的研究成果将为认知无线网络的实际部署与应用提供相应的理论支撑。
大规模多核系统芯片测试压缩方法研究	系统芯片;测试压缩;大规模多核;低功耗测试;未知位	芯片性能的提升日益依赖于芯核数目的增加，大规模多核系统芯片是在片上集成多个处理器核以增强计算性能。针对同构多核结构特征而采用的测试压缩方法，能够有效减少测试数据量以降低测试成本。本项目将协同考虑激励压缩、响应压缩和测试功耗等因素，研究同构多核系统芯片的测试压缩方法：（1）研究核间压缩和核内压缩结合的两层测试激励压缩方法，使用同一共享测试数据源和广播式方法进行核间压缩，使用可重构广播式方法进行核内压缩，以减少需存储的测试数据量；（2）研究同构多核的低功耗并行测试方法，在扫描移入、功能捕获和扫描移出三个阶段采用错位机制，以有效避免峰值功耗叠加；（3）研究同构多核的响应互比压缩方法，并结合响应中的未知位分布，设计优化的编码方案以屏蔽未知位，最终实现基于单比较电路的同构多核互比，以减少压缩电路的硬件开销。本项目将结合多核处理器开展实验研究，为大规模多核系统芯片的测试数据量问题提供有效方法和关键技术
高通量网络仿真的弹性调度模型与方法研究	同步;虚拟机;网络仿真;事件调度;负载均衡	随着网络技术的发展，网络规模不断扩大、网络新技术与新应用日益涌现、网络安全问题日益凸显，已使得传统的网络实验技术无法满足当前各类网络系统、协议及网络行为的研究、验证需求。不妨把新兴网络应用和网络安全研究等对大规模、高吞吐率、异构接入以及多用户交互能力的实验需求统称为对高通量网络仿真的需求。由于当前的大部分网络实验方法都无法完全满足这一需求，因此当前对网络新技术和网络安全的研究只能基于现有网络实验技术进行有限的扩展和改进，效率低、适应性差。因此，深入、系统的研究支持高通量网络交互仿真的关键调度机制与算法，具有重要的研究意义和实用价值。在此背景下，我们提出面向高通量网络仿真的弹性调度机制研究，它将流量仿真与离散事件模拟技术有机结合，尝试从流量、时间、空间三个维度提高网络仿真系统对计算负载的适应性处理能力。
体系结构级GPU功耗建模及软件低功耗优化方法研究	体系结构级功耗建模;软件低功耗;任务均衡划分;关键路径查询;图形处理器	随着处理器功耗不断增大，功耗问题逐渐成为高性能计算机系统设计与实现的首要问题。低功耗优化技术将提升系统性能的同时防止功耗过快增长，并提高系统的可靠性及可用性。 本课题开展体系结构级GPU功耗模型及软件低功耗优化方法研究，面向CPU-GPU异构并行系统，以降低系统能耗，提升系统效能为目标，拟重点研究计算与存储划分的动态功耗模型；研究基于温度实时感知的静态功耗模型；并在该模型基础上，研究面向程序执行特征的关键路径查询动态功耗优化方法；研究基于任务均衡划分的多维GPU静态功耗优化方法；设计并实现功耗模型及功耗优化方法验证系统。 本课题将在体系结构级GPU功耗建模、基于关键路径分析的动态功耗优化算法、基于自适应任务划分的静态功耗优化算法，以及功耗模型及功耗优化验证理论与方法等方面形成创新性成果，为GPU低功耗优化研究提供理论依据与技术支撑。
片上多核主动适应存储体系结构研究	存储器系统;主动适应;微处理器;体系结构;片上多核	多核多线程技术可使处理器的性能得到极大提高，但同时也对存储系统提出了更高要求。等效存储访问延迟和有效存储带宽的问题已成为影响多核多线程处理器性能提高的重要因素之一。.我们提出片上多核主动适应存储体系结构，研究把智能存储系统技术与多核多线程技术结合起来。将原先的被动处理访存请求造成的瓶颈转化为智能片上存储系统主动将要处理的数据推向适合的处理核，从而实现以存储系统为信息中心，处理核服务于相关信息处理的目标。.本项目将研究在处理器核个数和线程个数不同时，使用恰当的分布式多级cache结构、表预测策略、线程预取推送方式等提高多级cache的命中率。研究主动适应计算需求变化，使用存储辅助线程进行多核多级cache预取、数据主动调度、线程迁移、多核间的快速有效互连等方法，探索获得等效访存延时减少和有效存储带宽增加的渠道。
面向无线移动网络环境的协作化流媒体服务技术研究	无线移动网络;自组织协作;社会网络;移动流媒体服务	本项目将以优化无线网络环境中移动流媒体服务质量、提高移动流媒体服务系统可伸缩性为目标，以面向无线网络环境、建立基于移动节点自组织协作的移动流媒体服务体系为核心，以分析/挖掘移动网络环境中用户活动模式、流媒体服务关联性所呈现的规律/特征为切入，从基于移动节点协作的流服务覆盖网络拓扑管理与演化机制、支持移动流媒体自组织协作服务的缓存管理、面向无线自组织环境的流媒体缓存激励机制等层面开展工作。在此基础上，从增强无线流服务覆盖网络拓扑管理、提高移动流媒体自主协作能力、优化流媒体缓存激励效能等角度对支持移动流媒体服务所涉的关键技术及协议支撑机制开展研究。项目预期成果包括：建立受社会网络启发的流服务协作覆盖网络拓扑管理与动态演化机制、提供基于用户关联关系社会网络的移动流媒体动态缓存管理与容量平衡机制与评估模型、建立基于经济学模型的自适应协作式缓存激励机制与公平性评价模型。
面向可信互联云服务的轻量级信任管理与计算	信任管理与计算;可信云服务;互联云;资源匹配;服务数据感知	可信服务是云计算赖以生存的基石，用户愿意把最敏感的数据交给云数据中心去管理，就是建立在用户对云服务信任的基础之上的。然而，随着互联云时代的来临，云环境中存在着数以万计的服务资源和数以千万计的服务请求，为了及时地提供高可信的服务，系统需要在瞬间高效地处理数以千万计的服务请求和匹配数以万计的服务资源。在此背景下，轻量级(高速、低开销)的信任管理与计算方法成为了实现可信互联云服务的基本需求。本项目以面向可信互联云服务的轻量级信任管理与计算问题为导向，系统深入地研究基于云服务中介(CSB)的可信互联云体系架构；新型的基于主动式计算软件传感器的互联云服务数据高效感知及快速预处理技术；基于感知数据的轻量级互联云服务可信态势评估与预测模型；可靠性增强的轻量级服务推荐机制等关键问题。本项目的研究对于建立可信的互联云服务基础架构和轻量级的互联云服务信任管理与计算方法，具有重要理论意义和实用价值。
移动终端感知安全管控机制与优化方法研究	感知控制;策略管理;计算机系统安全;移动安全;访问控制	拥有丰富感知能力移动终端的普及，极大改善了移动用户应用体验，但也改变着移动终端所遭受的攻击面，因而对移动终端系统安全提出严峻挑战。移动终端感知安全问题面临着持续感知能力和混合感知数据安全管控挑战，解决该问题也亟待改善策略管理自动化程度。为此，本项目拟从"安全模型-管理机制-分析及优化实现"三个视角展开项目研究：面向移动终端泛在感知能力和敏感数据安全构造自适应管控模型；面向移动互联环境设计多维度协同策略管理机制和协议，提升授权过程自动化；分析移动终端安全管控机制安全性，设计高效感知安全管控算法，并度量和优化新机制对当前移动终端应用生态的影响。本项目旨在为移动终端面临的感知安全问题，特别是为持续感知能力和混合感知数据的安全管控提供有效机制与方法，为研究自适应访问控制的安全性分析奠定方法基础，为分析安全技术演进对移动终端应用生态影响提供度量方法指导，为移动终端操作系统的安全性设计提供良好借鉴。
动态与不完全覆盖容错系统可靠性建模、分析与优化关键技术研究	不完全覆盖;可靠性模型;可靠性分析;可靠性优化;动态故障树	容错技术是实现高可信系统的关键技术之一。在容错系统中，系统行为的动态性与故障覆盖的不完全性是系统安全可靠性的两个重要研究课题。本项目主要围绕模型简化、有效解析与系统优化这三个科学问题展开研究工作。首先针对动态性，从传统动态故障树定性分析的最小割序排列问题和定量分析的状态空间爆炸问题出发，研究从动态故障树到静态故障树的转换（简化）方法及其对应的有效组合定性定量解析方法。其次，针对传统不完全覆盖模型只注重覆盖故障组件而忽视组件的全局相关性问题，研究同时覆盖故障组件与无关组件的各种无关性覆盖模型，以及基于无关性覆盖的组件相关度与重要度和系统多目标优化方法，通过模型优化与结构优化有效提高系统可靠性。最后，集成融合上述两方面的研究成果，建立基于无关性覆盖的动态复杂系统的可靠性建模理论及其分析与优化方法。本项目的研究有助于为动态与不完全覆盖容错系统的可靠性建模、分析与优化提供新的理论、方法与技术支持。
车联网环境下数据驱动的云服务访问建模及任务优化策略研究	能效优化;车载云计算;任务调度;车联网;数据驱动建模	车联网云计算是实现车载应用服务的重要技术方向，而车联网复杂动态特性导致云服务访问效能受多种网络环境因素制约，给访问优化带来巨大挑战。现有的服务访问优化策略通常针对特定网络参数建立先验模型设计优化方案，无法体现车联网动态特性对访问优化过程的影响，导致服务响应滞后于服务请求的变化。因此，建立能够综合反映车联网复杂特性的服务访问动态模型，是解决问题的关键。本项目将根据交通离散数据，将数据驱动的连续性建模思想引入到车联网云服务访问建模过程，拟研究针对云服务访问过程的，基于数据处理分组神经网络方法的离散-连续建模策略及其优化方案，并将其应用于服务访问的接入、调度和交付环节，建立相关网络参数的动态连续性模型，进而构建相应的动态优化策略。为评估模型方案的可行性，项目将通过物理验证平台和基于交通数据集的网络耦合仿真平台，从不同层面对提出的模型方案进行综合评估，为车联网云服务建模和优化提供理论依据和方案。
高可靠低功耗片上互连网络体系结构关键技术研究	片上路由器;低功耗;体系结构;可靠性;片上互连网络	随着处理器的多核化发展趋势，片上互连网络较好克服了传统总线结构的诸多不足，能够在芯片多内核之间提供一种简单高效的沟通机制，但工艺的不断发展与内核数量的继续增加给片上互连网络带来了传输延迟、通信带宽、系统功耗与可靠性等方面的严峻挑战。本课题将提出一种支持高可靠低功耗的高性能片上互连网络体系结构，并深入研究其设计实现中的关键技术，主要包括：支持单周期路由转发的高带宽片上路由器体系结构；系统层、体系结构层、电路实现层等多个不同层次的片上互连网络低功耗技术；门级与体系结构级的容错加固技术、错误检测恢复技术与可靠性评估；片上互连网络的解析式性能分析方法与体系结构级功耗评估模型，以及高效的片上互连网络硬件资源优化设计方法等关键技术。本课题的研究可以为未来超高性能处理器片内互连结构的设计与实现奠定坚实的理论和技术基础，具有重要的理论意义和应用价值。
互连网络条件路覆盖与综合诊断策略下的故障诊断性研究	互连网络;并行系统;图论;故障诊断;连通性	故障系统中数据传输路径的选择和构造，以及通过故障诊断进行系统维护，是高性能互连网络设计以及可靠性分析的主要研究方向之一。而依据实际需求，对高阶互连网络的故障分布、路径设计以及故障诊断策略均提出相应的约束条件后，在路径构造以及故障诊断性方面还有许多重要的问题有待解决。本项目拟研究当互连网络满足一定约束条件时，能够充分使用和覆盖网络所有节点的路径的存在性，以及故障系统的诊断能力。其中，约束条件主要包括：(1)根据实际情况，为提高网络的可靠性，采用相应的网络故障模型；(2)为提高数据传输效率以及安全性，可同时设计多条互不相交的路径，并且构造的路径需要经过预先指定的多条互不相交的线路；(3)结合网络的结构特点和故障模型，设计并综合使用多种故障诊断策略，达到大幅度提高故障诊断能力的目的。同时，拟编写高效故障网络路径设计算法和故障诊断算法，为设计和选择大型并行系统互连网络拓扑结构提供有效理论依据。
基于空间覆盖的智能手机低功耗移动感知技术	嵌入式系统;移动感知;移动计算系统;智能手机	针对移动感知应用中的不确定因素，基于智能手机研究面向空间覆盖约束的低功耗移动协同感知技术。通过引入概率理论，采用云辅助技术分析目标感知区域内智能手机的概率分布，研究面向运动轨迹预测的低功耗手机传感器调度方法，使智能手机在持续执行感知任务时，有效控制传感器的使用频率，降低手机能耗；结合手机用户的历史运动特征采用Markov预测模型，研究基于概率的智能手机运动轨迹预测，有效解决手机运动轨迹不可控的不确定性问题；以智能手机的运动轨迹为基础，研究多手机动态协同感知的空间覆盖估计方法，并设计高效启发式优化算法实现手机选择和感知任务分配，减少冗余感知数据，降低感知系统总体能耗。研究成果为构建低功耗移动感知系统提供技术支持与核心算法，对物联网信息感知与交互技术的发展与应用具有重要的现实意义。
云数据中心并行应用多目标优化调度算法研究	云数据中心;任务调度;负载调度;科学工作流;工作流调度	针对云数据中心负载动态变化、虚拟机性能波动带来的任务执行时间不确定、竞价拍卖型虚拟机价格动态变化导致的虚拟机存活时间不确定等问题，开展中长期并行负载与大规模科学工作流优化调度算法研究。一是研究利用虚拟机集群弹性配置、多策略协同、时间约束策略仿真优选技术，实现中长期并行负载的成本效益感知调度；二是研究利用概率性能建模、截止期划分、动态约束违背纠正技术，实现科学工作流的可靠调度；三是研究利用按需竞价、工作流结构剖分、选择性检查点技术，实现科学工作流的容错调度。依托国防科大银河云环境和广州超算中心天河二号云环境，采用并行负载踪迹和实际科学工作流对提出的算法进行测试验证。本项目的实施，将为中长期并行负载和大规模科学工作流在云数据中心环境下实现成本效益感知的容错可靠执行打下坚实的算法基础，为科学家利用云计算新模式高效开展科学实验做出积极贡献。
面向纠删码云存储系统的数据快速重构技术研究	降级读;数据重构;云存储系统;纠删码	大数据时代下，很多云存储系统都采用纠删码技术，以保障数据的可靠性。随着存储规模的增长，每年发生故障的存储设备变得越来越多，如何快速重构故障设备中的失效数据，已逐渐成为了学术界的研究热点。本项目拟研究面向纠删码云存储系统的数据快速重构技术，结合国内外的研究现状，分别在数据快速重构与"降级"访问失效数据的过程中，对所需访问的数据总量、异构存储介质间的访问并行度与相对负载均衡等方面进行优化。具体的研究内容包含以下几方面：（1）针对现有纠删码技术，研究用于优化数据快速重构效率的算法，以适应异构存储环境的需求；（2）针对云存储系统的特点，设计新的纠删码技术，进一步提升数据快速重构的效率；（3）针对"降级读"操作的特点，设计能够提升"降级读"性能的纠删码技术。本项目将搭建真实的云存储平台验证所研究的内容，为面向纠删码云存储系统的数据快速重构技术研究提供一定的理论依据和实验基础。
支持对象数据与动态功能扩展的存储片上多处理器体系结构研究	片上多处理器;对象存储;主动存储;存储处理器	目前主流的存储处理器普遍采用静态的、硬件固化的功能扩展模式,无法满足存储应用的多样化功能扩展需求。针对这一问题，本课题提出了以可编程方式实现动态的存储功能扩展的设计理念- - 即依托微电子工艺的进步，在存储处理器内部集成额外的处理器内核等部件，以支持存储功能扩展代码的片上下载与执行。课题的目标为：研究实现以对象数据为存储数据类型、以代码的动态下载与执行为功能扩展模式、以执行型命令为软硬件接口扩展的基于片上多处理器架构的存储处理器体系结构模型。主要采用对于典型应用的存储行为跟踪、性能分析、系统模拟与评测等研究方法，进行着重于系统级处理器模型方面的研究：包括处理器软硬件功能划分、扩展机器指令设计、结构优化与性能模拟测试等内容。预计这一新型的存储处理器架构可适用于被互联网应用广泛使用的半结构化数据的存储处理，从而提高硬件使用效率以及存储应用的整体性能。
基于危险特征和协同机制的计算机病毒免疫方法研究	协同机制;危险特征相关性网络;增量学习方法和并行算法;计算机病毒免疫;危险特征	本项目基于生物免疫原理，开展基于危险特征和协同机制的计算机病毒免疫方法研究。首先，研究基于免疫原理的计算机病毒危险特征提取方法，并进一步对危险特征的危险度进行客观度量；然后，借鉴生物适应性免疫应答中的免疫信号协同机制，将其融入到计算机病毒免疫方法研究中，显著改善计算机病毒免疫方法的检测效果；接着，对不同的计算机病毒危险特征之间的相关性进行分析，建立危险特征相关性网络，挖掘计算机病毒之间的关系，作为分析和检测计算机病毒的工具；最后，在上述算法的基础上，提出计算机病毒免疫增量学习方法，克服现有杀毒软件被动记忆特征码和特征库庞大的缺陷；同时，提出并实现计算机病毒免疫方法的并行算法，以适应计算机多核处理器的发展趋势。本项目工作的完成将对计算机病毒免疫方法的研究水平具有极大的推动作用。
基于阻变器件的存储计算深度融合多尺度可重构架构设计研究	基于忆阻器的计算;内存计算;可重构计算	近年来，ReRAM阻变器件为不同于传统CMOS工艺的电路和体系结构设计提供了新的发展动力和方向。面对传统计算系统中因计算和存储资源分离所造成的通信瓶颈问题，ReRAM统一的阻变工作机理为两者融合的新型结构设计和计算方法提供了根本保证。本项目面向未来通用大数据信息存储和高强度数据处理需求，突破传统工艺和体系结构设计约束，以基于ReRAM阻变机理的新型计算理论和方法为研究对象，以建立通用阻变计算的新型体系结构和计算模型为研究目标，通过计算存储深度融合的基本电路设计探索，提出资源形式、算子结构和计算精度多尺度可重构计算形式，实现通用高效的大规模并行计算兼存储原型架构，构建计算存储深度融合条件下的新型计算模型，突破现有通信瓶颈以实现通用处理性能的提升，从而解决通用阻变计算的主要理论方法、结构实现和应用技术等关键问题，促进计算模式从过去的以计算为中心向以数据为中心的根本转变。
设计用于神经计算和高密度信息存储的金属-绝缘体-金属纳米结构	ReRAM;MIM;神经计算机;电子突触;阻变	神经计算机能够使人们用很有效的方法进行多种复杂操作，这对社会具有巨大的积极影响。其基本单元叫做突触，运行过程可以用金属-绝缘体-金属（MIM）结构来简单模拟，该结构可通过改变外界参数来导通或阻断电流。但在将该器件构筑成神经计算机前还需要解决一些问题，如器件的转换速度、能耗、耐受性、可叠加性和单元间的噪声影响。在该项目中，我们将致力于制备用于电子突触的性能优良的MIM器件。另外，因为MIM本身就能构成非易失性存储单元，所以我们的工作可直接影响市场。我们用来改善MIM器件性能的主要方法包括使用有先进功能的新材料，这可能并不能解决上前述问题，但可能提高器件如柔性和透明性的性能。而且我们将设计并制备可高度集成的结构。为实现该目标，我们将把器件尺度和纳米尺度制备以及表征结合起来。该项目的最显著标志就是我们将会使用导电原子力显微镜并将其与半导体分析测试仪联接在可控环境中。这一先进设施全世界只有三个研究
结合设备老化效应量化建模的低功耗片上网络可靠性动态优化方法	可靠性设计;功耗优化;片上网络;负偏置温度不稳定性	设备性能老化效应下的可靠性设计是片上网络研究领域的一类重要问题，如负偏置温度不稳定性可能导致片上网络的设备发生20%以上的性能退化，甚至发生功能故障。本项目拟针对设备老化效应导致的片上网络可靠性问题，深入研究老化效应下的低功耗片上网络的动态优化方法，提出一系列的理论和算法模型，在提高片上网络系统效能和可靠性的同时有效缓解老化效应对设备性能的退化影响。建立合理的理论模型以量化片上网络设备在老化效应下的性能差异。并以量化的能力指数作为负载约束，设计新型的动态分区和动态任务分配算法，实现片上网络的负载平衡并缓解设备性能退化对系统可靠性的影响。提出新型的运行工况自修正优化框架，实现功耗高效和减缓老化效应的协同优化。本项目的研究成果可应用于实际片上网络多核系统并可推广至其它具有可靠性设计要求的高性能并行计算系统，具有广泛的应用前景、重要的理论价值和实际意义。
中子点火MC输运问题可扩展并行计算研究	中子点火;MC输运;负载平衡;并行计算	本项目研究中子点火MC输运问题在大规模多核体系结构并行机上的可扩展并行计算。中子点火MC输运计算在具有传统MC输运计算特征的同时,具有很强的时序性，这为其进行大规模可扩展并行计算带来了很大的困难。本项目深入分析中子点火MC输运问题的计算特征，结合新型多核并行机的体系结构特点，研究一套有效的三层可扩展并行计算模式。第一层采用Master/Slave模式的任务级并行，其中Slave端对应的是一个处理器组；第二层在各个Slave端的处理器组内部采用粒子分段划分的方法实现对所有中子按时间点进行同步跟踪计算；第三层采用OpenMP多线程方法实现MC输运几何计算的并行处理。同时研究三层并行计算模式下各层独立的负载平衡方法，主要研究源中子的动态分配技术和反应区内中子的动态迁移策略。通过本项目的研究使得中子点火MC输运程序在数千个处理器的新型超级计算机系统上达到80％以上的并行效率，处于国际领先水平。
面向超高性能计算的众线程宽向量微体系结构	高性能计算;微体系结构;模拟器;宽向量;众线程	未来ExaFLOPS级超高性能计算已经对微处理器体系结构研究提出紧迫的需求。传统通用处理器架构的性能提升空间有限，新型处理器架构往往因改变用户编程习惯而难以迅速推广。本项目基于向量和多线程编程模型，提出一种众线程宽向量微体系结构，利用宽向量的数据并行提高峰值性能，利用众线程的交叉执行提高实际性能。主要研究内容有：（1）面向高性能计算的宽向量指令集扩展，包括向量宽度的设计、向量寄存器扩展和向量操作指令扩展等。（2）众线程宽向量执行模型，包括指令调度策略、向量执行部件设计、存储系统设计和访存调度策略，以及微体系结构设计空间探索等。（3）模拟器设计，支持快速功能模拟器和精确的多指标细节模拟，基于典型程序的模拟测试来验证众线程宽向量微体系结构的有效性。本项目的研究成果将为未来高性能计算做技术储备，为传统程序进一步提高性能提供硬件基础，为提高国产CPU设计能力和实用性能贡献力量。
基于DNA可编程自组装技术的分子计算研究	可编程纳米结构;分子逻辑门;DNA自组装	DNA可编程自组装技术是在分子计算探索研究中发展起来的，通过DNA分子Tile的粘性末端互补配对实现Wang Tile自组装过程。DNA分子自组装技术不仅对分子计算的研究有重要意义，同时在纳米分子材料，分子传感器，纳米分子器件及电路研究中有广泛的应用前景。目前DNA可编程自组装存在的主要问题是自组装计算过程的可靠性，以及自组装的可扩展性，本课题拟通过对DNA分子Tile构造及可编程粘性末端特性的进一步研究，引入DNA链置换反应构造可编程分子全处理器Tile，实现DNA分子Tile级联自组装四则运算；在实现可编程分子Tile计算模型的基础上，结合DNA折纸技术，研究基于DNA折纸技术的大规模DNA分子自组装计算方法，探索可编程分子Tile自组装在二维平面自组装纳米材料及纳米器件中的应用；并将DNA分子自组装计算模型应用于其他仿生计算模型如神经计算、膜计算等。
基于IPv6的无线传感网协议转换与组网关键技术研究	网关;无线传感网;网络接入;适配层	当前，无线传感网的传感类型与应用场景更加多样化，与此同时，IPv6技术正取代传统的IPv4网络，成为新的网络互联标准。如何实现多元化的无线传感网与IPv6网络间的无缝互联，成为亟待解决的问题。本课题首先研究基于网关的IPv6与无线传感网间的互联架构、协议转换模型与协议栈精简方法，利用网关实现基于IPv6的无线传感网编址寻址；接着，将进一步研究基于网关的IPv6无线传感网接入机制，实现基于虚拟IPv6的网关互联方案以及面向IPv6的网关信息处理技术；在此基础上，重点研究基于IPv6的无线传感网适配层关键技术，包括拓扑管理、动态路由协议以及基于移动基础设施的6LoWPAN数据收集技术，设计IPv6网络支撑下的多元无线传感网安全方案，研究其密钥管理与认证技术、网间接入认证方案、数据加密技术等；最终，将构建一个通用的验证平台，通过仿真与系统实测相结合的方式对本课题的理论进行验证与完善。
面向大规模蒙特卡罗科学计算的FPGA加速关键技术研究与应用	FPGA加速;多维高斯随机数;信用违约互换;蒙特卡罗计算;位宽优化	蒙特卡罗模拟是一种广泛应用于分子物理学、金融工程学等领域求解科学计算问题的重要方法。随着科学技术的发展，运用蒙特卡罗方法解决实际问题的复杂性不断增大，导致对计算设备运算能力的需求也在不断地增强。开发新的计算模式以实现对蒙特卡罗模拟进行加速已成为现代科学计算急需解决的重要问题。近年来，FPGA芯片以其可重构、支持细粒度并行、高性能、低功耗等优势，已成为理想的计算加速平台。然而，FPGA加速蒙特卡罗计算还面临着算法特征多样性、算法并行结构设计困难、硬件结构优化复杂等挑战。.本项目以FPGA可重构体系结构为研究平台，在深入分析蒙特卡罗科学计算特点的基础上，对利用FPGA加速蒙特卡罗计算所需解决的浮点转定点、并行随机数生成等关键技术进行深入地研究。并在此基础上实现光子/电子联合输运方程以及金融工程领域CDS定价模型的FPGA加速系统。研究成果将为我国高性能科学计算的发展提供坚实的理论与技术支撑。
三维片上网络通信SoC及安全性研究	嵌入式操作系统;任务调度;存取控制模型;片上网络	基于片上网络通信的嵌入式多核SoC成为嵌入式发展主流。本项目通过分析片上网络通信特点，研究类蜂窝状的三维网络拓扑和快速自适应路由算法，在降低片上资源占用和功耗的同时，更加快速高效的实现多核间通信；针对嵌入式操作系统在异构多核任务调度方面需求，建立能真实反映任务间制约关系的任务模型，设计快速任务调度映射算法，增强操作系统的多核任务调度能力，提高整个系统的运行效率；同时，针对嵌入式系统来自安全方面的威胁与攻击的现实，结合NoC体系结构，研究嵌入式系统在自主安全防护方面的策略及方法，通过增强操作系统安全访问控制模型，提高整个系统的安全等级，有效保护系统安全。研究成果将形成一个具有高安全等级、高速片上网络通信、高效任务并行调度的SoC系统，可广泛应用于各种嵌入式环境中。
面向自供能嵌入式系统非易失处理器的关键技术研究	嵌入式系统;自供能;混合内存;非易失处理器;备份感知	为解决自供能嵌入式系统中供电不稳定而引发系统断电的问题，非易失处理器将非易失性存储介质引入传统处理器和内存空间，在系统断电前将程序数据备份至非易失性存储介质，系统继续供电后将程序状态恢复，使得程序可以继续执行。非易失处理器的设计可以提高程序执行效率，但其应用仍存在诸多问题。首先，数据备份存在一致性隐患，缺少正确性维护的备份过程可能引起程序执行错误。其次，由于传统的内存管理机制未考虑备份过程，因此并不适合直接应用于基于非易失处理器的嵌入式系统。本项目将深入研究基于非易失处理器嵌入式系统的备份技术，构建多层混合内存中数据备份正确性模型，并从编译、操作系统等软件层次以及缓存控制器的硬件层次研究备份感知的设计和优化方案，旨在提高系统性能，降低其能耗，推动非易失处理器的可靠高效应用。
基于AUTOSAR新平台标准的汽车CPS自适应安全调度	AUTOSAR自适应平台;调度;自适应;安全;汽车信息物理融合系统	AUTOSAR自适应平台标准是于2017年发布的支持汽车CPS动态与并行、以及汽车功能安全标准ISO 26262的新平台标准。本项目旨在基于AUTOSAR新平台标准，研究汽车CPS的自适应安全调度。 在建立基于AUTOSAR新平台标准的汽车CPS模型的基础上，研究其自适应安全调度的2个关键问题：功能安全需求验证与自适应调节。分别提出符合ISO 26262标准的快速的功能安全需求联合验证方法和适应AUTOSAR新平台标准的汽车CPS低开销与积极主动的自适应调节机制。通过本项目的研究，将汽车CPS与AUTOSAR新平台标准建立紧密的联系，为汽车CPS的自适应安全调度算法研究提供有价值的理论成果及设计参考。
面向高速网络内容安全处理的专用系统结构	网络内容安全处理系统;专用指令集;并行算法;模式串匹配算法;专用系统结构	通用处理器的存储容量和运算速度的增加已经远远落后于高速网络带宽的增长速度，传统的高速网络内容安全处理系统已经面临着巨大的挑战。本课题研究网络内容安全处理系统的硬件加速策略，提出一种基于"通用处理器+专用处理器"架构的网络内容安全专用处理系统结构。该系统既具有通用处理器的灵活性，又具有专用硬件处理的高性能。本课题的主要研究内容包括：选择典型的网络内容安全处理算法进行并行化改造，抽象出适合硬件实现的专用指令集；设计一个网络内容安全处理专用系统的模拟器，用来评估专用系统的性能；采用硬件的超级并行和深度流水线技术对专用指令集进行优化实现；基于该专用系统结构设计新的网络内容安全处理算法。本课题的研究成果可以广泛应用于高速网络安全、信息内容安全甚至网络信息处理等领域，可以大幅度地提升我国信息安全基础设施的处理效率，具有巨大的经济效益和社会价值。
云系统低速流DoS攻击防御关键技术研究	应用流;局部特征;隐半马乐可夫模型;低速流DoS攻击;优先级排队与流量控制	低速流DoS攻击降低了云系统的服务质量、增加了云租户的服务成本，阻碍了云技术应用的普及和推广。针对此问题，本项目研究内容包括：（1）结合雷尼信息熵理论，研究到达云系统的各种应用流的识别方法，以及从各种应用流中分离低速流的方法；（2）针对低速流可利用的局部特征信息较少问题，构建卷积神经网络---多矩阵化分类器层次模型，以二次抽取模式的局部特征，丰富低速流的特征信息，为有效检测低速流DoS攻击奠定基础；（3）运用隐半马尔可夫模型理论，建立能精确描述到达云系统的各种低速流特性的HsMM和有效检测各种低速DoS攻击流的在线检测算法，发现各种低速流的DoS攻击行为；（4）研究透明的在线排队与流量控制技术，消除各种低速DoS攻击流对云系统的影响。该项目的研究成果为云系统安全检测提供了一种新的方法和思路，为增强云系统安全性提供了一种有价值的参考方案，具有重要的理论意义和应用价值。
针对暗硅片的多核处理器体系结构研究和优化平台搭建	多核;暗硅片;良品率;功耗墙	半导体工艺的深入发展可以将更多的晶体管集成在芯片上。但是现代多核处理器芯片受到功耗墙，良品率和程序并行度等因素的制约，难以继续靠增加核的数目来提高性能。这些问题都预示着未来多核处理器设计会遇到瓶颈：我们可以在芯片上集成更多的晶体管，但是却无法有效的利用它们。这些没有办法被充分利用的芯片面积被称之为"暗硅片"。本课题重点研究未来处理器发展中的暗硅片问题，讨论如何充分利用这些"冗余"的芯片面积，针对不同类型的应用设计新型处理器体系结构以达到性能和功耗的优化。.    研究内容为1)暗硅片的发展趋势建模2)适用于暗硅片的芯片部件选择和参数提取,包括集成电压调制器，可编程门阵列，特殊接口，各类特殊电路等3)不同类别处理器的性能功耗，适用领域及主要应用的研究和分析4)建立一个面向暗硅片体系结构设计的智能优化平台5)针对不同类型的处理器应用，在暗硅片中集成特定的芯片部件优化体系结构。
面向高性能云平台的并行程序优化关键技术研究	并行程序;高性能计算;云计算;性能优化	随着云计算的发展以及面向并行计算领域优化的高性能云平台的出现，越来越多的用户开始在高性能云平台上运行各种科学计算程序。但是，复杂的云平台计价模型、灵活的云资源配置模式、非定制的通信网络以及显著的系统噪音等因素给高性能云平台上运行大规模并行程序带来新的挑战。.针对上述问题，本项目研究工作包括：首先，提出面向高性能云平台的半弹性虚拟集群计算模型。通过聚合大量用户的作业请求，实现统一的云资源调度和管理，并根据作业规模动态调整虚拟集群大小，降低用户使用成本并提高作业运行效率。其次，提出基于学习排序的方法实现自动预测给定并行程序的最优云配置方案。针对云平台资源配置组合空间爆炸的问题，提出基于PB矩阵的统计方法对高维参数空间进行降维。最后，针对高性能云平台的特点，提出采用静态分析的技术实现并行程序的通信自动隐藏，以及基于性能断言的技术在线检测云平台上存在的系统噪音，提高并行程序的性能和可扩展性。
基于社会化机会物联的移动用户感知信息收集服务研究	感知信息收集;数据采样;隐私保护;信息转发;移动终端	随着移动终端感知、计算、存储和通信技术的发展，社会化移动应用和服务需要收集移动环境中用户、环境及社会相关的多种感知信息。本课题以提供多粒度、安全的移动用户感知信息收集服务为目标，研究利用用户携带的移动终端或者传感设备进行用户相关的感知信息收集的相关技术和理论，主要包括移动用户多源异构感知信息采集、基于社会化机会物联的感知信息传播、移动用户感知信息隐私保护，以及通过模拟或原型系统验证提出的研究成果。本课题拟采用基于强化学习的自适应感知数据采样机制、基于本体分层的感知信息上下文模型、基于社会关系属性的感知数据机会传输机制、以及基于个性化K-匿名模型的感知信息隐私保护机制等技术解决方案，预期在国内外重要刊物和会议上发表10篇以上学术论文，申请3项以上国家专利，力争在国际上相关研究领域产生一定的影响力。
基于数据共享的高并发图计算系统及核心技术研究	并发控制;大数据处理;图计算;并行计算;数据共享	图数据是大数据的重要数据类型之一，图处理技术是当前的研究热点，出现了以Pregel， Giraph，GraphLab，GraphX等为代表的图处理的系统。这些系统大多基于"面向任务"的处理模式：将图计算分解成一个个相互独立的任务来完成，每个任务中计算程序和数据紧密耦合。这样的模式在并发度不高的情况下，收到了预期的效果。但是，随着应用的不断扩展，要求并发处理的任务越来越多，数据和计算绑定的模式则遇到性能瓶颈。由于面向任务的模式，不支持共享数据，每个任务都需导入各自所需的数据，往往造成冗余数据占据内存，消耗巨大，并发执行的任务极其有限，严重阻碍了图处理系统性能的提升。 本申请提出一种新型的"面向数据"的图计算模式，以支持图数据共享为基础，目的是有效使用内存，支持高并发的任务执行，从而从整体上提高图计算的效率。本项目将对支持高并发图计算系统的图数据管理、流式计算模型、执行机制和技术展开详细研究。
基于BIP的嵌入式多核软硬件系统建模及其设计方法	性能评估;BIP;多核系统模型;形式化验证;嵌入式	多核并行化的迅速普及给嵌入式软硬件系统设计带来了巨大的机遇和挑战。当前相关工作的核心问题在于：1）软件和硬件建模分离；2）基于最差情况分析技术的性能评估准确率过低；3）基于仿真的功能验证可靠性较低。针对这些问题，本项目提出描述统一的、形式化的、可分析和可执行的嵌入式多核软硬件系统模型，如实反映软件应用运行于硬件平台时的总体特征。以统一软硬件的系统模型为基础，可通过组合验证方法保证系统模型设计的正确性，通过激励仿真和概率统计结合保证性能分析的准确性，并实现从系统模型设计到硬件平台中多线程软件代码生成的整个流程。本项目将以国产嵌入式处理器为平台，把建模方法学在实际应用中进行推广，为多核软硬件系统设计的研究和发展提供具有自主创新和指导意义的研究结论。
多核平台下的高效线程级猜测执行机制研究	多核;运行时系统;体系结构;线程级猜测执行;优化	如何简洁、高效地利用处理器中集成的丰富计算资源，是多核体系结构带来的一个重要挑战。为解决这一问题，必须将编程模型、运行时环境及体系结构有机地结合在一起。尽管线程级猜测执行（Thread Level Speculation，TLS）机制具备解决该问题的足够潜力，但由于缺乏体系结构和运行时环境支撑，TLS的自身开销又比较大，严重制约了其实际效果。本项目将从深入分析TLS机制自身的特点入手，研究并提出高效的TLS模型，以及多核体系结构和运行时环境对TLS的高效支撑机制，在此基础上总结TLS编程模型，设计编译框架，自动将串行C/C++代码或二进制代码转换为多线程代码，并获得有效的性能提升。本项目研究将在软硬协同的TLS模型、高效支持TLS的体系结构和运行时环境、面向TLS的编译技术和线程划分策略等方面取得创新研究成果。这些成果可以直接应用于多核乃至众核平台设计，具有重要的理论意义和实际应用价值。
分布式不确定skyline查询处理关键技术研究	大数据集;并行与分布式计算;skyline查询;不确定数据;数据管理	数据呈现出海量性和不确定性，大规模不确定数据的有效管理已成为当前信息学科研究最活跃的领域之一。不确定skyline查询是数据管理中的重要研究技术，在决策支持、数据挖掘和环境监控等方面的应用极为普遍。当前不确定skyline查询的相关研究成果大多面向集中式数据库，对于分布式数据库的研究还比较少。本课题中，我们将研究分布式不确定skyline查询处理关键技术。首先针对不同的数据类型及应用场景，分别提出不确定skyline查询模型和top k不确定skyline查询、不确定k支配skyline查询的新模型；其次，分别设计高效的空间和概率剪枝策略，降低问题的计算复杂度；随后，引入模糊数学、博弈论中的算法设计技术，分别设计以上问题的分布式算法。并根据通信成本及执行时间两大性能指标对算法进行分析。本课题的研究不仅能够为不确定skyline查询算法的设计提供新思路, 还将丰富传统分布式数据处理的研究内容。
在线社交网络中时间敏感信息的高效传播策略研究	用户影响力;实时信息传播;快速信息传播;在线社交网络	在线社交网络正在经历爆炸式的发展，它已成为一种重要的信息传播媒介，是扩散时间敏感、受众广泛信息的有力工具。由于在线社交网络结构复杂、规模庞大、高度异构且动态变化，并且用户呈现多样性，目前对于信息传播过程的理解还非常有限。已有研究通常关注信息的长期扩散影响，而忽略了信息的扩散速度。本课题研究在线社交网络中时间敏感信息的高效传播策略，建立包含时间动态的信息传播预测模型，刻画信息如何随着时间的推移而扩散，预测消息扩散概率和扩散时间；设计时间敏感信息的实时传播算法，给定截止时间，寻找最小的种子集合，使得在截止时间内，网络中至少给定比例的用户被该信息所影响；设计时间敏感信息的快速传播算法，在一定的社交距离约束下，寻找最小的种子集合发布消息，使得至少给定比例的用户被该息所影响。研究成果可直接应用于社交网络的信息传播，具有较强的理论和实际研究价值。
大数据驱动的复杂系统协同创新理论与方法研究	协同;工作流技术;大数据	大数据驱动的复杂系统协同创新，是数据科学在人类解决重大科学与工程问题中发挥核心驱动力的重要体现。随着大数据资源在复杂系统协同创新过程中发挥越来越重要的应用价值，一些新的应用与技术问题也越来越引起关注。首先是大数据的价值稀疏性特征，导致群智协同过程所需决策数据的获取，在时效性与一致性方面的问题越来越突出。而协同创新的动态演化，使得传统的数据挖掘算法，经常面临参数、条件和目标的变化。完全随机的一个意外情况，都有可能导致系统全局规则的巨大调整。针对上述应用挑战，本课题着眼于复杂系统在大数据环境下协同运行所面临的新的"生态环境"，按照问题驱动的研究思路，重点围绕决策依据挖掘、群智协同推进、创新可信保障等应用环节，有针对性地开展研究工作，并提出相应的理论与技术解决方案。在此基础上，结合典型的复杂系统协同应用，对课题的研究成果进行理论与方法验证。
基于虚拟化与体系结构支持的移动平台系统安全研究	虚拟化;安全硬件;体系结构扩展;移动平台	随着移动设备的广泛应用，其安全问题日益严重，其中挑战主要来自于移动平台的以下特点：首先，用户隐私数据集中，使移动设备极易成为攻击目标；其次，丰富的功能使软件系统日趋复杂，潜在漏洞也随之增加；再次，设备容易丢失或被窃，使攻击者可使用包括物理攻击在内的多种攻击手段。已有研究大多集中在应用层与操作系统层，主要检测已知的恶意软件以及防御已知的安全漏洞，且依赖于庞大的可信计算基。本项目拟从硬件层与虚拟化层自底向上地探索一种通用的、保护力度更强的安全机制。一方面，利用在服务器领域取得重大突破的虚拟化安全技术，研究对移动平台关键数据和代码的不同层次的安全监控、安全隔离、可控交互等技术；另一方面，结合移动端的安全硬件，在减小可信计算基的同时，为上层软件提供安全支撑，并进一步扩展体系结构以抵御物理攻击。研究以保证对现有移动应用的兼容性为前提，充分考虑移动平台计算的限制，在提高安全性的同时保证系统实用性。
3D堆叠众核处理器共享存储访问均衡性研究	片上网络;众核处理器;共享存储;访问均衡性	随着集成电路技术的快速发展，片上系统逐渐由基于总线的单核或少量多核结构发展到基于片上网络和分布式共享存储的大量多核（众核）结构，并采用3D堆叠技术，该结构称为3D堆叠众核处理器，是支撑片上高性能并行计算的有效途径。在3D堆叠众核处理器中，二级分布式共享存储的划分和组织方式、并行访存请求竞争、3D片上网络延时差异等使得共享存储访问的均衡性问题日趋严重，成为系统性能瓶颈。本项目拟针对3D堆叠众核处理器的共享存储访问均衡性问题展开深入研究，突破3D堆叠众核处理器访存关键技术，取得原创性研究成果。主要研究内容包括：1）面向访存均衡性的二级分布式共享存储划分技术；2）支持均衡响应的并行存储编组和并发访存调度技术；3）支持均衡响应的DRAM访存请求调度技术；4）基于可变优先级和总路由时长预测的均衡流控技术。最后，将建立具有共享存储访问均衡性的128核以上的3D堆叠众核处理器模拟平台和FPGA仿真环境。
低功耗光片上网络的设计与研究	交换机制;片上网络;路由算法;拓扑结构	片上网络从体系结构上解决了现有总线结构芯片设计所面临的可扩展性能等诸多问题，是下一代高性能芯片设计的发展方向。但是随着芯片工艺的进一步提高以及应用带宽的不断增加,传统的电片上网络面临时延长、电磁互扰严重、串扰大、损耗大、电子带宽瓶颈和功耗限制等诸多问题,而光片上网络是解决这一问题的有效途径。本课题在跟踪片上光器件的最新研究发展动态的基础上，通过对拓扑结构、路由器结构、路由算法及交换机制等关键技术的研究，构建基于簇的新型无阻塞拓扑结构，设计具有流量均衡和防死锁功能的严格无阻塞片上光路由器，引入损耗为优化目标兼顾时延和功耗设计新的自适应路由算法，改进现有交换机制并综合其优势设计混合交换机制。最后，搭建仿真平台，从吞吐、时延、功耗、损耗、面积等方面对所提方案进行分析，与现有结构进行对比后进一步优化完善设计方案。
能源互联网建模、分析与优化理论研究	云数据中心;信息能源基础设施;能源路由器;电力感知计算;能源互联网	杰里米o里夫金在《第三次工业革命》一书中指出，"第三次工业革命"的标志是互联网和可再生能源的结合而形成的新型的能源互联网。运用先进的信息和互联网的理念、方法和技术，与可再生能源相结合，构建能源互联网，成为未来信息能源基础设施一体化可持续发展的途径之一。能源互联网是以互联网理念构建的新型信息-能源融合"广域网"，它以大电网为"主干网"，以微网为"局域网"，通过能源路由器自下而上、开放对等地构建局域集中自治、广域分散协同相结合的信息能源一体化基础设施。本项目提出能源互联网基础体系架构，建立一套能源路由器多元系统建模方法，并以此为基础进行能源局域、广域和主干网联合仿真分析，进一步实现分散协同的能量路由资源调度算法和服务效益最大化系统动态优化策略，最终形成能源互联网半实物仿真系统和开发环境，为能源互联网关键技术研发与示范推广提供坚实的理论基础。
基于流模型的同时多粒度并行众核体系结构关键技术研究	多态化;流计算;众核体系结构;并行	随着密集计算类应用蓬勃兴起和VLSI技术不断发展，流计算模型研究获得了巨大的应用驱动和基础技术支撑。以流计算模型为基础，构建新型体系结构，适合于更大规模集成电路，成为学术前沿关注的焦点。本项目研究以面向未来超十亿只晶体管时代1K ALU以上规模的众核微处理器为目标，以支持同时多粒度并行为特点，开展适用于流计算的新型体系结构研究。其主要思想是将并行性开发与硬件多态化相结合,使得流计算可以在一定的体系结构框架下，针对不同的应用需求实现自己独特的并行执行模式和并行数据粒度。本研究首次提出Tile化众核流体系结构、同时多粒度并行调度和节点多态化等关键技术的研究方法，具有原创意义，对我国未来高性能图像信号处理、音视频编解码、数字通信、密码和科学计算等领域中的核心处理芯片研发将会产生积极影响。
分簇VLIW处理器的模调度及低功耗编译优化技术研究	低功耗;分簇VLIW处理器;模调度;指令调度;寄存器分配	VLIW(Very Long Instruction Word)体系结构在嵌入式处理器中得到了广泛的引用。分簇是改进VLIW处理器可扩展性及能量消耗的一种有效技术。通过编译技术优化应用程序的性能，以及在不影响程序运行性能的条件下最小化系统或处理器的运行功耗，是目前编译优化技术的研究热点。分簇VLIW体系结构带来的簇间指令分配问题，对编译器中的指令调度、寄存器分配、软件流水等问题提出了更大的挑战。本项目将通过指令调度、寄存器分配、软件流水等编译技术有效地优化程序在分簇VLIW处理器的运行时间和功耗。针对串行程序提出有效地降低分簇VLIW DSP处理器功能部件功耗的指令调度和寄存器分配算法。针对循环分别提出有效优化分簇VLIW DSP处理器性能以及功耗的指令调度和寄存器分配算法。将算法实现和应用在Trimaran编译器中。此外，提出准确的性能模型和功耗模型来评估程序的运行时间和能量消耗。
面向逆时偏移算法的FPGA加速技术研究	并行处理;逆时偏移;FPGA;计算加速	近年来，石油勘探和开发开始转向深海等更复杂的地质区域。地震数据规模越来越大，处理算法也开始转向逆时偏移等更准确但同时更复杂的算法。相对于传统的Kirchhoff偏移算法，逆时偏移算法的计算量要高出一百倍以上，对现有计算平台的处理能力提出了巨大的挑战。相对于CPU和GPU等结构固定的处理器，基于FPGA的计算加速平台，可针对应用定制硬件结构，具有更广阔的性能优化空间。本项目将以硬件可编程的FPGA加速平台为基础，针对逆时偏移算法，实现软硬件的协同设计：（1）在软件算法上，针对FPGA的架构特点，研究和探索不同格式的有限差分算子；（2）在硬件架构上，研究和设计高并行度的数据处理流水线和高效缓存,以解除内存带宽对计算性能的限制；（3）研究计算精度优化技术，来进一步提升在固定带宽和固定硬件资源下可获得的计算性能。本项目将为FPGA在科学计算其他领域开展应用提供良好的参考和借鉴。
基于共享关系Cache的片上多处理器系统结构和并行程序优化研究	片上多处理器;Cache一致性协议;网络消息量;共享关系Cache	片上多处理器将多个处理器内核集成在一块芯片上，可充分利用应用的线程级并行性提升性能，已成为处理器系统结构研究的热点之一。本项目基于对多个并行程序的特征分析，发现绝大多数并行程序的多个线程间存在对共享数据访问的时间局部性。基于上述特征，一方面本项目提出了基于共享关系Cache的一致性协议设计思想，通过共享关系Cache缓存最近出现的共享关系，为后续对该共享数据的访问请求提供目的结点集合信息，减少采用广播机制的多处理器系统的网络消息量。初步模拟实验表明，基于Token协议的16核片上多处理器在集成共享关系Cache后，网络消息量降低约15％。另一方面本项目提出了以提高并行程序对共享数据访问的时间局部性为目标的并行程序优化思想，优化后程序将能够更好地利用共享关系Cache的有限存储空间。本项目的研究面向多处理器系统结构设计和并行程序优化，将两者统一考虑，有助于全面提升片上多处理系统的整体性能。
虚拟计算系统的能耗管理方法研究	在线迁移;云计算;虚拟计算系统;能耗管理;绿色计算	云数据中心中资源的高度集中带来了高能耗问题。现有大多数节能方法都集中在粗粒度的数据中心管理层次，即通过关闭空闲物理机来实现节能，而缺少在虚拟机层次进行细粒度节能管理。如何在虚拟计算系统层次实现有效的能耗管理是一个迫切需要解决的问题，也是实现绿色云计算的重要保证。本项目以虚拟计算系统中能耗测量、精准建模、节能优化、节能评价为目标，围绕计算资源的虚拟化给能耗管理带来的问题：1）外围设备无法直接测量虚拟机能耗,现有测量方法准确度差、开销大；2)虚拟机能耗建模精度低、缺乏异构平台的一致性建模方法；3）传统虚拟资源管理忽视能耗因素；4）虚拟机能耗评价缺乏系统化及标准化等问题开展研究。力求在轻量级能耗间接测量方法，基于相关性分析的能耗一致性动态建模方法，能耗感知的虚拟机部署、服务器整合和在线迁移策略，以及能耗评价指标、方法和框架方面做出创新性研究成果，为云数据计算中心的节能减排提供技术支持。
基于DNA自组装的通用纳米信息处理系统的研究	纳米信息处理系统;功能基元;可重用;DNA自组装	基于DNA纳米技术的各种超分子体（功能单元），已能实现简单计算、移动、靶向送药等功能，其纳米结构的控制精度达到了原子级。目前已有的功能单元信息编码规则、交互协议和数据输入输出方式各不相同，极大限制了用标准信息处理技术来控制生物系统和构建复杂纳米机器人的发展。本项目基于DNA自组装和折纸术，研究功能单元封装技术，构成功能基元，实现基元交互，构建可重用基元的通用纳米信息处理系统。研究内容包括信息规范化编码设计，实现功能基元输入输出信息表示的一致性；实现功能单元封装，构成功能基元，建立基元库，包括识别、输入、逻辑运算、信号放大、输出和显示功能基元库；设计基元间规范化通信接口，实现基元标准交互协议；利用基元组合配置DNA传感器，验证组合系统的有效性、完整性和容错性。功能基元可灵活配置构成新系统，为设计实现可编程、可寻址的纳米机器人提供新方法和新技术，为建立标准信息处理系统提供理论和技术支撑。
时空-服务域多无人车自主行为协同模型与关键机制研究	智能交通;自主协同;无人驾驶车辆;信息物理社会融合;时空-服务域	基于综合感知、移动通信、智能控制等技术的智能交通系统，是未来智能城市与社会的重要组成与基础。随着该类系统日益呈现出信息物理社会融合的发展新趋势，面向服务的多无人车自主行为协同已开始成为该领域一个新兴的基础研究问题。针对时空-服务域的多无人车群智能协作特征，本项目重点研究基于车载通信的无人车协同模型以及面向多元服务的多无人车行为协同机制等主要内容，致力于解决时空域冲突预测模型、状态驱动的无人车行为协同控制、面向服务质量的多车协同机制与策略等关键问题，并形成针对该类问题研究的理论和方法体系。基于模型驱动的思想，本项目将对所研究模型、方法的正确性及算法、机制的效能进行分析和评价，进而构建原型系统对关键方法进行半实物仿真验证。本项目所研究方法和机制是新型智能交通系统研究的基础，亦可为多无人航行器、智能仓储与物流等领域的新型群智能系统研究提供支撑。
跨域可定向的非对称群组密钥协商研究	域间盲签名;移动云计算;非对称群组密钥协商;信息交换;联盟认证	移动云网络具有资源分布多域性、信息密级多等级化、终端资源受限等特点，该网络环境下群组密钥协商需解决成员分布跨域性、信息交换可定向性、计算不对等性等问题。当前研究方案不能满足该环境下复杂信息交换需求。本项目拟设计可定向的跨域非对称群组密钥协商协议，解决该环境下这些复杂信息交换问题。首先，拟提出域间结构化认证及隐藏签名算法，结合非交互式零知识证明理论，设计域间结构化联盟认证协议，为域间群组密钥协商提供模块化安全认证及终端隐私保护；其次，拟提出密钥因子存储及组合计算的迁移技术、批量认证及群公/私密钥对映射算法，设计域间无证书联盟认证的非对称群组密钥协商协议，解决群组信息交换的跨域性、可认证性及匿名性；在此基础上，拟提出群组贡献密钥因子提取、分发算法及组合技术，设计域间可定向的群组密钥协商协议，解决多层次信息安全交换。项目将为移动云中群组间跨平台资源共享、多方信息交换提供一定的理论基础和技术支持。
云数据中心并行计算模型与作业调度研究	云计算;资源分配;作业调度;并行计算模型	云计算平台的作业执行子系统主要负责计算模型、资源分配与作业调度。目前，云计算环境下的作业执行正面临着新的机遇与挑战，例如，单集群节点通常采用了多处理器混合架构；多个计算模型(MapReduce、DAG、Stream)和多种作业类型(系统作业、用户交互式作业、实时作业等)并存。因此，如何根据计算模型，调度不同类型作业到异构复杂的集群计算资源上执行成为云计算系统平台面临的关键科学问题。为了提高作业调度与执行效率，本项目将综合分析计算资源、计算模型、作业三者特点，从以下三个层面开展研究：(1)面向多处理器集群系统的作业调度；(2)面向异构集群计算资源的作业调度研究；(3)面向流数据处理的计算模型与作业执行优化。本项目的研究成果将可以直接应用于真实的云计算系统平台，支持多个计算模型和作业类型，能够提高作业执行子系统的吞吐率、资源利用率、公平性，减少单个作业完成时间。
高可靠嵌入式系统电源功率模块劣化过程的非线性时变行为研究	剩余使用寿命;嵌入式系统;级数;功率模块;Volterra;劣化	高可靠嵌入式系统广泛应用于航天、核电、高铁等关系国家战略与安全的领域中，其电源的可靠性直接影响系统的安全性，并成为电子信息和能源学科的研究热点。在影响电源可靠性的关键因素中，功率模块劣化占据主导地位，如何在运行中动态获取功率模块的劣化状态、预测剩余使用寿命已成为研究的焦点。因此，本研究拟建立功率模块劣化过程的非线性时变行为模型，基于时变状态方程解不稳定性的泛函理论探讨劣化动态演变过程，改传统劣化分析的结构性、统计性建模方式为行为性、解析性建模方式；提出基于Volterra级数的非侵入式功率模块劣化状态在线获取方法，以驱动信号为非侵入式激励，用Volterra级数频域核抽取劣化特征，变劣化状态在线不可测为可测；在此基础上，提出功率模块剩余使用寿命的动态预测算法，改传统离线统计分析为在线预测。为高可靠嵌入式系统电源健康状态预测与智能管理奠定理论基础，为提高其可靠性、可维护性提供新思路和新方法。
针对GPU的高效并行任务执行设计研究	计算图形处理器;并行任务执行;性能;缓存竞争;调度器	现代的图形处理器（GPU）已成为广泛使用的高性能计算平台。GPU对多种不同的应用都有着巨大的计算潜力。随着越来越多的通用应用程序被移植到GPU上执行，这也加剧了GPU资源的竞争，因此GPU资源的有效利用是提高系统性能的关键。这也为我们提供了并行任务执行的机会 - - - 多个任务并行执行共享GPU资源。 为了支持高效的并行任务执行, 我们打算提出一个系统的软硬件解决方案。我们首先寻求构建不同的硬件资源共享机制以支持不同的设计权衡。然后，我们提出了一个全新的运行时软件架构。该软件架构允许并行任务执行, 并且会基于任务的不同需要分配GPU资源。我们的软件架构具有以下特点 1）精确的性能分析模型 2）高效的任务调度算法 3）缓存感知的设计和优化。这个项目所提出的技术是对现有的GPU结构的重要补充,并对未来的GPU设计有着重要意义。所提出的并行任务执行技术可以显著的提高系统的整体性能和能源利用效率。
基于片内超速时延测试的小时延缺陷检测方法研究	测试向量;时延故障;小时延缺陷;超速时延测试;可测性设计	在深亚微米以及纳米工艺尺寸下，为了确保芯片的品质和可靠性，小时延缺陷的检测已经至关重要。本项目拟开展片内超速时延测试方法的研究，通过在芯片片内提供频率精准的超速时延测试时钟，并以此降低被测通路的时隙值，从而为小时延缺陷的检测提供关键技术和实现方案，主要研究内容包括：(1) 研究基于频率编程和环形振荡校正的超速时延测试时钟生成方法，从而在工艺偏差的影响下，依然能够为芯片提供频率精准的超速时延测试时钟; (2) 研究基于短通路可测优先的超速时延测试向量选择方法，并以此降低超速时延测试时工艺偏差导致的小时延缺陷漏测概率；(3) 研究基于通路时延分析的超速时延测试向量分组和压缩方法，并以此降低超速时延测试的应用时间和测试向量规模。通过在芯片片内设计超速时延测试时钟生成结构以及相应的测试向量选择、分组和压缩方法，可以对芯片中的小时延缺陷进行有效检测，从而对于提升芯片的品质和可靠性具有非常重要的意义。
基于风险熵模型的社会安防系统网络效能评估技术研究	安防系统;风险熵;效能评估;防护能力	随着工业化、城镇化的快速发展，国家安全面临的形势更为严峻复杂。在社会治安防控领域，各类安防设施虽然发挥了积极作用，但存在系统设计不当和人防、物防、技防脱节等缺陷。安防系统效能评估侧重于管理和经验层面，缺乏科学定量评估的理论方法和技术手段，难以支撑完整、高效的综合防控体系的形成和建立。因此，研究安防设施防护能力和风险预测技术，建立系统防护效能与风险的客观评价体系，是当前国家安全防范领域发展的重大战略任务。本课题根据香农信息论，提出风险熵的概念，建立安防节点风险熵模型，研究度量节点防护效能的表示和计算方法。基于安防节点和交通路网连接形成的广义安防系统，抽象建立仿真安防网络模型，研究路径依赖的安防网络效能计算方法，根据安防网络的效能完成系统的风险与效能评估。本项成果可突破安防系统防护能力和风险客观评测的技术瓶颈，对建立科学预测、有效防控与高效应急的公共安全体系具有重要的科学意义和应用推广前景。
低功耗异构片上网络关键问题研究	多片上网络;路由协议;拥塞控制;电源门控;众核体系结构	为了满足日益增长的计算性能需求，通过集成更多内核形成众核芯片处理器成为必然选择。目前众核芯片在云计算、移动设备、高性能计算等领域得到广泛应用。然而众核芯片规模和计算性能的增长导致芯片功耗快速增长，成为众核芯片发展的主要制约。本项目主要研究低功耗的高性能片上网络的一系列系统架构级的关键问题，研究课题包括研究低功耗异构多片上网络的整体设计；设计多片上网络各个子网络的低功耗路由协议和路由器；制定相关网络拥塞的的检测策略和拥塞状态信息的快速传播和收集方法；建立低功耗多片上网络的网络模型、功率模型，和电源门控节能效果和网络性能综合评估和选择机制，探讨应用不同类型的异构多片上网络以构建低功耗、高性能众核通讯机制的新理论和新方法。
基于非易失性内存的新型体系结构的系统优化关键技术	损耗均衡;数据分布;写感知调度;非易失性内存;非易失性内存文件系统	新一代非易失性存储器，如相变存储器、忆阻器等，在数据读写时间及功耗等方面具有优越的性能。把非易失性存储纳入内存空间将成为解决传统计算机系统数据I/O瓶颈的有效的解决方案。同时，根据传统的计算机存储层次构建的计算机系统及优化技术将因此而发生革命性的变化。 非易失性存储器的共同特性是：1.非对称读写速度和能耗。2.有限的擦写耐受性能。本项目对新型非易失性存储器应用于计算机系统内存的设计与优化的根本性问题做基础研究。从系统优化的角度，对具有非易失性内存的单核与多核系统中的写感知调度、数据分布优化、数据迁移、损耗均衡优化，以及新型非易失性内存文件系统的设计和能效优化等问题进行深入的研究，分析问题的本质及复杂度，研究最优解算法或高效的近似最优解算法，提出综合的解决方案。研究成果将为非易失性内存在计算机系统中的应用提供具有普遍意义的优化算法和解决方案。
面向计算密集型移动应用的朵云高效服务提供及无缝切换技术研究	机会卸载;切换增益函数;无缝服务切换;应用弹性划分;朵云	随着基于移动设备的计算密集型复杂应用不断增长，研究如何在移动云中高效地为移动端用户提供优质服务具有重要的理论和应用价值。本项目在一种移动云计算模式——朵云环境下，针对移动端资源贫乏、移动应用类型多样及朵云服务范围有限等特点，研究面向计算密集型移动应用的朵云高效服务提供及无缝切换技术，以提升朵云服务质量、移动用户满意度和系统性能。确定弹性划分粒度、任务耦合度和均衡度，研究基于图聚类的上下文感知朵云应用弹性划分方法，提高应用划分精准度。建立机会卸载模型，定义卸载成本函数，研究基于广义Nash均衡的多朵云用户机会卸载方法，降低卸载成本及终端能耗。提出基于影子页表和副本限制的朵云实时任务双容错调度策略，提高朵云系统的可靠性。建议基于切换增益函数和TOPSIS的水平/垂直朵云服务切换方法，保证服务提供的连续性。构建基于校园移动网络的朵云实验环境，对所提出的方法进行测试，并提供校际朵云示范应用环境。
分布式闪存文件系统的高效构建技术研究	非易失内存级存储系统;闪存;闪存文件系统	分布式文件系统为当前高性能计算、大数据分析和云计算平台提供者基础的存储支撑。近年来，闪存的快速发展为构建高速的分布式文件系统带来了新的机遇。然而，传统的分布式文件系统厚重的软件管理难以有效发挥闪存的性能优势，且对闪存寿命考虑较少，难以匹配闪存高速的硬件性能。针对该问题，本课题拟研究基于闪存的分布式文件系统的高效构建技术，具体包括：（1）结合闪存特性优化缓存数据组织与管理方法，提升缓存效率与闪存寿命；（2）键值友好的松耦合元数据数据组织与分布方法，提高元数据的访问性能与扩展性；（3）引入内存级闪存等持久性内存，研究基于持久内存的元数据管理方法，加速元数据访问；（4）基于RDMA的访问模式重建高效的分布式存储协议，提高交互效率。本课题的研究将为闪存分布式文件系统的高效运行提供理论和技术探索，为提升高性能计算、大数据分析和云计算平台的数据存储能力提供技术支持。
大规模数据处理中的高可靠性GPU集群关键技术研究	大规模数据;通信机制;GPU集群;数据分配;容错	随着大规模数据处理对处理器计算能力与存储带宽的要求越来越高，将GPU集群应用于大规模数据处理成为此领域的研究热点。申请此项目旨在研究如何保证大规模数据处理在GPU集群中的可靠性和高效性。为保证大规模数据在GPU集群发生错误时能继续处理，研究结合虚拟机与协同式检查点/回滚技术的低消耗容错技术，主要包括如何确立GPU建立检查点的时间，如何确立建立检查点的时间间隔，以及如何保存各结点的检查点信息等。为降低GPU集群中大规模数据处理所引起的传输开销，研究合理的数据布局方案，主要包括如何确保各结点的负载均衡，如何减少数据间的依赖性，以及如何在GPU多层次存储结构上优化分配数据等。为有效管理大规模数据处理中GPU集群各结点之间以及结点内部的消息传递，研究如何将MPI和CUDA紧密结合，设计统一的通信语义环境。此项目的研究能为大规模数据处理相关应用提供高效性和可靠性兼顾的的运行平台。
视频应用中运动估计算法的并行与访存优化研究	并行化;运动估计;访存优化;数据重用;自动调优	人们对高清视频的需求日益增长，提高视频处理速度成为视频应用发展的关键问题。运动估计是视频应用的核心算法，但其时间耗费大，成为性能瓶颈。本项目拟从帧间与帧内数据重用、并行与访存协同优化、自动调优等方面开展运动估计的性能优化研究：（1）针对片外访存需求过大的问题，研究更高效的数据重用方法，通过帧间数据重用减少全搜索运动估计的片外访存，通过帧内数据重用减少快速搜索运动估计的非规则访存开销。（2）针对多计算单元对共享存储空间和片外访存带宽的竞争问题，研究运动估计算法并行与访存的协同优化，通过计算单元之间的片上数据共享减少对片上存储的竞争，通过提升并行度与访存带宽之间的匹配度有效利用访存带宽。（3）针对手动调优时间耗费大的问题，研究运动估计算法的自动调优，通过自动选择算法或硬件参数，达到最优性能。在GPU和FPGA平台上实现上述优化方法，将大幅提升运动估计算法的性能，满足高清、实时的视频处理需求。
基于认知学习的动态信任关系的度测机理研究	信任推理;认知学习;上下文察觉;信任度量;分布式系统	在网格、P2P和普适计算等各种开放式网络应用环境中，涉及数目巨大的、处在不同安全域的计算资源与软件实体，大量资源的接入，实体行为的动态变化，使得网络的各个实体之间难以建立动态的信任关系。研究和探索动态信任关系的度量与预测（简称度测）机理已成为动态信任管理技术的基础性工作，并成为急需解决的科学问题。本项目在感知网络实体行为和剖析行为间关系的基础上，研究信任关系的多维属性和心理认知特征，探索基于认知学习的动态信任关系度测机理，克服传统方法对环境特征和应用行为适应不足的问题。具体引入本体论、粗糙集、粒计算和信息熵等认知学习领域的前沿理论与方法，建立动态信任关系的形式化概念模型，研究行为感知与分析技术，提出信任度量与分配方法，改进基于机器学习的信任关系推理与决策算法，实现基于决策的信任预测与反馈。本项目的研究成果对于可信网络、可信系统和可信软件的研究具有良好的指导和借鉴作用。
基于协作的云存储外延服务优化机制研究	云存储;质量优化;协作机制;存储的可用性;高效传输	云存储作为未来信息存储的重要服务模式，吸引了大批研究人员来关注云存储的发展。很多研究集中在和数据中心相关的问题上，却忽略了用户到数据中心之间的互联网空间上存在的问题对云存储服务质量产生的影响。例如：由于互联网在复杂环境下性能的不可预测性，导致海量数据传输时间代价巨大，没有可靠性保证；数据密集型访问会造成可用性和效率的显著下降；此外，随着信息的不断增多，对存储容量的要求越来越大，不仅云存储服务基础设置的硬件成本越来越高，用户的存储费用也随之增加。已有的对相关问题的研究往往针对一个个孤立的问题，而本申请则把这类问题和云存储服务质量结合起来，在整体的构架下探讨云存储外延服务质量问题。本申请将根据云存储服务本身拥有的广大用户群的特点，重点研究有效的用户协作机制来优化云存储外延服务的质量，在协作资源调度、协作副本维护、协作数据消重、协作数据传输等机制的保障下，高效、发便、经济使用云存储服务。
云计算数据中心应用感知的动态资源配置技术研究	应用感知;云计算;数据中心;资源配置	应用类型决定了其对底层资源的耗用特征，而复杂应用运行中功能模式的切换对系统资源的需求则会发生本质变化，如果云计算数据中心仍采用单一资源配置方式为应用提供资源，那很难实现"按需服务"。本课题从提高云计算数据中心资源利用率为基本出发点，面向复杂应用建立高效的资源配置机制为核心，拟从分析和刻画应用的各功能模式在虚拟化平台上运行时对资源的细粒度需求特征为切入点，构建由基本资源拓扑与不同模式下增量资源需求构成的资源概率需求拓扑。由此拟基于多个应用间增量资源需求的关联关系，研究共享增量资源的应用社区构造及资源需求复合算法，并实现以计算集群为单位的资源规划配置机制。针对集群内的虚拟机部署，基于不同类型应用的性能干扰关系，研究虚拟机均衡资源部署机制。针对运行时应用的演变与整体性能的优化，研究效益最大化的动态资源调整策略，保持系统高效、稳定。上述成果将运行于课题组前期自主研发的云平台原型，验证其正确性。
多维度智能群体网络的感知与协同优化研究	多智能体系统;多维度;协同;感知;优化	网络环境下不同类型智能体的协同机制在国民经济和社会发展中越来越发挥着不可替代的重大作用。这类由异质、异智的智能体在网络环境下通过彼此之间的信息交互构成的涵盖海、陆、空多平台的多维立体异构网络，称为多维度智能群体网络。不同维度的智能群体如何优势互补、协调优化，迫切需要建立相应的理论与方法，以指导实际多维度智能群体系统的设计、开发与实现。项目拟从空地机器人协同编队、海陆空多机种联合攻击等几类典型多维度智能群体系统的实际问题出发，分析其节点特性和网络拓扑结构特征，建立能够反映多维度智能群体节点和网络自身结构特点和动态特征的时变网络模型；结合几类典型实际系统中的协同感知、多目标编队等具体的多协同问题，研究多维度智能群体网络的协同演化与优化问题，建立多维度智能群体网络的协同演化动力学与优化算法的有关理论和方法，并对所得结果进行数值仿真、软件模拟和实物验证。
基于冗余编码的云存储技术研究	云存储;冗余编码;分布式存储	基于副本和基于冗余编码的存储是保证云存储系统中数据可靠性的两项基本技术。随着数据存储量的爆炸式增长，基于副本的云存储技术已经难以满足云计算服务的需求；而基于冗余编码的存储技术不仅具有数据可靠，而且具有存储量相对较小的优势，被认为是下一代云存储系统的关键技术。然而，将现有的基于冗余编码的存储技术应用于云计算系统中，存在数据写入慢、数据分配难、数据维护慢和解码计算复杂度高等诸多难题。为此，面向上述挑战性问题，我们将基于编码的云存储技术研究,包括云系统中数据的写入、分配、维护、读取等开展研究，旨在突破其中的一系列关键性技术，提出新的理论模型和技术方法，达到冗余数据的高效写入、数据分配的最小化冗余、失效数据的快速修复、原始数据的快速解码重构的目的。为测试和验证上述研究成果的有效性，本项目将在实际的云计算实验系统中进行实验验证，为实现低成本、高可用、高效能的云存储系统提供新的技术途径。
逻辑错误屏蔽的近似电路逻辑综合多目标优化方法研究	Pareto解集;容错设计;多目标优化;逻辑综合;近似电路	集成电路工艺向纳米级发展，芯片尺寸压缩、工作电压下降，逻辑错误率不断提高，逻辑错误屏蔽已成为电路设计的挑战性问题。近似电路以小面积、低功耗代价获得较高错误屏蔽性能，在芯片有限内部资源和低能耗需求下，对近似电路逻辑综合优化研究具有重要实际意义。近似电路逻辑综合优化是对错误覆盖率、面积和功耗三个互冲突目标的同时优化。本课题研究一种近似电路综合多目标优化方法。首先确立多级逻辑函数节点的0/1相位选择机制和多路径节点处理机制，保证全局函数单相近似，满足错误屏蔽充要条件；建立面积、功耗与错误覆盖率的近似电路综合多目标优化模型，引入遗传算法和小生境机制，设计均匀进化算子，保持群体多样性，提高非劣解沿Pareto占优面的均匀分布度；确立优先级偏好的近似电路非支配排序策略，提高全局收敛性和计算效率，获得容错性价比高的近似电路解集。通过对基准电路测试，验证近似电路逻辑错误屏蔽有效性和多目标遗传算法优越性。
同指令集异构多核系统共享资源管理策略研究	内存调度;同指令集异构多核;资源划分;代价模型;能效比	同指令集异构多核（也叫性能异构多核）系统因核性能的异构性所具备的潜在高能效比，使其在服务器和移动设备上拥有光明的前景，但共享资源包括最外层cache、内存、带宽等的管理策略是限制其能效比的关键因素之一。当前共享资源管理策略往往基于性能同构性或者功能异构性的单一特征，难以适用于性能异构多核系统，从而无法发挥系统的高能效比。本项目拟突破现有策略的单一性特征，为性能异构的多核平台制定高效的共享资源管理策略，最大程度提升同指令集异构多核系统的能效比。主要研究内容包括：(1)针对现有共享资源管理策略无法确定性能异构多核系统中各核对共享资源的需求问题，研究构建一个统一的代价模型；(2)针对共享资源的使用优先级问题，研究代价感知的内存调度策略；(3)针对核间的干扰问题，研究抗干扰的共享资源划分策略。本项目研究成果可很好地挖掘同指令集异构多核系统的高能效比特征，从而对于同指令集异构多核系统优化有重要意义。
高分辨率全堆粒子输运模拟的SN并行算法研究	并行算法;中子输运计算;高性能计算;离散纵标方法;迭代加速算法	粒子输运数值模拟研究当前正朝着计算模型越来越精确、分辨率越来越高等趋势发展，高性能计算已成为其中必不可少的支撑手段。本项目面向基于SN（离散纵标）方法的高分辨率全堆粒子输运模拟，针对已有迭代加速方法和并行算法研究在数值稳定性、计算效率和并行可扩展性方面的瓶颈，以数万核并行扫描算法与迭代加速算法的前期研究成果为基础，从新型迭代求解与加速方法、并行算法与高效实现以及软件模块研制和应用验证方面开展研究。研究将重点突破基于物理的预条件子方法、"能群-空间-方向"多级并行算法以及针对多级嵌套体系结构性能优化两项关键科学问题，实现具有良好数值稳定性和计算效率的、适应于数十万处理器核的可扩展并行计算。研究成果可为高分辨率反应堆全堆输运模拟等问题提供必要支撑，对提升核电经济性与安全性、新堆型研发以及武器物理研究，具有重要意义和工程应用价值。
面向科学计算的异构多流体系结构关键技术研究	可扩展;科学计算;编程模型;带宽平衡;流体系结构	高性能计算的整体目标正从过去单纯追求高峰值速度向高效能转变。流体系结构能有效处理具有计算密集特征的应用，是当前国际高效能体系结构的研究热点。我们设计了国际上第一款面向科学计算的64位流处理器，选用多种类型的科学计算算法对其进行测试，结果表明：对数据密集型计算，该处理器的效能超过了1.6GHz Itanium2处理器。研究成果发表在计算机体系结构国际顶级会议ISCA'07上。进一步研究发现，要适应广泛类型的科学计算，如存储密集型计算和稀疏计算，需要在流体系结构、编程模式、访存和多芯片扩展等多个方面进行进一步创新。 本课题在已有研究的基础上，提出一种融合超标量和SIMD于一体异构多流新型体系结构。该结构解决了传统流编程困难、针对存储密集型和稀疏计算低效问题采用硬件支持多种计算特征的访存模式与带宽平衡、针对大规模计算采用流模式与消息传递模式相结合的并行多流扩展等，使之广泛高效地适应科学计算应用。
移动P2P网络数据分发机制研究	网络编码;路由;移动P2P网络;匿名认证;节点波动	移动P2P网络的数据分发是一个新兴的研究领域，开放性的体系结构使移动P2P网络数据分发广泛应用于流媒体数据分发、协同工作和数据共享等领域。移动P2P网络与互联网的P2P网络有着本质区别，因此现有的数据传输机制不能直接应用于移动P2P网络。本课题的研究目的是提出适用于移动P2P网络的数据传输机制，并提高数据传输的效率和安全性，研究的内容包括：（1）利用非确定性的网络构造方法，解决移动P2P网络在大量节点波动情况下的拓扑构造问题，并设计可靠路由算法；（2）利用小世界理论的短链特性和令牌机制的半被动特点，解决移动P2P网络的搜索效率低下和维护开销巨大的问题；（3）提出基于网络编码的数据传输方案，提高数据吞吐量和节点负载均衡能力，节约传输带宽；（4）利用环签名具有的匿名性和自发性，解决移动P2P网络数据分发过程中的节点匿名认证和内容认证的问题。
众核体系结构中的渗透式延迟容忍方法研究	延迟容忍;渗透;片上网络;众核处理器	随着众核体系结构的蓬勃发展，以及存储墙问题的日益严峻，数据传输延迟已经成为众核体系结构性能提升的主要瓶颈。渗透式延迟容忍方法可以通过更精确地获取线程上下文来有效控制访存延迟，本项目从如下三个维度研究众核体系结构中的渗透式延迟容忍方法：1)层次化渗透：针对存储层次增加而导致的数据传输延迟增大的问题，研究流水式的层次化渗透式延迟容忍方法，以实现众核结构的性能可扩展性。2)规则化渗透：针对非规则数据组织和处理低效的问题，研究非规则数据在渗透过程中的规则化重组方法，以形成渗透过程中的即时空间局部性。3)正交化渗透：针对片上网络中渗透数据与非渗透数据传输冲突的问题，研究正交化渗透方法，以提高片上网络数据传输的稳定性。本项目通过从层次化、规则化和正交化三个维度研究众核体系结构中的渗透式延迟容忍方法，为数据传输延迟容忍提供系统而有效的解决方案。
面向芯片级的多核处理器故障恢复方法研究	多核处理器;计算机系统结构;可重构计算;片上网络	多核处理器芯片所面临的瞬时故障和局部永久故障，对高可靠多核处理器芯片的设计提出了更高的要求。软件层或系统层的故障恢复无法同时保证故障恢复的透明性、确定性、高可用性。基于硬件方式故障恢复有更多的优越性，未来多核处理器芯片将具有更高的集成度和可扩展性，这为实现芯片级的故障恢复提供了可能。本课题拟从芯片级故障恢复出发，为多核处理器提供高可靠的故障恢复方法和模型。研究基于芯片级硬件检查点机制的多核处理器卷回恢复方法，实现瞬时故障恢复的透明性、通用性和高可用性；在此基础上，提出一种新的分离式日志记录机制，保证瞬时故障恢复的确定性；研究区域约束下的硬件演化机制，实现低代价、细粒度的多核处理器局部永久故障恢复；通过分析多种故障下多核处理器的执行模式，研究多模式故障恢复的多核处理器芯片模型，保证多核处理器对故障恢复的自适应性。本项目的研究将为未来高可靠多核处理器芯片的设计提供重要理论基础和技术支撑。
数据驱动的高性能计算机上并行程序性能预测研究	数据驱动;高性能计算机;性能预测;并行程序;性能建模	对高性能计算机上并行程序的性能进行分析建模，并在建模基础上进行执行时间预测具有重要的研究意义和应用价值，长期受到研究者的关注。并行应用程序的性能受到相当多的因素影响，通常难以达到较为准确的预测。近年来，数据挖掘技术得到快速发展，为发掘数据中潜在的规律提供了有效的方法和工具。并行应用程序在运行过程中可以产生大量可以反映程序的行为特征的数据，包括硬件资源使用情况的数据，以及通过插入特征检测代码所获得的程序运行逻辑数据等。本项目开展利用数据挖掘技术进行程序性能建模和执行时间预测的研究工作，对并行程序运行时的行为特征数据进行自动化收集、分析与筛选，对程序性能进行建模。本项目的目标是建立起系统的数据驱动的并行程序行为特征建模的方法，利用并行程序插桩分析与建模工具，对高性能计算机上并行应用程序的执行时间做出较为准确的预测，并将预测应用于任务调度、程序参数优化选择等实际问题。
基于信息物理融合的电网数据完整性攻击检测方法	信息物理融合系统;电网数据一致性攻击;物联网安全;智能电网;攻击检测	电网数据完整性攻击（DIA）是一种基于信息物理融合的物联网攻击方法，其针对电力系统的物理安全防护体系和运行机制设计攻击方案，利用信息网络攻击技术实现攻击目标。本项目从攻击者角度，将DIA分为攻击资源受限和攻击资源完备两类，将DIA策略构建转化为在信息网络和电力系统安全约束下的最佳攻击策略搜索问题，并建立两类DIA模型；从信息物理融合的角度，提出基于异常流量检测的电力系统状态估计方法和电网移动目标防御方法，检测两类DIA；设计智能电网半实物仿真平台，验证研究成果。.针对DIA检测的两个难题：信息物理系统综合安全建模和异构数据关联融合，本项目提出：基于攻击者视图的攻击模型，将信息网络安全（Security）和物理系统安全（Safety）转化为对攻击的共同约束；将信息网络的异常流量检测结果转化为电网数据的可信性参数，与物理系统安全模型联合作用于电网数据分析。研究成果为物联网的安全监控提出新思路。
基于计算和存储感知的运动估计算法与结构研究	vlsi;motion;H.264;estimation;computation-aware;avs	运动估计是视频编码中计算复杂度和存储复杂度最高的部分，在新标准H.264和AVS中更是如此。尽管集成电路技术、通信技术及电池技术的发展一定程度上缓解了运动估计实现的难度，但并没有根本解决问题，运动估计的实现仍然受到计算资源、功耗资源、带宽和存储资源的制约。考虑到编码器资源有限的问题，从资源受限和资源利用的角度出发，课题采用感知（aware）的方法研究运动估计算法和结构，包括三方面内容：.1..计算感知的运动估计算法研究，即根据可用计算资源情况对计算子任务合理分配计算量，使用有限的计算资源达到高编码质量；.2..存储感知的运动估计算法研究，即根据可用存储和带宽情况对子任务合理分配，满足实时和编码质量要求；.3..资源感知的硬件实现结构研究，即根据上述研究出的算法，设计相应的硬件实现结构；
面向高性能计算应用的软件定义网络技术研究	集合通信;高性能计算;网络管理;路由算法;软件定义网络	SDN技术主要用于网络管理，并且服务于数据中心，而其是否也适用于HPC网络，并服务于HPC应用尚未有相关研究。本课题通过调研发现，SDN技术与HPC应用之间产生了紧密的联系。一方面，未来部分HPC计算机的网络本身就是具有SDN属性的网络，另一方面越来越多的HPC应用部署在以SDN属性网络搭建起来的数据中心。同时，HPC通信在多"子HPC"网络管理，应用路由优化以及特定通信模式加速方面也存在着对SDN技术的需求。因此，针对这些问题，本课题结合SDN技术的特点开展相关研究，包括：利用SDN技术研究多个"子HPC"的网络划分和路由算法；针对HPC应用的通信模式，利用SDN技术研究面向应用的路由优化问题；在具有SDN属性的HPC网络中，开展特定通信模式如小消息通信和集合通信的加速研究。通过这些问题的研究，本课题期望能够将SDN技术拓展到HPC领域，并且提高HPC应用的性能。
大规模非对称可重构流计算理论与技术研究	流计算;体系结构;可重构系统	在众多高效能计算需求领域，可重构技术以其极为卓越的性能而被寄予厚望，成为超越传统计算架构的重要手段。然而，随着可重构系统资源大规模化、异构化的发展趋势，一些关键基础问题，例如大规模可重构系统计算模型、体系结构一体化设计及管理、非对称性难题等等，极大限制了可重构计算的发展和应用。新兴的流计算模型以存储与计算解耦合、计算密集化、数据规则化和高预知性等特点，为可重构计算的发展提供了新的思路。本课题对流计算模型进行创新并引入到可重构计算中，首次提出大规模非对称可重构流计算理论与技术研究，引入了多态可重构计算模型和与之相匹配的可重构流体系结构模板，并基于共有模型和模板对可重构系统进行软硬件协同管理，支持面向应用快速构建和优化可重构平台，提高可重构系统实际计算效能。该项研究具有原创性，对我国发展面向大规模数字信号处理、科学模拟以及其它特殊领域的高效能计算平台具有重要意义。
高阶广义k-对角线性系统的异构协同并行求解算法研究与探索	并行求解算法;异构计算机系统;线性系统求解;并行计算;异构协同编程	目前异构计算系统的高性能和低能耗优势日渐得到业界认可，我国国产大规模异构计算机系统在国际上已处于领先地位，但与之相适应的并行数值模拟应用软件尚较为缺乏。数值模拟计算中的核心问题是线性系统求解，而在化学过程、热传导、电路模拟以及核物理模拟等许多领域的数值模拟中需要求解的线性系统具有广义k-对角特征。为此，本项目针对高阶广义k-对角线性系统在大规模异构计算系统上的高效求解展开研究。首先针对广义对角特征和异构计算系统的体系结构研究矩阵的预处理技术，构建并行的数据模型；然后设计适合异构处理器多层次协同并行的求解算法，并探索影响求解性能的关键因素—数据访问和通信的优化策略；最后力争形成可屏蔽异构协同编程细节的共性函数库，以提高数值模拟应用的开发效率和求解性能。本项目研究成果将在"天河一号"超级计算机上进行实验验证，这将对提高国产大规模异构计算系统在科学计算和工程应用方面的服务水平起到一定的推动作用.
异构服务器共享RAID的IO流互扰分析和数据排布优化研究	存储共享;异构服务器;RAID;IO请求	当今数量庞大的中小型IT企业中RAID存储子系统往往被多个异构服务器共享，对存储资源的竞争，以及异构服务器的IO流互相干扰引起的RAID磁盘磁头频繁寻道，导致共享RAID性能降低，违背了企业存储共享的初衷。本项目提出基于对IO访问的在线分析，结合数据动态重排，实施优化的共享RAID数据排布方案来解决该问题：在共享RAID的物理存储空间划分一个缓存区，用于为每个服务器配置一个"子RAID"；再通过在线的IO访问特征分析，基于对单个磁盘顺序访问的数据块最大化或对多个阵列磁盘并行访问的磁盘数目最大化原则，确定"子RAID"结构，并选择最佳数据集从大磁盘阵列动态重排到"子RAID"，使各"子RAID"始终对相应服务器呈现"专有逻辑高速缓存"的功能，从而隔离异构服务器IO流互扰，提高存储系统IO吞吐率。课题研究成果能让共享RAID为异构服务器提供优化、均衡的存储服务，实现真正的企业RAID存储共享。
面向复杂实时应用的混合存储系统数据分布模型研究	复杂实时应用;固态盘;数据分布;相变存储器	基于相变存储器、固态盘和磁盘的混合存储系统是未来存储系统的发展趋势，其中最核心的一点是基于混合存储系统的数据分布模型。本项目拟研究满足复杂实时应用需求的，基于相变存储器、固态盘和磁盘的混合存储系统的数据分布模型。该模型包括数据分布全局视图、数据分布策略、数据移动策略和数据一致性确保机制，旨在利用不同存储设备的特点，使混合存储系统提供的存储服务能够满足复杂实时应用的时间约束性条件和可预测性条件要求。本项目的研究可为面向复杂实时应用的混合存储系统的设计提供前期理论研究支持和部分实现技术探索。
多域安全互操作环境下隐私保护研究	分布式系统;访问控制;隐私保护;多域安全互操作	在多域环境中，安全互操作是资源共享和协作的重要方式，研究多域环境下的隐私保护安全机制具有重要意义。本项目首先研究多域安全互操作环境下信息隐私性的特征，提出了一种基于虚拟重构的信息隐私性可图形化描述方法；依据隐私安全目标需求提取隐私安全元素，采用云模型理论对定性安全元素进行定量转化，并建立系统隐私安全性可信评估函数；采用安全多方计算理论的方法，结合基于标识的加密技术，构建内容隐私保护机制；基于语义推理和概率推理理论的方法，并结合AHP分析法，针对推理攻击实施协同防御，构建拥有隐私保护机制；最后，分析各安全变量对评估结果的影响，定位系统安全缺陷，基于反馈机制实现安全策略、隐私保护机制等系统配置的动态调整，提出一种动态多域安全互操作隐私保护模型，实现系统隐私安全性的优化。本课题的研究成果可为多域安全互操作安全目标在隐私保护方面的扩展研究提供了方法和思路。
面向赛道存储器多核嵌入式系统的调度与数据分配研究	嵌入式系统;便签式存储器;调度;数据分布;赛道存储器	随着数据密集型应用的增加，嵌入式系统的存储访问开销极大的影响了其性能与能耗。当前，嵌入式系统对于大容量、高性能、低功耗存储介质的需求愈加迫切。近年来，一些新型的非易失性存储器得到了学术界和产业界的广泛关注，成为了解决存储系统性能和功耗问题的最为引人瞩目的新一代存储器件。赛道存储器凭借其高集成密度、低静态功耗以及可与SRAM匹敌的访问速度，被认为可以替代传统存储介质作为嵌入式系统片上存储器与主存储器。但是赛道存储器访问速度不固定的问题极大的制约了其性能与能耗。本项目面向基于赛道存储器的多核嵌入式系统，结合嵌入式应用的任务处理与存储访问模型，建立考虑赛道存储器访问特性的系统性能与能耗模型，在系统软件层设计任务级和指令级两个层次的调度与数据分配策略，研究数据的动态迁移策略，优化多核架构下赛道存储器的性能与能耗，进而提高系统性能，降低系统能耗，为将赛道存储器应用于多核嵌入式系统奠定理论研究基础。
基于移动车辆的城市感知信息收集关键技术研究	城市信息收集;移动车辆;通信机会发现;信息转发;传输速率调整	城市应用和服务需要收集城市环境中多种感知信息。本课题以提供低成本、高效的感知信息收集服务为目标，研究利用移动车辆进行城市信息收集的相关理论和关键技术，主要包括移动车辆获取传感器节点或网络采集的感知信息、移动车辆之间的信息转发、移动车辆通过互联网接入点AP上传感知信息，以及通过模拟或原型系统验证提出的研究成果。本课题拟采用多信道、多模块协作的移动车辆与传感器节点间通信机会发现机制、基于车辆路口转向概率预测的信息转发策略、基于车辆组内协作的信息转发增强机制，以及自适应链路通信质量的传输速率调整策略等技术解决方案，计划在国内外重要刊物和会议上发表12篇以上学术论文，申请4项以上国家专利，力争在国际上相关研究领域产生一定的影响力。
捕获电子辅助比特图纹磁记录技术	磁记录介质;比特图纹磁记录;捕获电子辅助	占信息存储市场90%份额的磁盘记录技术面临着面记录密度提高的瓶颈，主要的挑战来自写入磁头不能在具有超高磁晶各向异性能的磁性介质上写入要存储的信息。辅助型比特图纹磁记录技术可以在不影响介质的热稳定性和信噪比的同时，大幅降低介质的磁化翻转场，有可能使得磁记录的面记录密度提高到10 Tb/in2以上的水平。.铁磁/铁电结构的捕获电子辅助比特图纹磁记录技术是基于磁性材料的晶格获得电子后磁晶各向异性将发生改变，从而通过铁电材料和外加电场双重调节记录介质磁化翻转磁场的方法，更准确和方便地适应不同写磁头的写入能力和各种记录密度的要求。该技术与现有磁记录技术完全兼容，无需对现有磁记录产业的设备等进行复杂的改造，没有明显的成本增加。本项目将制备铁磁/铁电结构的比特图纹磁记录介质，并系统研究该介质应用于10 Tb/in2的面记录密度的可行性和可能存在的问题，为磁记录技术中记录密度的进一步提高进行必要的探索。
面向集群式内存的容错机制和数据组织策略研究	内存存储;数据恢复;集群存储;数据布局;大规模存储系统	分析发现，现有集群式存储的整体性能仍受限于磁盘等外存设备。利用高速网络和节点空闲内存构建的集群式内存能够提供高IOPS、低延迟的数据访问，有助于提升集群式存储的访问性能。针对集群式内存的内存数据容错和内存数据组织挑战，本项目提出一种基于纠删码、采用日志结构的集群式内存方案，通过数据分段、编码、布局等方式，将集群中多个节点的空闲内存构建成一个大容量内存级空间，并按副本散布机制来放置内存数据和外存副本，获得数据持久性支持。理论上，构建集群式内存存储框架，研究其数据容错机制和数据组织策略，建立集群式内存可靠性模型；方案上，研究一组关键支撑技术，包括基于副本散布的内外存数据布局、基于流水线的内存数据恢复、基于日志的内存数据组织和基于日志分段的内存碎片回收方案等。.  本项目旨在研究一种高效、可靠的集群式内存方案，其预取成果将丰富内存层数据容错研究，并为优化集群式存储提供方案借鉴和技术参考。
基于本体的信息网格访问控制研究	本体;访问控制;信息网格;安全策略	访问控制是信息网格系统的重要安全措施，可确保资源的合法有效访问。信息网格访问控制系统的特点在于跨管理域数巨大、同时存在数量众多的访问控制策略、需要授权的用户数量和资源数量巨大、授权和撤销非常频繁、不同管理域间访问控制策略和访问控制实体互操作性要求高等，因而不能直接套用和简单移植现有访问控制模型。本课题在分析上述特点的基础上，通过对现有访问控制模型进行扩展和改进，首先提出适合信息网格环境的访问控制模型框架。然后，将本体技术引入到本研究中，利用本体来识别和表达访问控制实体及其之间的相互关系；利用本体识别与表达不同管理域的访问控制策略，并通过本体及其描述语言建立推理机制，实现不同访问控制策略的语义冲突检查及映射转换；利用基于本体的推理机制实现大规模用户授权及撤销；利用本体实现权限相似性判断等。最后，在理论研究的基础上，在网格实验平台上建立基于本体的信息网格访问控制系统原型。
面向GPU的体系结构敏感型数值算法优化技术研究	数值算法;非规则计算;性能模型;GPU体系结构;自动调优	随着多核技术的不断发展，GPU已经成为高性能计算的主流平台，与以往相比，该系统的体系结构具有更多的存储层次和多样化的线程管理，传统的优化技术已经不能满足体系结构敏感型数值算法的性能要求。该类算法必须重构以实现深度挖掘自身的并行性、局部性和非规则计算特性,通过充分发挥GPU的体系结构优势,提高程序性能。为此，本课题面向GPU体系结构,通过定量的测试和分析影响体系结构敏感型数值算法执行效率的各种因素，形成GPU性能模型，刻画体系结构敏感性指标。在此基础上，研究体系结构敏感型数值算法的多层次优化方法及自动调优策略,改善访存局部性、线程间负载均衡、数据读写和流处理方式。研究规则计算和非规则计算统一的性能优化方法。并且将GPU性能模型用于指导体系结构敏感型数值算法的调优方法和策略。本项目研究成果可以很好地提高体系结构敏感型数值算法执行效率，具有重要的理论意义和应用价值。
基于混合存储介质的云存储环境的元数据管理和数据布局研究	元数据管理;云存储;混合存储;固态存储;数据布局	基于云存储对固态存储介质的需求以及固态存储介质和磁盘存储介质各自的优 势，构建高效能、大容量、多介质、多层次的基于混合存储介质的云存储是存储系统的主要 发展趋势之一。现有的元数据管理和数据布局是面向基于磁盘的传统存储系统，没有考虑固 态存储的特性，在基于混合存储介质的云存储环境中无法充分发挥异质存储资源的优势。 本项目面向混合存储介质的云存储环境，研究和突破不同存储介质和层次结构的元数据 管理和数据布局关键技术：研究面向混合存储介质的云存储环境的可扩展的体系结构，以此 结构为基础，探索有效的元数据冗余分配算法和动态、自适应的元数据负载均衡机制，突破 基于数据访问行为的动态、自适应的数据布局和面向混合存储介质特性的公平、自适应的副 本布局等技术，提高存储的I/O 性能，自适应存储规模的变化，提高数据存储的可用性和可 靠性，为基于混合存储介质的云存储环境的数据管理提供理论方法和技术支撑。
无线Mesh网中基于网络演算理论的QoS保证研究	网络演算;无线Mesh网络;QoS	无线Mesh网是重要的下一代无线宽带接入技术，其中的QoS保证研究正成为当前的研究热点。基于无线Mesh网的流量特征，无线Mesh网QoS保证研究关键在于两方面：其一是性能瓶颈（网关）的QoS性能研究，其二是网关与Mesh路由器之间端到端QoS保证研究。本项目将分析无线Mesh网的流量特征，应用网络演算理论分析网关QoS性能上界，推导保证终端用户QoS的网关数据流带宽分配理论，分析网关与Mesh路由器之间端到端QoS保证机制，建立一套适用于分析无线Mesh网QoS保证的网络演算扩展理论；利用该理论探讨QoS保证的无线Mesh网网关部署和网关协作策略。本项目利用网络演算理论在描述和定性分析网络流量方面的优势，结合无线Mesh网的体系结构和流量特征，研究无线Mesh网的QoS性能，将为无线Mesh网的QoS研究探索新方法，对促进无线Mesh网技术的发展有重要意义。
大规模存储系统性能测试方法与技术研究	trace重放;性能测试;存储系统;人工负载;状态重建	存储系统性能对于访问大量数据的应用至关重要。性能测试有助于确定影响存储系统性能的关键因素，从而可以有针对性地进行分析和改进。存储系统性能测试主要涉及两个技术挑战：创建真实的存储系统状态，生成有代表性的访问负载。虽然存储系统设计取得很大进步，但存储系统性能测试技术的发展却非常滞后。主要表现在一些关键技术问题依然悬而未决，例如：如何在B系统上重放从A系统采集的trace而又保持负载模式；如何保证分布式播放时整体负载的行为特征；如何快速构建大规模存储系统的真实状态。针对现有大规模存储系统性能测试技术的不足，本项目提出了大规模存储系统性能测试方法与技术研究，面向大规模存储系统性能测试的两大技术挑战，主要研究：访问trace的高效收集与合理重放技术；可扩展的有代表性人工负载生成方法；大规模存储系统真实状态的快速构建技术。最终目标是构建可重复、可扩展、易配置、可信赖的存储系统性能评测体系。
高并发数据访问模式的基础理论与系统设计	基础理论;混合存储系统;并发存储访问;存储墙;访存模式	存储系统已成为从多核处理器到超级计算机的各种规模计算机系统中最严重的性能瓶颈，各种新兴数据密集型应用与新型存储技术不断呈现，面对新的挑战，学术界亟需建立相应的基础理论。适应后摩尔定律时代的要求，本课题提出用体系结构的方法尤其是并发的方法解决日益严重的存储墙问题，在前期研究基础上，立足国际研究前沿，构建高并发数据访问的基础理论，具体包含四个方面的研究工作：1）建立高并发数据访问模式的数学模型和用于解决存储墙问题的"访存模式周期表"；2）建立数据访问并发度与数据访问性能之间的关系模型；3）提出混合存储系统中访存模式的优化算法；4）提出分布式共享存储系统中远地访存模式的优化算法。本课题将回答关于访存模式的12个基本的科学问题，注重理论的简洁性、易用性、完备性，将对优化数据密集型应用的性能、提高超级计算机系统效率发挥重要作用。本项目拟发表6篇以上国际顶级水平论文，并申请3项以上美国或中国发明专利。
安全关键的信息物理系统中时序可预测性问题的研究	服务体模型;分布式编程模型;信息物理系统;时序可预测性;时序验证	安全关键的信息物理系统中，可预测性是保证系统实时性和可靠性的关键，但学术界对如何定义和衡量系统的可预测性，如何设计与实现具有可预测性的系统尚无统一认识。本课题拟通过对系统行为特征的深入分析和建模，给出CPS中可预测性的形式化定义，并据此提出可预测性系统的构建方法。首先，研究CPS中可预测性的形式化定义以及衡量标准。与传统的实时系统可预测研究中只注重任务执行时间的可预测性不同，本课题提出CPS的可预测性应该包含时间可预测和顺序可预测两个方面。其次，研究具有可预测性的时序模型，提出模型层间的时序映射方法，确保时序语义的一致性。第三，基于上述时序模型建立具有分布透明特点的编程模型，设计并实现编程语言，以及具有时序语义一致性的运行时系统。最后，提出一种基于模型相似度理论的评估方法，支持在设计阶段验证系统时序行为。
异构内存计算系统的扩展性问题研究	GPU通用计算;并行编程;异构体系结构;扩展性;内存计算	异构体系结构和内存计算是当前计算机科学研究快速发展的两个方向，异构内存计算系统的研究也日益受到学术界和工业界的重视，内存数据库系统、内存图计算等内存计算应用被广泛部署。目前，异构内存计算系统的研究还存在几个主要问题。第一，分离的异构设备间地址空间，增加了异构系统的数据管理的复杂性和编程难度；第二，传统NUMA结构和异构设备组成非对称的异构NUMA，使得优化内存访问尤为重要并且更具挑战性；第三，缺乏异构并行任务的并发控制协议，使得开发高并发高性能异构计算应用非常困难。因此，本项目研究目标是：实现异构内存空间的统一管理，优化复杂NUMA架构的内存数据组织，设计异构系统并发控制协议；从内存地址空间可扩展、复杂NUMA架构下系统性能可扩展、并行计算可扩展三个方面研究异构内存计算系统的扩展性问题。聚焦于前沿研究方向和异构内存计算中的关键科学问题，将有助于推动相关理论问题的研究和应用技术的开发。
基于占空比的无线多跳网络分布式数据传输关键技术研究	延迟调度;占空比;能量均衡;无线多跳网络	随着移动互联网和无线通信技术的迅猛发展，无线多跳网络成为目前科学家所关注的一个核心研究问题。在无线多跳网络中，低占空比的无线多跳网络调度算法是其研究的关键性问题之一。.   传统的无线多跳网络调度问题均假设以无线网络节点为全激活状态为基础。这种网络全激活状态，虽然一定程度上解决网络中延迟等网络性能问题，却忽视了网络节点全激活的高能耗问题。本项目针对这一问题，基于前期对无线多跳网络的虚拟骨干网构建和网络延迟调度的研究基础上，对基于占空比的无线多跳网络数据传输调度算法进行深入的研究。理论上，本项目致力构建新型基于低占空比的无线多跳网络数据传输调度问题算法，即针对基于占空比的无线多跳网络中不同网络中的核心问题来进行研究，从理论模型向实际模型转化，在设计上综合考虑能量均衡及网络延迟等因素的低占空比无线多跳网络数据传输调度算法方面取得突破。
基于全局时钟的并行程序调试的若干关键技术研究	并行理论;片上多核处理器;调试;全局时钟;可调试性设计	为充分发挥片上多核处理器的计算能力，并行程序越来越受到人们的关注。但由于访存执行时间的不确定性，进程间访存交错空间呈指数级增长，并行程序的调试非常困难。因此学术界和工业界开始广泛关注并行调试问题，希望通过软硬件系统的支持来降低多核处理器上并行调试的难度。然而传统的并行调试都是建立在Lamport多年前提出的分布式系统逻辑序理论之上。这套理论不考虑全局时钟，仅依靠进程间的通讯来对访存事件排序，给记录、检测和重放等调试常见行为都带来了很大的困难,使得目前的并行调试代价大、效率低。本项目考虑到片上多核处理器中各个核距离非常近，已不再是传统意义上的分布式系统，因而引入可扩展的低误差全局时钟。通过全局时钟带来的全局序，可望对并行调试带来变革，将并行调试的一些关键问题如信息采集、结果检测和确定性重放大幅度简化，实现一套较为完善的片上多核处理器上的并行程序调试系统，为国产多核处理器的设计提供借鉴。
异构多核平台上基于软件分布式共享内存的编程模型研究	软件分布式共享内存;性能优化;编程模型;异构多核	异构多核是由架构不同的核心组成的计算平台，由于其能够在特定的领域中针对应用的特点充分发挥不同处理器核的作用，成为了高性能计算平台主流架构和硬件基础。但也由于其不同架构组合复杂多样，传统的单一消息或者共享的编程模型都难以清晰描述各个计算部件之间的协同关系，导致其面临着编程要求高、产能低和调试的困难等挑战。本项目在由GPU集群构成的异构多核计算平台上，以软件分布式共享内存模型为基础，研究基于GPU设备内存的分布式共享设备内存的编程模型，达到简化编程，优化性能的目的。重点研究如下几个问题：(1)基于GPU设备内存、主存的两级共享内存的数据一致性维护；(2)GPU设备内存地址空间到主存地址空间的映射机制；(3)CPU-GPU间负载合理分配策略；(4)共享内存中数据预取策略。本项目旨在能为降低异构多核平台上的编程复杂性，提高编程效率奠定理论和技术基础。
片上网络的高效拥塞感知及协同拥塞控制方法研究	拥塞感知;适应性路由算法;片上网络;拥塞控制;限流	网络拥塞是片上网络设计需要解决的一个关键问题。对网络拥塞进行控制的方法主要有两类：适应性路由算法与限流方法。已有的适应性路由算法所采用的拥塞感知策略效率较低，难以兼顾拥塞信息的高清晰度、全局性与可扩展性，能够成功避让路由器热点的概率较小，使得端到端延时较大；已有的限流方法在拥塞感知时同样难以兼顾拥塞信息的高清晰度、全局性与可扩展性，使得精确的限流比例难以被设定，出现因限流不足或限流过度导致的系统性能下降问题。.    本课题拟利用片上网络中流(flow)传输范围固定且可见等特征，研究片上网络的高效拥塞感知策略；在此基础上，分析路由器热点位置对适应性路由算法和限流方法性能影响的规律，研究更高性能的适应性路由算法和限流方法；此外，针对这两类拥塞控制方法间未能有效协同的问题，将量化其各自适用的网络场景，研究其协同拥塞控制策略，在保持高吞吐量的情况下有效降低端到端延时，提升系统性能。
面向绿色数据中心的高效能分布式储能技术	分布式储能;可再生能源;电能供给;绿色数据中心;功耗管理	信息时代的能耗和环境问题使得绿色数据中心备受瞩目。分布式储能最近成为绿色数据中心中新兴的重要技术: 它能削减负载峰值功耗带来的运营成本，还能便于融合新能源来限制碳排放。然而以往研究忽视了储能设备本身的结构优化，因此面临着一个关键设计瓶颈。目前方案多局限于利用单一同构的储能技术完成简单的电力缓存任务，它们非但缺乏应对特殊功耗波动的能力，还难以保证系统的额定使用寿命。更重要的是，若没有一种跨层次的管理，数据中心会因缺失备份能源而面临宕机风险。本项目将探索新型的数据中心储能方法：一种跨越储能设备和软硬件的综合电源管理技术。研究将侧重三个关键科学问题，即， 如何设计混合异构储能系统来管理复杂供电异常，如何均衡使用储能设备从而提升其使用寿命，及如何协同调度负载以降低供电失效风险和优化系统全局可用性。本项目将有效提升数据中心的可持续性和经济性，并为新型绿色中心提供有力的支持。
基于共享射频互连的片上网络通信资源分配及布局的优化技术研究	高性能嵌入式系统;射频互连;片上网络;资源分配;路由策略	随着半导体集成电路的飞速发展，单芯片上的处理核心数量急剧增加，传统的基于电气互连的片上网络将成为大规模片上系统性能提升的瓶颈。基于射频互连的片上网络能够以接近光的速度实现低能耗的远距离数据传输，是极具潜力的有效解决方案之一，成为未来片上网络发展的重要趋势。但是，由于片上资源的有限性和应用需求的多样性，如何实现射频通信资源的合理分配和高效利用，成为了射频片上网络研究迫切需要解决的关键问题。针对该问题，本项目创新性地提出了射频通信资源的全局共享和实时按需分配，设计和实现基于共享射频互连的片上网络系统模型，提出动态与静态相结合的通信资源分配策略和无死锁的自适应路由机制，对片上资源节点的布局进行联合优化设计，高效率利用有限的片上射频通信资源，满足不同特征应用的实时通信需求。本项目的成果将为构建高性能互连网络提供新思路，为新型数据计算系统设计提供一定的理论和技术支撑。
微处理器硬件木马旁路检测技术研究	微处理器;硬件木马;旁路分析;电磁云图;差分信号采集	微处理器是完成信息处理的核心芯片，是系统安全的关键环节，其安全性受到硬件木马的严重威胁。旁路分析方法利用芯片结构与旁路信号之间的关联检测芯片内部的异常变化，是进行硬件木马检测的最具潜力方法之一。然而，当前旁路检测方法主要针对小规模电路或加密等特定功能电路，难以满足微处理器硬件木马的检测需要。本项目以微处理器为研究对象，分析典型微处理器的体系结构特征，研究微处理器硬件木马的设计机理及其特征；为了解决示波器采集旁路信号时存在的信号不对齐、噪声干扰大等问题，设计面向微处理器的旁路信号差分采集平台，实现高质量的差分旁路信号采集；研究微处理器功能模块的电磁辐射特征及激励技术，利用电磁云图技术进行区域电磁旁路信号采集；研究差分旁路信号特征提取与差异判别技术，实现准确可靠的微处理器硬件木马检测。课题研究成果能够有效提高微处理器的安全性，为实现信息安全处理与安全控制奠定基础。
基于曲线演化的智能图像轮廓提取方法与并行处理研究	并行处理;曲线演化;轮廓提取;神经网络	图像处理和计算机视觉有许多底层任务，如搜索、图像分割、体视匹配、运动跟踪等，这些都涉及到图像轮廓的提取。曲线演化对于广泛的一系列视觉问题给出了统一的解决方法，成为解决轮廓提取一类问题的有效途径。尽管有关曲线演化的研究已有了相当的成果，提出了一些解决方法，但普遍存在分析结果受初始曲线影响较大，曲线演化收敛性难以保证以及计算量大等问题。.本项目将对基于曲线演化提取图像轮廓所面临的关键问题进行研究，以提高轮廓提取的准确性、实时性和稳定性为研究目标。在曲线演化建模和数值实现的过程中，综合运用模糊理论、细胞神经网络和多网格并行处理等手段，设计和构造出一套能够依赖应用要求自适应演化的，与初始曲线无关，具有快速收敛和精确定位能力的图像轮廓自动提取算法，并通过理论分析和实验的手段评价和比较所设计的算法的性能，给出理论分析结果。
移动自组织网络中实时流媒体传输的控制与调度协同方法	调度;实时控制;移动自组织网络;流媒体	移动流媒体是为移动用户实时传输流媒体数据的新型移动业务。实现移动自组织网络中实时流媒体传输，保证服务质量，将产生巨大的社会经济效益。本课题针对移动自组织网络和实时流媒体的特殊性，围绕移动自组织网络实时流媒体传输涉及的多约束条件，面向大规模、多目标、多约束和网络异构性等问题，研究实时流媒体在移动自组织网络中的传输机制。研究基于QoS约束的并发多径路由协议设计、多信道传输机制设计、能源智能管理设计。应用多信道技术和并发多径路由协议联合进行网络控制和资源调度，结合主动队列管理和能源优化，实现流媒体的最佳传输，适应多种网络条件的实时流媒体传输，实现异构网络无缝连接，克服无线网络中实时稳定传输流媒体的困难，满足实际工程应用中的要求。研究理论成果的工程应用方法，能够在工程中得到应用，促进理论研究的深入。本课题的研究成果将为实现流媒体网络的工程应用提供理论基础，具有重要的理论意义和工程应用价值。
云环境中的安全外包计算技术研究	安全模型;云计算;数据隐私保护;安全外包计算	云计算是分布式计算、并行处理计算、网格计算等概念的发展和应用，它实现了人们长期以来的"把计算作为一种设施"的梦想。安全外包计算技术是云计算走向实际应用不可或缺的关键技术。本项目主要研究云环境中的安全外包计算技术及其应用，其中着重研究外包计算的安全定义与模型；科学计算的安全外包技术；密码基础运算的安全外包技术；基于属性的密码方案的安全外包技术等。这些关键技术对于促进云计算健康、快速、长期的发展有着重要的意义。目前，安全外包计算已成为学术界和产业界共同关注的研究热点。因此，本项目的研究不仅具有重要的理论价值，而且具有巨大的实用价值。
基于多点协作的多媒体传感网数据感知与融合技术研究	数据感知;多点协作;数据融合;无线多媒体传感器网络	数据感知与融合是无线多媒体传感器网络数据收集的重要环节，作为物联网的重要基础感知网络，开展无线多媒体传感器网络数据感知与融合的科学研究具有重要的理论意义。本课题将重点研究无线多媒体传感器网络节点部署及覆盖调度技术、多媒体数据压缩采集方法、网内信息聚合处理技术等，在此基础上设计和实现多媒体传感网在公共安全监控中的数据融合应用及验证，从而为无线多媒体传感器网络的实际应用奠定理论基础。
多核混合关键度系统中软件级节能关键技术研究	信息物理融合系统;混合关键度任务;多核平台;软件级节能;DVFS和DPM	随着信息物理融合系统在体积、重量和功耗等方面要求越来越高，不同类型的任务正日益集成到单一平台中从而形成混合关键度系统。本项目以多核混合关键度系统为研究对象，研究软件级节能关键技术。本研究包括建立多核系统协同节能框架，为任务映射、空闲时间管理和运行时控制提供支持；提出内存和处理器核相结合的任务映射方法，从而在保证安全关键任务实时性的同时，提高非安全关键任务吞吐量并为降低处理器和内存的总体能耗提供支持；研究运行时核间和核内空闲时间回收、预取和预留等空闲时间管理方法，为任务和内存控制提供支持；以及探索运行时任务状态控制和内存功耗控制相结合的节能方法，从而在保证安全关键任务满足截止期限的情况下，重用空闲时间，提高处理器的利用率、非安全关键任务的吞吐量和降低能耗。本研究为设计多核平台中能量有效的混合关键度系统提供理论依据、算法和实现技术，有助于提高混合关键度系统的可靠性、吞吐量和节省能耗。
深度学习算法可重构加速器关键技术研究	可重构;异构体系结构;深度学习算法;加速器	深度学习算法已成为机器学习领域最新最重要的一类人工智能算法，应用范围和数据规模日益增大。由于涉及多次迭代和大规模矩阵运算，该类算法处理成为典型的计算密集型过程，其加速技术渐成为近年的研究热点。FPGA是实现高性能深度学习算法的有效平台，而目前的研究仅限于对深度信任网络这一种特定算法的定制实现，不能满足该类算法集合中各种改进和变形流程的加速需求，适用性差，算法精度影响分析不足。本项目将针对主流深度学习算法集合，研究算法流程共性和特异性,归纳统一的算法模板，建立定点化精度影响分析模型；面向FPGA特征，提出可重构的深度学习算法异构加速器体系结构，重点研究一套有效的硬件实现和加速优化方法；研究加速器参数化设计方法和自动生成技术，实现一个面向深度学习算法典型应用的可重构FPGA加速器原型系统和一个加速器自动生成软件系统，满足算法精度要求，全面提高这类算法加速应用的性能和灵活性，应用前景广阔。
重复数据删除存储系统的可靠性关键技术研究	重复数据删除;闪存存储;可靠性;备份存储	重复数据删除技术已被应用于备份存储系统和闪存存储系统中，但其对存储系统可靠性的影响仍是一个开放性的问题。本项目拟基于闪存存储系统，研究基于文件的内部校验冗余保护技术，既保护单个文件的所有数据块，也通过文件间迭代恢复技术进一步保护引用率高的关键数据块，解决重复数据删除技术对闪存存储系统可靠性影响，以基于重复数据删除技术的闪存存储系统的可靠性，进一步推动重复数据删除技术在闪存存储系统中的更广泛应用。
云计算中虚拟资源性能度量的指标和方法研究	虚拟机;云计算;性能度量	基础设施即服务（IaaS）的云计算服务模式一般以虚拟机的形式向用户提供计算资源，一个虚拟机具有一定的计算能力和存储容量，用户根据自己的计算需求租用虚拟机，按照资源提供方对虚拟机的性能度量数据支付费用。目前对云计算中虚拟机性能的度量方式存在度量指标粗糙、度量结果不准确等问题。本项目拟研究云计算中虚拟机的性能度量指标和度量方法。通过分析底层硬件特征、虚拟机间资源竞争关系、多虚拟机部署方式等影响性能的因素，建立云计算底层软硬件平台特征模型，制定全面的性能度量指标系统，反映虚拟机的处理能力、性能隔离度、稳定性、可用性等性能特征。进一步提供标准化的度量单位，计量单个虚拟机独立运行的性能和多个虚拟机联合工作的整体性能。最终的度量结果能指导用户合理选择云计算资源，指导资源提供方准确分配资源、合理部署服务，保证资源的服务质量，提高资源的利用率。在此基础上，根据用户应用需求和度量结果研究虚拟机部署策略。
高阶互连网络中路由算法与交换开关调度方法研究	互连网络;拥塞度建模;通信优化;路由算法;交换开关调度算法	随着计算速度的不断提高，高性能计算系统对通信性能的需求持续增长，高阶互连网络被提出并成为研究热点。本项目旨在研究高阶互连网络通信中路由算法设计和交换开关设计问题。通过分析影响负载分布和流向的因素，并综合考虑消息类型、通信负载、消息队列长度、路由器内部及物理链路上的传输延迟等信息，建立拥塞度的数学模型，递进设计确定性路由算法、适应性路由算法和拥塞相关的适应性路由算法；针对高阶网络中交换开关端口数较多的特性，在保证公平性和严格的时间约束下，建立交换开关资源-消息之间的调度模型，并针对不同的问题特点，设计求解模型的优化算法，在交换开关上递进实现拥塞无关的调度算法和拥塞感知的调度算法。项目在理论研究上解决高阶互连网络中通信优化问题，包括路由算法设计的关键问题以及交换开关设计中关键优化理论和方法，将为我国高性能计算系统的性能提高、新产品的研发、以及国际地位的赢得提供理论方法和技术支持。
GPGPU访存系统中共享资源管理和调度的关键技术研究	线程级并行性;访存系统;通用图形处理器;异构计算系统;基于内存语义的芯片间互连	GPGPU已成为多种计算系统中不可或缺的重要加速部件。然而，较高的线程级并行性以及系统中越发增加的GPGPU数量会竞争使用访存系统中各类有限的共享硬件资源，严重影响加速性能。针对上述问题，本课题拟从不同体系结构层次，对GPGPU访存系统共享资源的管理方法及调度策略展开研究，减少资源竞争的情况，进一步提升GPGPU处理性能，具体包括三个方面的研究工作：1）在SIMT处理器内部针对不同线程组织粒度的特点，进行活跃线程限制和缓存旁路，减少L1数据缓存抖动；2）在进行L2缓存资源分配和内存控制器请求调度时，考虑不同SIMT处理器的访存特征及延时差异；3）在多GPGPU系统中使用基于内存语义的统一片内-片间互连，降低远程访存开销，同时根据本地和远程访存请求的差异进行调度。通过软件模拟器评估及关键模块FPGA原型测试，验证上述研究内容。本课题拟发表3-5篇SCI/EI检索论文，并申请4-5项专利。
机群文件系统小文件I/O访问性能优化方法研究	访问模式;客户端缓存;机群文件系统;小文件I/O	小文件I/O访问是机群文件系统重要性能瓶颈之一，其性能优化是当前国内外的热点研究课题，现有的相关研究工作主要从I/O访问模式、文件系统体系结构和文件请求处理三方面来开展。针对现有研究还存在的数据分条和冗余链接延迟等问题，以如何在不影响大文件I/O访问性能的前提下，优化小文件I/O访问性能为目标，以小文件I/O性能瓶颈的源头（元数据服务器、I/O服务器、客户端）为主线，基于I/O访问处理过程，通过改进现有的元数据填充等方法，研究机群文件系统小文件I/O访问性能优化的新方法：提炼小文件I/O访问模式，建立面向小文件的存储优化、动态迁移优化、客户端缓存优化新方法并研制原型系统。从I/O性能优化理论入手、理论与实践互相促进、以实践为目标，探讨了增强、改进和创新机群文件系统性能优化的新理论和新方法。对机群I/O瓶颈缓解、机群整机性能和企业高效能计算能力提高都具有重要理论研究和工业实践意义。
车联网环境下基于5G和云平台的智能交通关键技术研究	智能交通;车联网;分布式系统;云计算环境;隐私保护	车联网作为物联网和移动互联网发展的代表性产物，成为现代智能交通的重要组成部分。随着5G通信的出现和云平台的日趋成熟，给智能交通带来了新的发展机遇。本项目拟针对车联网环境下基于5G和云平台的智能交通关键技术进行创新性研究。首先研究数据传输方法和优化策略，包括基于信息熵和QoS感知的传输方法、多链路和多信道联合的传输方法，以及基于激励机制的优化策略；然后研究分布式动态寻径方法，包括基于轨迹推测的道路开销评估模型、基于云端辅助的寻路数据融合方法和基于动态开销博弈的路径决策方法；同时研究紧急事件处理方法，在基于云平台的车辆协作式安全应用框架上，分别研究基于5G协助的辅助安全驾驶方法和紧急消息广播机制；最后研究车联网中的隐私保护与消息高效签名认证方法，包括车辆动态寻径过程中的隐私保护和基于云辅助的消息高效签名认证方法。本项目的研究成果将为车联网环境下智能交通的应用和发展提供理论支撑和技术保障。
大规模存储系统中多维元数据组织结构与快速查询方法研究	数据查询;元数据管理;存储系统;海量信息	大规模存储系统中多维元数据的组织结构和快速查询方法是面向海量信息提供可靠、有效的存储服务的前提和基础，目前已有的存储系统的元数据组织结构和查询方法还不能有效地支持存储系统中多维元数据的点查询和范围查询操作，其执行效率、查询速度和结果的准确性都较低，严重影响了存储系统的整体性能。本项目将解决通过现有方法还不能有效解决的、在大规模存储系统中组织和查询多维元数据的重要问题。项目拟通过融合具有空间有效性、快速查询优势的Bloom filter结构和支持多维元数据、范围查询的R-tree结构来提出一种新的、能够支持点查询和范围查询的多维元数据组织结构，称为RBF（R-tree with Bloom Filter），以及多维元数据的插入、删除、更新机制和点查询、范围查询方法，这将使得RBF结构在支持存储系统的可扩展性、节省存储空间、提高查询效率和查询结果的准确性、支持动态操作等方面具有突出的优势。
基于模型的自适应MPSoC系统设计技术研究	形式化模型和方法;片上多处理器系统;自适应系统及其控制;系统级设计分析和评估;离散控制器合成	自适应MPSoC系统具有根据应用场景和运行环境动态调节其运行行为的能力，比如动态应用映射、动态电压和频率调节（DVFS）等，从而达到降低系统功耗、提高系统容错能力和可靠性等目的。本项目拟采用基于高层形式化模型的方法对此类系统的设计展开研究，为其早期设计提供支持，帮助设计人员提高计效率的同时保证其所设计产品的可靠性。具体研究内容包括：1）系统每个应用场景的设计，即如何在考虑运行条件可能出现变化的条件下来对各个场景中的应用任务进行资源分配和调度，从而使设计满足系统正确性以及性能和功耗等方面的需求；2）系统自适应行为的动态管理和控制，即如何设计一个正确的管理器来对系统自适应行为进行动态管理和控制，从而满足系统可靠性和降低功耗等需求；3）自适应MPSoC系统设计流程和工具设计，即如何设计一个整合前两项研究内容的设计流程，并开发相应的辅助设计工具来协助设计人员进行高效可靠的设计。
面向大规模序列同源问题的并行分布式算法及其关键技术研究	并行分布式算法;测序序列比对;特征提取技术;索引和压缩技术;代码克隆检测	测序序列比对和代码克隆检测都是寻找一些序列片段的祖先或原版片段，我们称之为序列同源问题，学术界和工业界存在许多这样的问题和应用。像千人基因组数据在50TB以上、大型软件代码都在5000行以上，如此规模的序列数据组织、并行分布式算法与体系结构成为了严峻的挑战。我们对两个独立发展的问题在序列同源层面上进行关联研究，挖掘可借鉴的技术和方法，进而在更高层面上提出新概念和新方法。本项目将以索引、压缩和特征提取等关键技术创新为基础，面向问题的并行分布式算法全过程设计和优化为主线，应用并行分布使能技术和大规模计算模式进一步提升求解能力。创新和突破体现在：(1)借鉴序列比对中的索引和压缩技术，创建基于动态索引和压缩的代码克隆检测快速方法；(2)参考代码克隆中的特征提取技术，创建基于有损压缩的测序序列比对快速方法；(3)设计融合Hash和BWT一体的时空平衡索引结构，适应各种存储受限的并行分布式体系结构。
异构多核处理器片上数据管理的关键技术研究	高效能的流式数据存储技术;高效能的流式数据一致性协议;高效能异构单元访存接口设计;异构多核处理器	通过分析大量应用程序行为，我们发现异构多核处理器的片上数据通信具有明显的"流式"特征。即对于同一访问序列的绝大部分请求，①地址连续，②共享同一数据源或目的地，③数据访问呈现出有规律的时间间隔性。并且，数据流之间具有粗粒度数据级流水线特性。然而，目前异构多核处理器多是简单地将异构单元集成到传统的同构多核处理器体系结构中，继续沿用传统的互连结构和通信机制。这种简单集成导致主流异构多核体系机构的设计和优化都是以缓存行（cache line）为单位，在传输流式数据时存在不必要的能耗和性能代价。本项目针对现有异构多核体系结构在存储、共享及传输流式数据等方面面临的一系列问题开展深入研究。重点研究高效能的流式数据存储技术；高效能的流式数据一致性协议；高效能异构单元访存接口设计等。本项目的研究将解决流式数据的高效能管理机制的关键问题,对于异构多核处理器的体系结构设计将会产生积极影响和重要指导作用。
BC图多处理器网络类中基于限制故障集条件下的可靠单播和广播研究	限制连通度;限制故障集;可靠广播;BC图;可靠单播	通常的多处理器网络中的可靠性信息传递是基于无限制故障集条件下的，但基于该条件下网络的容错度一般是较低的。BC图是一类包含若干个性质优越的超立方体变型的多处理器网络。为了提高BC图的容错度，本项目将限制连通度的概念引入BC图多处理器网络类中并证明这一引入的合理性。证明由此使得BC图的容错度比无限制故障集条件下提高大约一倍。给出BC图中基于限制故障集条件下的高效可靠单播和广播算法；分析算法的时间复杂度，估算用可靠单播算法求得的给定两个无故障顶点间的可靠路径长度和用可靠广播算法求得的以给定无故障顶点为根的可靠生成树的高度。通过模拟试验将我们的算法与传统的广度优先生成树算法进行对比。所有内容都利用BC图中所有网络的共性进行研究，因此研究方法具有一般性，从而避免了对BC图中特殊网络逐一进行研究的缺点。研究结果将不仅适用于已定义的几种超立方体的变型，而且适用于除它们以外的尚未定义的超立方体的若干变型。
云计算数据中心高可用理论与方法研究	副本优化;虚拟化;云计算;数据中心;快速检查点	云计算以数据中心为基础，通过虚拟机技术对外提供灵活的IT服务。由于数据具有不可再生性，云计算数据中心的可靠性是云计算能否推广的关键。本课题以最大效能发挥数据中心资源的计算与存储能力为目标，基于多计算系统虚拟化技术，研究（1）具有容错功能的云计算编程模型，包括编程模型的数据感知等；（2）云计算中心多虚拟机可靠性增强技术，包括虚拟机内存压缩机制、虚拟机增量检查点机制、多机虚拟化检查点协同协议、拜占庭容错以及多虚拟机副本优化模型等；（3）云计算数据中心数据可用性增强模型与方法，包括多副本快速组通信机制、数据文件依赖性管理等。（4）云计算数据中心可靠性评测模型与方法。包括基于虚拟机重放机制的错误注入机制、分类引擎分析等。
面向对象的基三多核处理器体系结构关键技术研究	TriBA;CMP;多核;基三体系结构;面向对象	TriBA(Triplet Based Architecture，基3体系结构)是申请者提出的全新面向计算密集型应用的CMP体系结构，其处理节点硬件直接支撑高级语言及面向对象语法，与现有CMP结构相比具有多方面优势。.  本项目旨在通过从硬件上对面向对象高级语言语法提供直接支撑，缩小硬件结构与面向对象程序间的软硬件裂隙（Gap），进而提高处理器运行面向对象程序效率。将针对涉及TriBA有效性、可实现性的物理层布局布线总体策略、层次化共享存储Cache协议和访存机制、对象计算模型及多个面向对象语法硬件支撑实现策略等关键问题进行深入研究。.  本项目所提出的TriBA互连结构、层次化共享存储结构、面向对象语法硬件支撑实现策略、Tile化TriBA布局布线实现方案等具有十分明显的原创性特色。.  项目成果对国内多核体系结构研究，及未来多核处理器研究开发具有重要的理论意义。
众核处理器结构上的并行程序执行模型	片上多处理器体系结构;并行编程模式;众核处理器体系结构;处理器性能评价;并行程序执行模型	众核处理器结构能够很好地应对纳米工艺代芯片设计的线延迟、功耗和设计复杂性问题，是一种能达到万亿次级性能扩展潜力的片上多处理器结构设计方案。本项目从并行程序执行模型的角度来研究广泛适用的众核处理器结构，这样的结构不仅要支持性能的可扩展性，更要很好地支持可编程性。主要研究内容包括：并行程序执行模型的应用适用性、并行程序执行模型的硬件支持、并行程序执行模型的描述。本项目的预期目标是提出与不同类型的应用相匹配的并行程序执行模型，用于指导众核平台上的并行算法、并行编程模型和并行系统软件的设计，帮助结构设计人员设计高效能的众核处理器结构，以尽可能小的并行程序设计难度、系统软件复杂性和硬件实现代价尽可能多地从应用程序中开发出众核结构上可利用的并行性；对于探索2010- - 2020前后高效能通用微处理芯片体系结构的发展道路具有重要的研究意义。
基于关联感知的混和存储系统组织模式及关键技术研究	海量数据;计算机体系结构;存储系统;文件系统	当前的大规模文件系统的存储组织模式面临两个主要的问题，海量数据和异构设备，这使得存储系统的可扩展性和性能优化面临挑战。为解决这两个问题，项目提出基于关联感知的混和存储组织模式，其基本思想是在内存中通过局部性特征灵敏的哈希计算来快速、准确地聚集具有关联性特征的数据，这样便于在闪存固态盘中构建具有关联感知的数据的索引结构。多粒度存储模式的优势在于混和存储系统中的数据具有关联感知特性，因此大量的文件操作(如读/写/更新等)能够以顺序的方式执行，这样能够显著提高固态盘的性能和减少擦写次数；存储模式丰富了系统和用户的交互接口，便于用户表达需求，并转化为系统可执行的操作；面向关联特征的结构设计能使上层应用实时感知存储数据的布局和访问行为，提高海量数据的存储效率和异构设备的利用率；基于聚集的多版本更新方法能够提高系统的可靠性。研究具有一定的前期基础，研究工作将在海量存储环境中进行系统实现和测试分析。
面向异构并行系统的生物序列比对并行策略及算法研究	序列比对;并行算法;异构并行系统	序列比对是生物信息学中重要的基本问题,是生物信息学的基础,可用来预测序列的功能、结构和进化过程等. 随着大规模测序技术日益成熟,序列数据呈指数级增长,使得现有序列比对并行策略中存在的可扩展性问题日益突出.同时,现有的序列比对并行策略多使用同构系统求解,且极少采用数据并行方案. 随着高性能计算系统快速发展,应用异构并行系统求解各类NP难解问题已变得越来越普及和流行. 本项研究将在异构并行系统中求解序列比对问题.首先提出一种异构并行系统计算能力描述模型,然后设计基于聚类的新的数据并行策略,最后通过0-1整数规划求解并行调度最优解,并设计近似最优的启发式算法.本项研究不仅为生物序列比对基于异构超级计算机的并行化策略和方法奠定基础,为生物信息学中数据密集应用提供高性能计算解决方法,还将拓宽超级计算机应用领域,推动生物信息学的研究与发展.
宏观情景拟合的内容中心网络自适应路由方法研究	宏观情景拟合;内容路由;情景感知;隐私保护;内容中心网络	内容中心网络作为一种被广泛认可的未来互联网架构，但是其现有路由方法无法有效适应用户兴趣、位置等情景的变化，在移动性支持和服务质量保证方面亟待改善。为此，本项目提出宏观情景拟合的内容中心网络自适应路由方法，该方法突破单个用户情景感知的传统思路，将众多分散的用户情景抽象为高度收敛的宏观情景，并依据宏观情景信息对内容分发策略进行动态调整，实现用户在位置、兴趣等变化时的内容高效获取。研究内容包括：1）感知用户群体的宏观情景，即研究情景抽象模型以及情景语义识别和宏观情景挖掘技术；2）提出用户情景信息的隐私度量方法和隐私泄露限制方法；3）设计宏观情景拟合的内容路由方法，通过差异化的缓存决策和数据传输模式实现兴趣情景的拟合，通过基于拓扑势场的内容分发和基于网络编码的内容缓存实现位置情景的拟合。项目成果将为未来互联网内容分发等相关研究提供新思路和理论支撑。
基于弱同步策略的分布式深度学习并行优化理论与方法研究	深度学习;分布式系统;弱同步理论;并行优化	深度学习是当前大数据分析挖掘一个重要研究热点。巨量数据和超大网络规模使得采用并行分布式方法成为必然。然而，当前深度学习主流并行优化思想还处在以严格同步策略为基础、以矩阵计算优化、大粒度并行为手段的初级阶段，算法的效率和灵活性受到很大限制。为研究探索更高效的并行优化理论模型和方法，课题拟开展如下研究：1）深入研究分布式深度学习的弱同步策略及其理论基础，探索并行优化方法内在的基本规律和理论依据；2）研究多粒度融合的新的数据并行方法及分层弱同步策略，挖掘潜在的数据并行效能；3）研究参数模型并行中高频数据交换的流水线弱同步方法，突破数据交换的瓶颈；4）研究新的模型参数管理及维护模式，突破当前集中式参数服务模型的瓶颈，探索高效分布式参数管理方法及其在弱同步模式下的工作机制。本课题是对传统深度学习并行优化思想的一次突破，将为分布式深度学习提供更灵活高效的并行化策略及更强有力的理论支持。
面向云计算的虚拟机能耗模型及其应用方法研究	能耗优化;云计算;虚拟机能耗;能耗仿真;能耗模型	随着云计算的能耗问题日益突出，其能耗管理优化研究越显重要。能耗模型是云计算能效优化研究的基础，为此，本项目将研究云服务器的关键部件（CPU、内存和磁盘）的资源使用情况与系统能耗的数学关系，给出各关键部件能耗建模方法，并重点针对传统CPU能耗模型对虚拟化技术考虑不足等问题，提出基于幂函数的虚拟机能耗模型；为了实现面向云环境下多种类型负载（CPU密集型、内存密集型、IO密集型）的虚拟机能耗自适应建模，提出基于负载类型的自适应能耗模型。在提出与建立的能耗模型基础上，提出面向云环境的基于模型估算的能耗测算方法，并研发分布式能耗测算系统；提出面向多资源的云计算能耗仿真方法，通过扩展CloudSim类库实现CPU、内存、IO等多资源的能耗仿真API；并研究面向云计算和大数据应用的能耗优化算法与技术。本项目的研究不仅对云计算能耗模型发展有较好推动作用，而且对云数据中心的节能技术有一定应用价值。
云模式下多粒度计算服务的自适应调度策略与机制研究	云计算;自适应调度;多粒度;计算服务	云计算作为一种新型的网络服务和应用模式，具有可扩展、虚拟性、通用性及廉价等特点，是下一代网络计算模式的发展方向，具有广泛的应用前景。云环境下，各种应用以多粒度的形式对外提供服务，聚合可伸缩的多粒度异构资源，面对用户需求，如何灵活地构建计算环境，自适应地感知用户数据并完成服务调度，成为云计算需要解决的关键技术问题。本项目紧密结合云模式下多粒度计算的特点与需求，重点研究云计算模式下多粒度计算服务集的自适应调度策略与实现机制，包括云计算服务/资源的多粒度表示与管理模型、粒度敏感的云服务自适应调度策略及其性能评测。在构建的适度规模云计算系统中部署仿真服务和自适应调度器，相关算法和实现机制最终在仿真实验平台中进行验证、评价和完善。
广域分布式文件系统中的一致性服务研究	广域分布式文件系统;一致性模型;一致性服务	分布式文件系统是一种非常重要的应用，是云计算系统的基础设施，其中的一致性服务是保证用户应用正确性和至关文件系统性能的重要组成部分。本项目的目标是为广域分布式文件系统设计一个灵活且高效的一致性服务。1)健壮性：它能适应广域分布式环境，在动态、不可靠的环境中也能够持续地提供正确的服务；2）灵活性：将提供清晰的语义模型，不仅方便用户描述自己所期望的一致性要求，而且，方便系统实现者快捷、正确地向用户提供所需的一致性服务；3）高效性：在保证用户一致性要求的前提下，提高相关操作的执行效率，改善用户体验。要实现这个目标，我们会从理论和实践两方面着手，一方面，从理论上建立和分析模型，运用量化的方法进行验证；另一方面，除了通过可控环境内的模拟，还要在因特网上进行测试。此外，本项目所设计出的理论模型和验证机制是通用的，也适用于其他分布式文件系统，对实际工程具有很重要的现实意义。
近阈值电压高速缓存的可靠性技术研究	高速缓存;计算机体系结构;近阈值电压;可靠性;容错	近阈值电压技术是解决后摩尔定律时代"功耗墙"问题的有效手段之一，然而电压的降低使得高速缓存的可靠性和成品率面临严峻挑战。现有容错方案追求可靠性的同时不但牺牲高速缓存容量、增加访问延迟，而且缺乏灵活性、无法适应程序行为的动态变化。针对上述问题，本课题深入探索如下三方面的内容：针对硬错误研究逻辑高相联度高速缓存重构技术，保证可靠性的同时弥补容错机制牺牲容量所带来的性能损失；针对软错误研究基于数据存储特征的校验技术，设计分级校验保证可靠性并通过消除冗余减少容错的时空开销；针对单一容错级别无法满足不同应用程序可靠性需求的问题，研究软件定义容错能力的高速缓存结构，根据程序行为预测可靠性需求，设计支持多容错级别的高速缓存结构，提供灵活、动态的容错支持。本课题将极大改善近阈值电压技术面临的可靠性问题，并减少容错带来的性能损失。课题预期在国内外期刊和国际会议上发表6-8篇高水平论文，并申请2-3项专利。
异构能源供电的数据中心高效率能耗管理研究	供电配电架构;能量存储;数据中心;能耗管理;绿色计算	云计算与大数据技术的广泛应用突显了数据中心日益严重的能耗与碳排放等问题，使得数据中心能耗管理成为了计算机体系结构领域的研究热点。目前一些新兴的研究利用可再生能源、能量存储设备等能量源来解决数据中心碳排放与能耗问题。但是，已有的技术大多是基于单一类型的能量源，其能耗管理存在一定的局限性。随着分布式发电与智能电网技术的快速发展，通过统筹管理多种能量源像可再生能源、能量存储设备、市电、后备电源等组成异构能源来驱动数据中心具有进一步改善其能效、降低碳排放与成本的潜力。因此，本课题以多路异构能源驱动的数据中心为背景，研究不同种能量源的特性与容量配置方案；设计一种动态自适应的能量存储控制方法；提出可细粒度操控的异构能源电力体系架构；通过对多路异构能源建模，进一步提出一种动态、跨层的异构能源数据中心能耗管理算法。本课题对于设计环境友好、高效节能的新型绿色数据中心具有重要的意义。
并行计算机的结构级功耗优化技术研究	并行计算机;功耗优化;自适应;Cache;动态电压频率缩放	在计算机系统设计中，功耗是与性能同等重要的首要设计约束。并行计算机以追求高性能为目标，但同时也增大了芯片面积和硬件单元的互连开销，进而带来了功耗的增加。在体系结构级研究并行计算机系统的功耗，对设计拥有自主知识产权的面向特定应用的高性能低功耗计算机系统有着重要的支撑作用。.本项目面向并行计算机系统，以典型的并行计算机体系结构和工作负载为研究对象，强调以功耗阈值驱动性能优化的设计方法，建立有效的并行计算机系统的功耗分析模型。根据并行程序的工作特征，利用预测技术和可重构技术设计低功耗的片上Cache，利用分割技术和自适应技术设计系统主存。同时还结合电路技术的特征，设计一种稳定的、分布式的可调控动态电压频率缩放算法，实现整个并行计算机系统的功耗优化。
基于在线机器学习的超级计算机主动容错技术研究	在线学习;高性能计算;主动容错;故障预测	超级计算机正由当前的P级计算向E级计算迈进，专家预计E级计算系统的平均无故障时间仅有几十分钟，采用传统的被动容错方法因容错开销太大，将无法满足未来E级计算系统可用性的需求。主动容错利用故障预测技术提前对可能的故障进行处理，是提高系统可用性的重要途径。针对未来超级计算机系统面临的可靠性问题，本项目提出主被动容错相结合的容错策略，故障预测是该容错策略的关键。通过对各结点状态的实时获取与在线挖掘，获取各种故障的发生规律，然后利用学习的结果对系统故障进行预测，并对即将发生的故障实施低开销的主动容错，从而提高超级计算机的可用性。主要研究内容包括：故障在线学习与预测模型、系统状态数据的获取与预处理、故障在线学习方法、故障实时预测策略、故障规则获取技术、主动容错方法等。项目研究的目标是提高超级计算机的故障在线预测能力，降低系统容错开销，保证大规模并行应用的高效持续运行。
多维学习策略下数字集成电路高分辨率诊断模型研究	物理失效分析;诊断分辨率;故障诊断;机器学习	诊断是数字集成电路设计优化、制造工艺调整、封装测试改良的重要依据。提高诊断分辨率是绝大多数残次电路准确探寻故障缘由的唯一可行途径，也是提高随后PFA步骤成功几率的基础，具有重要现实意义。拟针对目前诊断分辨率欠佳的现状，结合现代工业中集成电路测试及诊断实际需要，深入开展高分辨率诊断模型构建研究。本项目在不同阶段和条件下，灵活运用包括监督学习、无监督学习、半监督学习、主动学习、集成学习等在内的多维机器学习策略建模。拟构建数字集成电路测试进程结束时机判定模型，在兼顾诊断准确率的情况下降低诊断结果集合的规模；构建真伪辨识模型，以逻辑及物理邻域信息为依据，为从诊断结果集合中剔伪存真提供理论指导；依据PFA报告及实际可用的PFA资源，校正上述模型，为研究成果贴近实际工业需求及应用奠定基础。本项目将为促进集成电路相关产业的发展提供理论和技术支持。
基于应用长时空域运行特征的图数据存储组织及系统优化研究	长时空域应用特征;基于收益评价的IO优化;动态图数据组织	图数据具有一般性表示能力，被广泛应用到交通物流、金融、生物信息、社交网络、电子商务、以及公共安全等众多重要领域。随着移动互联网、智能终端和社交媒体等技术的快速普及应用，图数据亦呈现爆炸式增长。而图数据的关系复杂性和大规模的数据量，给图数据的存储管理和计算分析带来重大技术挑战。目前图处理系统静态单一的内外存数据组织方式、低效的数据访问模式，无法高效地支持种类多样、特征各异的图算法应用，也进一步导致了大规模图处理集群系统能效不高。针对大规模图数据的高效组织，如存储、索引、更新、查找等，已成为急待解决的问题。本项目针对图数据内外存组织形态与图应用运行特征失配的问题，提出基于应用长时空域运行特征的图数据动态内存组织和基于收益评价的图数据外存组织模式，降低计算过程中数据访问开销，支持构建高效、绿色的大规模图处理系统。
高可靠实时系统的计算平台(SoPC)研究	系统可靠性;SoPC）;on;Chip;实时系统编程语言;嵌入式实时系统;片上可编程系统（System;Programmable	通过设计Java处理器并扩展应用程序接口，运用软硬件协同方式设计智能模块，并研究Java处理器和智能模块之间的协同工作，构建一种可支持高级并发语言Java的智能SoPC（片上可编程系统），为具有高可靠性要求的实时系统提供计算平台。重点研究支持Java语言编程后计算平台的运行效率、实时性能，以及消除数据相关并采用硬件加速后对优化算法（QPSO）性能、全局收敛性、局部收敛性的影响。研究结果将有利于揭示Java技术用于实时及嵌入式系统开发的可行性，以及实时系统智能化的可行途径。实时系统广泛用于汽车电子、电力、冶金、航空航天等可靠性要求极高的领域。在实时系统日趋复杂、软件危机持续凸现的情况下，本课题利用Java语言的面向对象、语言级并发支持，智能模块的演化能力等特点，从提高软件开发层次、降低系统复杂度等方面提高系统可靠性的思路可能对实时（嵌入式）系统计算平台的更新换代具有普遍的适用意义。
基于众核体系结构的并行算法方法研究	非规则;并行性;自适应;局部性;众核	随着计算机体系结构向多核和众核方向发展，并行计算扩展到更广泛的应用领域。一方面众核芯片上存储和通信资源竞争、存储墙（片外存储访问带宽和延迟）和体系结构多样性使得现有的并行算法在新的众核体系结构很难获得高性能；另一方面许多新兴并行应用问题存在大量非规则性计算行为，从而如何设计并行算法有效利用众核上丰富的计算资源成为迫切需要解决的问题。本项目拟从三个方面探索基于众核的并行算法设计方法：1）针对并行程序中的资源竞争问题，考虑挖掘多级并行的策略，设计有效的资源受限并行任务调度算法；2）针对存储墙问题，利用众核上大量的处理单元，研究并行任务划分和映射算法以提高并行算法的局部性；3）针对众核体系结构多样性的特点，研究算法和体系结构关系参数化描述，以实现体系结构自适应并行算法和性能可移植性。通过该项目的研究，将为设计众核上有效的并行算法提供系统可行的方法，促进众核体系结构和高性能计算应用的发展。
抵抗硬拷贝攻击的彩色图像数字水印的理论与方法	矢量半调;彩色图像;数字水印;硬拷贝攻击	目前，抵抗硬拷贝攻击的彩色图像数字水印方面的研究较少。现有方法主要是通过把彩色图像变换为一幅或多幅彩色分量灰度图像，将灰度水印方法进行扩展来实现。这些方法没有考虑色彩的相关性和色彩的一致再现问题。本项目主要突出彩色矢量的抵抗硬拷贝攻击的数字图像水印理论与方法。与以往不同，将彩色图像的各个颜色分量作为矢量，将数字水印与数字半调、打印和扫描作为一个整体进行研究，实现真实环境下的抵抗硬拷贝攻击。项目通过分析打印扫描过程对彩色图像和水印的影响，建立打印扫描自动校正系统；基于凸优化理论和最新的误差分散矢量半调算法、最小亮度标准的矢量误差分散法设计最优的矢量半调搜索策略，实现水印的间接嵌入；设计连续色调图像水印算法，基于颜色矢量，保持颜色的一致再现；通过校正和逆半调算法设计，实现水印的正确检测；项目对于彩色图像硬拷贝输出防伪具有重要意义。
面向云存储的多源数据安全查询机制和算法研究	完整性保护;云存储;关键字查询;半诚实但好奇;隐私保护	本项目面向"半诚实但好奇"的云存储环境，依据多源数据应用场景的需求，研究基于隐私和完整性保护的多源数据安全查询机制，并提出相关模型和算法。结合云端服务器存在"好奇性"的特点，研究多源数据的隐私保护问题，提出基于非对称标量积加密函数模型的隐私保护方案，保证云端服务器的数据安全且支持多源关键字查询；针对"半诚实"云端服务器返回不完整查询结果的问题，提出基于Merkle哈希树的完整性保护方案，检测云端服务器的自私行为；针对云端服务器处理海量数据时的查询效率问题，提出基于链式布鲁姆过滤器二叉树查询结构，避免现有查询方案中关键字枚举对性能的影响，有效提升云端服务器的处理能力。最后形成支持隐私和完整性保护的安全查询机制。项目将以Amazon EC2云计算平台中的实际数据对提出的算法和机制进行评测。该课题的研究，对提高云存储数据查询的安全有效性，推动云计算的应用具有重要意义。
干旱条件下树木生长发育建模与虚拟仿真	虚拟树木模型;快速绘制;生命过程可视化;图形处理器	本项目以世界上分布最广、适应性最强的树种- - 杨树作为对象，依据实测数据，基于植物的生理生态学原理，探寻干旱气候条件下杨树的生长和发育规律，利用计算机模拟和可视化技术建立杨树的参数化静态模型，并构造反映特殊气候条件下生长与栽培条件相互制约的虚拟树木模型，定量、直观地展示树木生长发育的生命过程。项目基于生物学原理，建立该虚拟模型对生物量、树高、冠形等数据进行预测的机制。对新型图形处理器在树木模型的快速真实感绘制以及大规模生物学计算方面的应用也是本项目的研究内容之一。.杨树是一种人工经营树种，本项目的研究对年降雨量不同的造林地区不同杨树品系的精确栽培、育林研究具有很强的推动作用，是具有实践意义和代表性的研究课题，对林业生产特别是干旱气候特征明显的我国北方的林业生产具有重要意义。.本项目的研究具有可推广性，可以应用于其它树种以及植物的虚拟建模，以及计算机游戏、计算机动画、室外景观设计中的植物建模。
QoS感知的SSL迁移协议及其优化策略研究	QoS感知;密钥交换算法;SSL协议;连接迁移;负载均衡	电子商务的发展已成为国际竞争的焦点和制高点，对于增强我国综合国力具有十分重要的意义，SSL协议作为电子商务中最重要的安全技术之一，是当前研究的热点。本项目面向客户对于QoS的需求，针对SSL协议造成服务器的负载过重，而导致延迟的响应时间，以及频繁的连接中断等问题，研究QoS感知的SSL迁移协议及其优化策略的基础理论及关键技术。首先提出客户端平衡的密钥交换算法，减轻握手协议中造成客户端和服务器端计算的不平衡性，显著提高SSL握手协议解密时间的性能。其次，提出建立服务器提供的客观的服务质量与客户对服务质量感受之间的关联，并提出相应的QoS感知的优化策略和算法。然后，提出选择性地分阶段部分重放的连接恢复机制，实现SSL连接迁移协议，提供客户端透明的连接恢复容错性保证。最后,面向实际应用环境，设计QoS感知的SSL迁移协议原型系统。本项目研究成果应用到电子商务领域,具有重要的理论意义和应用价值。
超宽向量SIMD处理器中的高效控制机制研究	线程级并行;非规整控制流;向量SIMD;串行处理	向量SIMD处理器为芯片适度功耗下的性能提升,以及海量晶体管资源的高效组织提供了创新思路，蕴藏着巨大的发展潜力。然而，向量SIMD处理器正面临着诸如串行处理，非规整控制流以及线程级并行性开发等挑战，特别是随着向量SIMD宽度逐步进入到"超宽"范畴，上述挑战已经成为亟待解决的关键问题。为此，本课题立足问题产生的本源，拟通过动态解耦机制，运行时分组机制以及同时多宽度SIMD支持等高效控制机制，对关键瓶颈进行各个击破，并将集成阶段性研究成果，设计一种可以协同解决向量SIMD处理器中关键瓶颈问题的多模控制机制。本课题的研究将有助于我国自主芯片设计在超宽向量SIMD处理器正蓬勃发展的大背景下抢占一席之地，具有重要的战略意义和应用价值。
众核处理器上并行稠密矩阵计算关键技术研究	并行性能模型;众核处理器;并行计算模型;稠密矩阵计算	众核处理器已成为高性能计算基本加速构件,其在计算、访存及通信方面的层次化发展对并行算法优化提出了更高挑战。本项目以科学计算的关键核心,稠密矩阵计算关键函数为研究对象,主要研究内容包括:1)众核架构并行计算模型研究。从理论性和正确性两个角度,对在计算、存储和通信多个方面出现层次化发展的众核架构进行建模,以指导矩阵核心函数算法设计;2)并行矩阵计算关键算法设计。基于众核计算模型,从计算和数学两个角度,在计算访存重叠、通信避免、非规则任务分解等方面提出新的适用于众核架构的新算法,提升关键函数性能达到或超过国际水平;3)并行性能模型研究。基于性能模型深入分析矩阵算法在众核架构上的优化方法，形成一族对矩阵计算具有一定通用性的优化序列，并建立其自适应参数调优方法。本项目中，底层硬件并行计算模型、上层程序并行性能模型和矩阵计算算法三位一体紧密结合。
基于硬件的多核程序执行不确定性消除技术研究	多核处理器;一致性协议;不确定性;确定性重演	运行在多核处理器上的不同线程可同时访问共享内存，现有的基于目录的Cache一致性协议虽然能有效防止共享内存访问的不一致性，但是对访问顺序无法控制。因此，多核程序即使输入集相同，也可能产生不同的输出结果。这种多核处理器执行环境的不确定性给多核程序的编写、调试、测试和维护都带来了巨大挑战，也成为多核编程实现广泛应用的瓶颈。本项目拟从多层存储访问模型出发，提出一种新的确定性Cache一致性协议，消除多核程序执行的不确定性。通过分析不确定性给多核系统带来的影响，研究去除不确定性的方法，给出合适的硬件设计取舍，抽象出优化的状态模型；研究高效、快速的确定性重演机制，让确定性重演能够得到更广泛的应用；在这些基础上提出支持多种工作模式的确定性多核原型系统。可以预见，本项目的研究将对确定性多核处理器设计、多核程序可靠性以及多核编程的推广具有非常重要的意义。
MPSoC的片上数据内存结构的软硬件协同设计方法研究	软硬件协同设计;片上内存结构;存储系统;MPSoC	在嵌入式MPSoC系统中，如何设计其片上存储结构和优化使用片上高速存储资源，减少处理器访问片外内存次数，对提高系统性能、降低系统成本和功耗有非常重要的现实意义。本项目以媒体应用为目标，采用软硬件协同设计方法，研究了如何进行基于SPM + Cache混合结构的MPSoC片上数据内存结构设计方法，核心是提出集成多核任务分配和调度、片上内存硬件结构设计、数据SPM和Cache分配这三种技术的联合优化设计方法。通过本项目的研究，可以得到一种面向SPM+Cache混合结构的MPSoC片上内存结构通用设计方法，采用该方法可以使MPSoC的硬件和软件更加匹配，能够减少程序访问数据存储系统的时间，从而能够提高系统整体性能和降低系统功耗。
避免无关依赖的众核线程划分机制	线程划分;隐式同步机制;并行编程模型;数据依赖;众核体系结构	如何将众核结构提供的丰富晶体管资源转化为有效的计算能力，实现主流应用软件可并行性的透明扩展，是当前研究中的热点问题。针对目前众核线程划分机制均以基本块为最小单位而引入无关依赖的现状，本项目提出一种基于依赖分离的细粒度线程划分方法，通过最小化线程间的无关数据依赖，将计算和访存依赖不规则、传统上难以手工和自动并行化的串行程序划分成精简而独立的线程并行执行，解决传统并行编程模型和环境存在的问题。主要研究内容包括：众核结构上基于依赖分离的细粒度线程划分方法、轻量级线程间隐式同步机制的硬件支持、符合线程并行执行特征的动态资源分配机制。本项目的预期目标是提出一种符合众核体系结构特征的线程划分方法,平衡并行编程的易编程性和并行程序的执行效率，改善众核结构研究当前面临的片上资源利用率不高、同步开销过大和负载不平衡等问题。对于探索2013-2020前后高效能通用微处理芯片体系结构的发展道路具有重要意义。
高利用率驱动多核QoS保障关键技术研究	组件模型;公平性;多核架构;资源分配;测量方法	对高质量计算服务的需求，使得多核系统的利用面临着两难的处境：利用多核架构特征并发执行更多任务面临着QoS目标不可控的风险；而降低任务数量提高任务性能又导致了大量资源闲置的问题。本项目所研究的科学问题即，当多核系统处于高负载时如何保障任务QoS目标的问题。先前的研究表明，通过优化资源分配提高任务性能及系统输出可行。基于此，本项目的研究思路如下：1.多核系统在多个任务并发执行时，我们从资源利用的角度研究对其负载能力进行精确测量和表达的方法；2.特别是，在资源被过度使用的情况下对系统负载能力进行衡量，并由此确定系统高负载能力的上限；3.在界定的高负载区间内，研究多维度且分配特征丰富的资源分配方法来达到任务QoS目标可控的理想状态。研究的目的是通过对多核系统可承受高负载区间的精确把握，实现区间内任务QoS的可控，进而为并发任务数量的有效增加和资源的高效利用提供充分的科学依据。
虚拟计算系统中基于负载特征反馈的计算资源分配策略研究	动态自适应;计算资源分配;计算系统虚拟化;负载特征	在虚拟计算系统中，依据负载特征参数和负载水平自适应调整的计算资源分配与调度策略及其关键技术研究对促进计算资源的高效利用、提高虚拟计算系统性能具有重要意义。已有虚拟计算系统中的计算资源分配是在创建虚拟机时确定分配权重策略，不能根据虚拟机的负载特征对权重参数进行动态调整，导致系统运行时的负载水平-计算资源分配的失衡，造成计算资源的浪费和服务质量的下降。本项目拟从典型负载特征的角度，通过对运行时负载的有限进程信息的收集、发掘并建立典型负载识别模型，使之能够在运行时识别出不同负载类型特征和负载水平，进而提出根据典型负载特征和负载水平动态调节计算资源分配权重策略，实现虚拟计算系统的计算资源动态分配和高效利用，提高虚拟计算系统的服务质量。
跨组织协同工作流挖掘的Petri网融合机理与保性控制技术研究	分布式系统建模;模型融合;工作流挖掘;保性分析;Petri网	针对跨组织协同工作流的挖掘问题，本项目主要从理论上研究同类和异类Petri网模型融合机理和融合满足的性质关系以及保性控制器设计，以此为理论基础研究跨组织工作流挖掘问题。着重解决协同模式下的多源异构引擎日志数据集成、支持跨组织协同的工作流形式化模型、组织部门之间的协同模式分类及其挖掘、跨组织协同工作流模型挖掘的正确性验证和评价等系列问题。理论和方法研究成果将以跨部门的突发事件应急联动信息系统为靶子得到实验应用验证。最后开发一个基于Petri网融合的跨组织流程挖掘原型系统改进和完善所提出的理论与方法。通过本项目的研究，从理论上探讨大规模系统建模与分析的Petri网方法，同时为大型分布式信息系统的流程挖掘奠定理论基础。
云环境GPU虚拟化的安全问题研究	GPU虚拟化;虚拟内存;云计算;安全;资源隔离	GPU（图形处理单元）因其在并行计算方面的优势，正广泛应用于高性能计算和云计算领域。GPU虚拟化作为这种趋势的技术支撑，将GPU设备共享给多个用户以提高硬件资源的使用和管理效率，但同时也带来了安全风险。 特别地，云计算环境GPU虚拟化场景中的安全问题包括以下两个方面，第一，在串行调度的GPU虚拟化方法中，恶意任务可以获取其它已完成任务的内存残留状态，导致信息泄露；第二，在并行调度的GPU虚拟化方法中，多个任务非隔离执行，会造成任务间的越权访问。本项目以CPU与GPU异构虚拟内存管理为基础，为GPU虚拟内存管理提供分页机制和虚拟物理地址转换功能，从程序静态分析与优化、异构地址空间隔离、运行时鉴权、页粒度敏感数据清除等角度，结合已有的GPU虚拟化研究成果，提出针对上述安全问题的系统性解决方案，并实现可部署系统，提高云环境中GPU共享的安全性，扩展GPU在通用计算领域的适用性。
大规模计算系统故障的主动检测技术研究	可靠性;失效检测;大规模计算系统;故障的主动检测;高可用	随着系统规模扩大，高性能计算系统可靠性问题日趋突出：频发的故障造成系统利用率和并行程序完成率降低、系统维护成本增加。当前，解决该问题的主要方法为：以部件冗余、失效检测及失效后的修复机制为基础，以提供不间断服务的高可用技术。而本研究引入预警机制，及时发现并定位系统中的隐患，并在系统崩溃之前进行主动的维护，从而避免无计划停机带来的巨大开销，相对于高可用技术的失效后被动修复方式，本研究将该方法称为主动方式的故障检测机制。故障主动检测过程可分三步：1）监控系统运行状态获得系统特征信号；2）从特征信号中提取与故障敏感的征兆；3）根据征兆和已经建立的映射关系对系统的实际运行状态进行评价。相对于高可用系统中的"失效检测"，本研究是在"失效"发生之前，提供故障的快速准确发现机制，特别是在大规模计算系统中引入故障的主动检测机制，能够及时发现故障隐患并提高大规模计算系统的可靠性、降低系统的维护开销。
基于用户情境感知的低能耗移动数据访问机制及技术	低能耗;移动数据访问;设备小型化;用户情境感知;普适计算	随着计算普适化和网络泛在化技术的发展，可以跨平台、随时随地进行的移动数据访问已经日益得到人们的重视。然而，对于智能移动设备，尤其是如手机等靠电池供电的手持设备而言，由于移动数据访问还要涉及到网络通信开销的能耗，因此如何降低其能耗更是非常重要。本项研究将围绕"降低移动数据访问能耗的机制和方法"，以及"能耗、性能与用户满意度之间的关系"这两个问题进行研究，探索新的移动数据访问策略，降低系统能耗，使得用户在移动数据访问时不影响甚至延长电池的使用寿命，同时提高系统的响应性能和用户的使用意愿。为此，本研究在已有研究的基础上，将"用户情境"引入移动数据访问的低能耗机制，即根据对用户行为的分析、判断和预测来动态调整系统的移动数据访问策略，以最大限度的降低能耗，并提高响应速度。同时，为了研究能耗、性能和用户满足度之间的关系，将利用经济学中的效用理论，通过构造合适的效用函数来表征用户使用时的满意度和认可度。
多核微处理器体系结构级容软错误设计与评估关键技术研究	软错误;容软错误设计;体系结构;可靠性评估;多核微处理器	本课题围绕多核微处理器体系结构级容软错误设计与评估展开研究，探索利用多核微处理器自身的丰富资源实现容软错误的可行性，通过体系结构进步带来的优势合理高效地应对制造工艺进步带来的软错误挑战。本课题将着力解决多核微处理器体系结构级容软设计中如何合理利用和如何量化评估两大关键科学问题。合理利用是指本课题将研究如何合理利用多核微处理器中丰富的资源，在少增加甚至不增加面积、性能、功耗开销的前提下，通过体系结构级设计有效地提高微处理器的可靠性。本课题将探索可重构的三核冗余执行模型来利用冗余的内核资源，探索硬件IO srcub技术来利用空闲的时间资源，探索基于扫描链的复位技术来利用已有的硬件结构，从而实现体系结构级容软错误设计。量化评估是指本课题将研究建立包括面积、性能、功耗维度在内的微处理器可靠性量化评估指标和框架，以便对微处理器容软错误能力进行更加准确的量化，从而有效地指导设计折中与设计选择。
云计算环境中租户数据的计算安全保障机制研究	数据流跟踪;软件安全更新;云安全;信息流无干扰;可信执行环境	云计算在带来易用和低成本特性的同时也提出了新的安全挑战，云计算基础设施公有化和多租户资源共享所引起的租户安全边界模糊化与租户对云端自身数据的安全可控需求之间存在矛盾。本课题旨在解决云平台中租户数据的执行环境和处理流程的安全性问题，执行环境涉及虚拟化架构的可信性以及云服务软件的安全性，传统可信计算技术无法满足云平台的动态可信需求，现有软件补丁方法不能应对虚拟机迁移和回滚等动态特征带来的软件更新挑战，传统监控手段无法跟踪和控制租户数据在云端的处理流程。拟开展研究：1）租户数据流安全控制理论：对租户数据在云端的处理进行建模和安全约束；2）虚拟域可信保障机制：解决租户数据执行环境的动态可信问题，并为云服务提供可信支撑；3）云服务软件安全更新机制：为云服务软件的动态更新提供审核、安全性测试及风险评估；4）租户数据流跟踪和控制：提供面向租户数据流的按需跟踪和自主控制。项目成果将用于提高云计算的安全性。
IaaS云环境下大规模科学工作流优化执行方法研究	科学工作流集合;科学工作流;工作流调度;IaaS云环境	针对IaaS云环境下科学工作流高效执行面临的资源供给与任务调度效率不高等问题，开展面向大规模计算密集型科学工作流、大规模科学工作流集合以及大规模数据密集型科学工作流这三类典型科学应用的多目标优化调度算法的研究工作。一是研究利用列表策略和确定性导向搜索技术实现10^3以上级别科学工作流的帕累托前沿高效快速算法；二是研究利用多工作流并发启动、工作流准入控制和任务回填技术，实现基于可伸缩异构虚拟机集群的满足截止期和费用约束的科学工作流集合高效调度算法；三是利用多层超图剖分技术实现多云环境下基于分割子工作流的数据密集型科学工作流的帕累托前沿高效算法。依托国防科大银河虚拟云环境和广州超算中心天河二号云环境，采用科学工作流标准案例和团队自主研制的全球中期数值天气预报科学工作流系统对上述算法进行全面测试验证。本项目若研究成功，将为提高科学家利用IaaS云环境进行大规模科学工作流实验的能力做出积极贡献。
基于FPGA的实时动态可重构系统关键技术研究	动态可重构;任务布线;嵌入式系统;去重压缩;任务调度与布局	针对动态可重构系统在实时应用中面临的任务布局成功率低、逻辑块布线复杂、配置文件加载时间长的挑战，本项目围绕基于FPGA的实时动态可重构技术展开研究，探索任务调度与布局、逻辑资源布线和配置的相关理论和关键技术。根据任务的客观形状，拟构建一种新型不规则任务描述模型，研究相应的任务调度和布局算法，提高可重构任务调度效率和布局成功率；基于不规则任务模型，采用图论相关知识，研究逻辑资源块布线策略，减少布线时间和关键路径长度；根据二进制配置文件帧内与帧间的重复特征，采用配置文件去重预处理和压缩处理相结合的方法，解决因配置文件大而造成配置时间长和空间开销大的难题；将研究成果集成到开源FPGA开发环境中，实现实时动态可重构系统验证平台。开展本项目的研究有利于拓展动态可重构计算技术在国防、工业4.0和个性化数字生活中的应用，对促进我国FPGA技术的发展和提升高端嵌入式产品的国际竞争力具有重要意义。
基于网格的多源异构数据访问与集成方法研究	元数据管理;数据网格;共享与集成;访问代理	面对信息爆炸时代的海量数据，传统的数据管理方式已不能满足数据处理的高性能需求，数据网格为现今的海量数据管理带来了新的解决思路与方法，如何便捷、高效的访问和集成各种异构、异源数据，是网格环境下使用数据的关键。本项目旨在网格技术基础上将访问代理与中间件相结合，在网格数据源上增设访问代理，根据异构数据源的各种特点采用不同的数据访问方式，屏蔽多源异构特性，进行数据的访问和集成。研究支持多种数据形式的网格元数据模型，研究数据转换与映射机制，研究异构网格数据资源集成方法，实现多源数据的共享和访问。该方法面向各种信息系统、数据库和文件系统，与系统自身机制相结合，克服数据直接访问方式的不足，确保数据的安全性和保密性，利用系统提供的接口进行数据的高效、协同访问，实现数据资源的广泛共享。该方法将对科学数据共享问题的研究产生重大影响，对解决目前信息化建设中"信息孤岛"和"数据鸿沟"等问题起到积极推动作用。
面向类人机器人动作规划的参数最优控制技术研究	自动细胞机;类人机器人;可重构计算;参数最优控制	类人机器人的动作规划如倒地控制等是机器人设计与应用中需要研究和解决的重要问题。项目旨在建立面向机器人动作规划的参数最优化可重构计算体系结构，提高机器人动作规划的参数最优化效率。具体内容包括，首先，建立类人机器人动作规划的参数最优化定点模型，研究参数最优化的算法特征；其次，针对模型内的算法瓶颈进行分析和优化，使其能采用可重构计算方法进行加速；再次，建立参数最优化可重构计算体系结构，对模型中的复杂算法做进一步优化，设计粗粒度自动细胞机结构和细粒度可重构加速部件，以提高处理效率。最后，构建面向类人机器人动作规划的性能测试平台。利用Nao型类人机器人和基于FPGA硬件平台的动作规划可重构体系结构，研究定点模型的真实性能，克服直接软件仿真和现场实测的缺陷。本项目建立的计算体系结构将更好地应用于机器人动作规划实践中，并促进类人机器人动作规划相关理论的发展。
基于跨层机制的嵌入式虚拟化技术优化方法研究	嵌入式系统软件;嵌入式虚拟化;异构多核调度;设备资源分配;I/O调度优化	嵌入式虚拟化技术能够降低开发成本、提升硬件利用率和安全性，具有广阔的应用前景，但由于通用虚拟化框架缺少针对嵌入式系统资源受限环境的设计，导致在实时性、能效、性能隔离等方面存在不足。而且虚拟化软件层次结构复杂，各层设计相对独立，使得单一层次的优化难以有效解决问题。本项目旨在利用跨层机制取得突破，打破原有的软件层次隔离，为调度决策提供更完整的上下文信息和更精确的控制接口，从而实现多层协同的高效调度机制，在不同层面对嵌入式虚拟化技术进行优化。重点研究以下内容：利用应用层的响应时间信息优化虚拟机调度层的嵌入式异构多核处理器调度以提高能效；利用I/O软件栈各层信息和控制接口进行请求的调度优化以提高实时性；利用可编程硬件在设备控制层实现调度层的资源分配策略以提高隔离性。本项目的研究成果能够提升嵌入式虚拟化技术的性能和可用性，推动其产业应用，并能促进虚拟化技术在其他领域的应用优化。
面向云计算的自含式数据安全控制理论与方法	云计算;数据安全;访问控制;基于属性数据加密方法;自含式安全控制	数据安全被认为是云计算中用户的首要安全目标。由于数据脱离用户的控制域，使得数据的安全保护变得更加困难，目前还缺乏有效的数据保护方法。本项目针对云计算中数据保护的特殊问题，提出以基于属性的加密算法ABE为基础，将ABE与访问控制理论有机融合的方法，研究数据自含式安全控制理论与技术。在理论层面上，通过建立广义访问策略树，研究数据加密与数据安全控制的混合算法；结合典型模型研究以数据对象为中心的访问控制模型定义方法；引入SAT理论，研究策略一致性合成算法。在技术层面上，引入数据起源理论与网络层数据包起源管理方法，研究数据读写留痕技术；提出数据对象安全控制容器概念，研究实用的自含式数据对象安全控制机制的实现方法。本项目最终将建立一种以数据为中心的新的数据保护理论体系，使数据自身具有机密性与完整性的保护能力。项目的研究成果，将开创云计算数据安全研究的新途径，为丰富云计算中数据的服务功能提供必要保障。
面向数十万核以上异构众核平台的CFD领域性能建模与应用	计算流体动力学;性能模型;大规模数值模拟;数十万核计算机;并行计算	数十万核的异构众核计算机正在成为高性能计算领域的主流硬件平台，这为大规模CFD数值模拟提供了前所未有的良好机遇，然而大规模CFD并行模拟向数十万核并行平台上的移植还面临着平台与应用相融合的性能建模、异构体系结构下的负载平衡、CFD应用多层次并行性开发、模板计算访存性能优化等一系列挑战性问题。本项目拟以数十万处理器核心的高性能计算机为目标平台，以多区三维可压流场的CFD并行模拟为典型案例，以面向目标平台的CFD领域应用性能建模为主线，重点研究：构建典型高性能计算平台的性能分析模型，开发CFD应用问题中的多层次并行性，优化CFD典型求解计算过程的数据局部性和访存特性、提高计算密度，建立高效模拟的任务映射与负载均衡策略；以期突破若干关键算法，推动CFD应用向更大规模、更高效率、更高精度方面发展，为超大规模CFD工程应用提供新的技术途径。
基于网络编码的多云存储若干关键问题研究	网络编码;云存储;存储容错	随着科学界和工商界逐渐采用多类型和多品牌的云来部署计算和数据平台，多云或将成为下一代云计算模式，而多云存储正是其中重要的一环。由于多云存储节点之间传输数据需要缴纳大量费用，从而导致多云存储在应用推广上存在一定的障碍。但是，目前仍然缺乏有效机制去降低多云存储在系统操作中所耗费的带宽开销。本项目拟以多云存储系统为出发点，以网络流图和乘积矩阵为主要建模分析方法，以随机线性网络编码为主要技术手段，针对多云存储的几个关键问题进行研究，内容包括：(1) 设计并实现一个利用云计算资源进行网络编码的去代理化多云存储容错架构，并在该架构下设计实现最优的容错修复方案；(2) 设计并实现基于网络编码的最优的多云扩展算法；(3) 设计并实现基于网络编码的高效容错转换机制。该研究主要目标是降低多云存储在"数据修复、数据扩展、容错转换"所产生的带宽开销以降低运营成本，从而促进多云存储应用和普及。
网格环境下基于数据交换区的作业分配与再调度	分布文件系统;数据复制;网格;性能	网格计算能够高效的完成各种大规模的科学计算以及仿真模拟。随着大规模计算精度的提高，其产生的数据量也呈指数级不断增加，网格环境下受到网络和存储设备的限制，数据的访问速率远远低于处理器的处理速率，成为系统的瓶颈。本项目将提出一种新的作业执行辅助设施：网格数据交换区GSwap，用来为作业的数据交换提供一个高效可靠的场所，并统一管理与作业相关的文件，改进作业执行中数据交换的模式。提供一个GSwap文件的访问代理，依据作业的类型选择数据访问模式以及数据分布，为作业访问GSwap提供一个统一的、透明的访问接口；提供GSwap节点的异常检测与处理，提高其文件访问服务的可靠性。基于GSwap作业执行中间结果实现检查点算法，缩小检查点算法和再调度算法的开销。在动态网格环境下为调度决策系统提供调度依据，提高作业对数据的访问效率，使得网格资源的利用更加合理，保证任务完成时间门限值，改进应用程序的执行性能。
带有可重配置特性的高性能无线局域网物理层与数据链路层硬件体系结构	物理层;体系结构;数据链路层;可重配置;无线局域网	无线局域网（WLAN）技术已经深入到生活的每个角落。越来越高的数据传输率/能源效率、快速的标准演化等趋势对物理层和数据链路层硬件提出了更高的要求。目前国内外工业界和学术界还在不断为满足这些要求而努力。我们拟提出创新的带有可重配置特性的高性能WLAN物理层和数据链路层硬件体系结构，可以在WLAN标准快速演进的过程中，在合理功耗下实现高性能，并具有三个突出优点：实现快速的特性响应时间和更长的生命周期、大幅度减少开发周期并降低开发成本、对WLAN层次协议栈跨层优化提供有效的支持。本项目考虑标准分析与算法结构改进、硬件体系结构设计、高可编程性与兼容性、准确验证与原型系统的实现四个方面的内容，不仅得到创新、严谨、有效的体系结构设计及前端面积/功耗数据，还会针对当前及下一代WLAN标准提供开放易用的FPGA高性能原型系统，在科学研究、国家产业、国产CPU支持等方面均有较好的应用前景。
基于软硬件混合监测机制的多核环境下内存系统性能分析与优化技术研究	多核环境;调度与预取;软硬件混合监测;I/O访存;内存系统	计算机内存系统是影响体系结构、系统软件和应用软件效能的最重要因素之一。由于处理器与内存之间的性能差距越大（即"Memory Wall"问题），内存系统已成为制约系统性能的瓶颈，而多核技术的发展使"内存墙"问题更严重。本课题将设计与实现一种新颖的软硬件混合监测机制来研究多核环境下内存系统的性能分析与优化技术，主要研究内容包括：（1）研究一种软硬件结合的监测技术以弥补传统硬件监测机制存在的Trace语义鸿沟问题（Semantic Gap），有效地增强计算机内存系统的性能监测与分析手段；（2）将上述混合监测机制应用到多核平台，研究多核环境下内存系统的延迟与带宽问题，并与操作系统、运行时环境以及编译等结合研究提出适合多核的调度、预取等优化技术；（3）通过研究I/O系统访存特征提出优化技术，提高I/O系统的访存性能。
面向高性能混合计算的计算型存储器体系结构研究	计算机系统结构;eDRAM;混合计算;计算型存储器;处理器	近年来，混合计算结构逐步成为未来高性能计算机系统体系结构发展的重要趋势之一，同时，随着集成电路和多核处理器体系结构技术的发展，高性能处理器系统的"存储墙"问题、效能问题、编程模型问题、可扩展性问题等也逐步成为其研究热点。本课题面向高性能混合计算，基于PIM技术、多核技术和集群技术，提出一种新的计算型存储器体系结构，并深入研究其核心关键科学技术问题，主要包括：基于计算型存储器的高性能混合计算模型及系统结构；结构简单、计算资源利用率高、计算能力强、可扩展性好的同步数据触发高性能计算内核计算模型及其体系结构；基于主动消息的计算型存储器通信模型；面向高性能混合计算的eDRAM体系结构；计算型存储器片上轻量级集群节点的并行编程模型，以及计算型存储器相关的设计、实现和评估关键技术。该课题研究将为设计实现单芯片千亿次至万亿次高性能计算型存储器（处理器）奠定坚实的理论和技术基础，具有重大的理论和现实意义。
科学大数据处理优化理论与关键技术研究	数据密集型计算;运行时;科学大数据;编程模型	在大数据与第四范式的时代背景下，传统的数据密集型计算面临着新的机遇与挑战。科学领域的数据具有非增量式特征，数据的处理具有多迭代、频域计算多等特点，传统高性能计算中以内存共享与消息通信为中心的数据处理方式已不能应对大数据特征的科学数据处理，高性能计算机I/O系统边缘化设计现状使得这一问题尤为突出。本课题围绕高性能计算机上科学大数据处理的I/O瓶颈与可靠性问题，对其展开深入研究。课题围绕三个方面开展：（1）研究高性能计算机架构下的数据密集型编程模型，包括数据并行规则、可靠性保证等；（2）研究数据访问目的驱动的在线I/O性能优化方法，包括数据处理在线Profiling、内存复用机制、增量处理等；（3）研究结构感知的科学大数据处理数据分割与数据调度算法，包括科学大数据的静态分割与数据动态调度。本课题的成功研究将有助于促进高性能计算技术的发展，拓宽高性能计算的应用领域。
基于流模型的面向应用可配置DSP关键技术研究	面向应用;流模型;DSP;可配置	数字信号处理器（DSP）已经成为宽带接入、数字音频和数字视频、新一代无线通信等应用的核心平台。随着人们的需求不断提高，应用对DSP提出越来越高的要求，并且呈分化趋势。面对需求各异的种种数字信号处理应用，当前的DSP主要存在通用性、高性能、功耗效率、适应性的矛盾。能够面向应用量身定制DSP，是一个迫切需要解决的DSP体系结构问题。本课题提出可配置的流数字信号处理器- - 在流模型的框架下，根据应用的流化特征量身定做处理器，使得处理器在统一的体系结构框架下，针对应用需求配置硬件，满足性能、功耗效率、通用性的指标。本研究首次提出"面向应用"的流体系结构设计思想，在流体系结构和DSP结合的地方，引入可配置处理,是一种平衡速度、功耗、可编程性之间矛盾的创新方法，适应微处理器和DSP体系结构发展，有利于简化设计、降低成本、缩短周期，具有原创意义，对发展我国微处理器和DSP的研发将会产生积极影响。
片上网络高性能互连技术的研究	片上网络;路由算法;拓扑结构	片上网络从体系结构上解决了现有总线结构芯片设计所面临的能耗，可扩展性等诸多问题，是下一代高性能芯片设计的发展方向。本项目重点对片上网络互连技术中的拓扑结构和路由算法进行研究，通过引入包括并行计算机互连网络、因特网、社会学科、生物学科等其他学科知识来解决片上网络路由算法中容错、流量均衡以及服务质量保证等问题。构建具有小世界性质及成簇结构的新型网络拓扑是本项目另一个创新点，该类网络具有网络直径小，对分带宽大，可扩展性好以及本地业务保证等优良特性。对现有路由算法具体研究也各具特色，包括模拟气体扩散原理来解决片上网络路由算法中的流量均衡问题，利用新方法放宽现有算法对故障区域形状限制，将因特网中的服务质量路由算法应用到片上网络中等。最后，在各个分项研究的基础上，提出一种综合的分布式路由算法，全面解决均衡、容错以及服务质量保证问题。
面向通用GPU虚拟化多任务的三维堆叠存储架构研究	虚拟化;存储系统;通用图形处理器;三维堆叠集成;多任务处理	近年来，基于GPU处理器的通用GPU计算平台以其成本和能效优势为未来的高性能计算提供了新的方法和手段。但GPU本身架构和运行框架的限制使其只能以加速器的角色出现，而无法在以并行多任务处理为基本特征的通用计算环境中直接地发挥作用，从而制约了通用GPU计算的广泛应用。因此，本课题提出对现有GPU架构的拓展和运行框架的优化来实现计算、存储和互连资源的虚拟共享，进而支持GPU处理器片上虚拟化多任务并行处理的目标。针对这一目标，本课题以存储架构为切入点，利用最新的三维堆叠存储器集成工艺所提供的优势，应对现有GPU二维存储设计所面临的存储容量、通信带宽以及延时的诸多挑战。在此基础上，进一步探索和研究符合通用GPU计算虚拟化多任务需求的存储架构设计方法和资源动态管理机制，并借助运行框架多任务处理的拓展，实现更细粒度、更为经济的存储虚拟化方式，为未来的高性能计算及GPU虚拟化提供硬件架构和运行框架的支持。
面向高性能嵌入式系统的软件并行化和数据分布优化	多核嵌入式系统;并行处理;数据分布;调度算法;代码转换	高性能嵌入式系统正在迅速地往多核化并行体系结构发展，而软件并行化的发展始终滞后，并行硬件资源的作用无法得到充分发挥。这是我们面临的第一大挑战。本项目的第一个目标是研究高效的代码优化技术，实现软件的高度并行化。我们面临的第二大挑战是如何克服内存瓶颈的问题。内存访问和处理器之间的速度差异在多核体系结构下显得更为严重。纯硬件或者纯软件的方法都无法完全解决这个难题。本项目将考虑多种多核体系结构，研究数据在多个或多级内存上的优化分布，从而使数据访问的时间和能耗最小化。这是本项目的第二个研究目标。嵌入式系统的特性是可以针对应用的特征和性能要求调整软硬件的配置。所以本项目将以前面两项研究为基础，研究如何在巨大的设计空间上快速找到最佳软硬件配置的方法。本项目的特色及创新性在于：1）与实际应用相结合的基础研究；2）在各种不同的体系结构下研究最优解的算法及其复杂度分析；3）对软件并行度和数据分布的综合研究。
面向艾级计算机系统的大规模光电混合互连网络研究	高性能互连网络;艾级计算机;光电混合网络	艾级计算机系统中，过高的网络直径和仅面向局部通信优化的网络结构，导致大规模直接网络面临全局通信性能大幅缩减问题。为提高网络的全局通信能力，本项目提出在高性能互连网络领域开展光电混合网络结构的研究。与面向数据中心胖树拓扑的光电混合网络研究不同，本项目将支持低延迟和高弹性的新兴光交换技术引入混合直接网络设计，以满足高性能计算对低延迟通信的需求。针对光交换技术所面临的网络冲突和多跳步问题，本项目拟提出一种光域、电域交换网络的协同设计方法和结构：使用高带宽、低延迟的光域网络弥补大规模直接网络的全局通信能力（性能）；基于WDM和O-OFDM技术实现光域网络结构与通信需求的动态匹配（弹性）；基于电域缓存和单次转发，解决光域网络缓存和多跳步问题（扩展性）。本项目从网络分区算法、光交换网络结构、流量分配算法和光电边界路由算法四个方面开展研究，解决光电混合网络的边界划分、协同结构设计和协同通信优化等问题。
面向云计算的软件定义安全关键技术研究	软件定义安全;安全可视化;云计算技术;大数据分析;虚拟化安全	云计算在提供方便易用与低成本特性的同时也带来了较传统计算模式更严重的安全威胁，当前云计算的安全性仍由众多安全技术的集成提供保障，存在如下问题：通用性和扩展性差，无法适应云环境的动态异构特征；整体防护能力弱，各安全技术无法联动；安全管理效率低下，没有统一的配置方法，以及安全无法为用户所感知。软件定义安全为解决上述问题提供了新视角：它将底层资源与其部署方式和功能实现进行解耦，上层通过软件编程进行安全分析和响应。拟开展研究包括：1）软件定义安全基础架构，包括：管理协议、状态收集与安全控制机制等，解决云安全的通用性和扩展性问题；2）软件定义安全管理，基于全局状态的安全分析和统一配置既提高了安全管理的效率，又能增强整体安全防护能力；3）软件定义安全可视化，为用户提供全面、直观的安全状态信息，实现云安全的用户可感知。项目研发成功将实现云安全从传统集成方式向软件定义方式转变，从用户透明化向可视化转变。
硬件木马检测方法和可信设计技术研究	在线检测;离线检测;硬件木马;可信设计;硬件安全	随着IC的全球化以及采用代工的生产方式，攻击者可从IC设计到制造过中故意植入带有恶意的硬件电路，通常称为硬件木马（Hardware Trojan）。硬件木马目前已经成为国家安全、武器装备、金融系统和保密系统等重要的威胁。本项目主要开展硬件木马检测方法和可信设计技术研究，包括集成电路不同设计层次的硬件木马模型、硬件木马电路结构、硬件木马离线检测、在线检测和可信设计方法研究。鉴于硬件木马的多样性，在版图的设计层次表现的行为不同，本课题拟在RTL级、门级、晶体管级、版图级和工艺级开展硬件木马电路检测方法研究，创新提出了一种可有效提高硬件木马检测分辨率和覆盖率的多层次的多模型的检测方法。由于传统的硬件木马离线检测存在不安全和覆盖率低的问题，为了提高系统的安全性，提出了一种完整的离线检测、在线检测和可信设计的抗硬件木马的系统防护方法，在线检测和恢复技术成为装备系统的最后屏障，最大限度地保证系统安全。
大数据平台计算安全保障机制研究	程序划分;信息流控制;访问控制;大数据安全;可信执行环境	大数据平台在带来巨大便利的同时也提出了新的安全挑战，基础设施公有化和组件资源共享导致大数据平台的不可信，而数据资产价值的重要性又对数据计算的安全性提出了很高的要求。本课题旨在解决大数据平台中的计算安全问题，在不可信计算环境下，保障数据计算和数据流动的安全性。.项目拟从四个方面开展研究：1）面向大数据平台的信息流控制理论：面向大数据实体多级多域信息共享需求，研究信息流控制模型，并对模型进行安全性验证；2）基于安全硬件的可信执行环境构建：面向非可信计算环境中的安全执行需求，基于可信硬件构建可信执行环境，为大数据应用提供可信支撑； 3）基于程序划分的安全执行机制：基于安全标记方法对程序进行划分，实现隐私敏感操作的安全隔离；4）基于信息流控制的分布式访问控制：面向大数据平台访问控制和审计需求，实现数据流动的可控、可审计。
超高密度磁记录用软硬磁复合式存储介质的制备与研究	交换耦合式复合磁记录介质;纳米结构薄膜;薄膜的外延生长;信息磁记录	社会的高度信息化直接促成了低成本、高密度磁性信息存储技术- - 硬盘驱动器技术（包括集成闪存的复合式硬盘）的快速发展。现在，500 Gb/in2或更高面记录密度的垂直磁记录面临巨大的困难：（1）写磁头无法对记录介质进行写入操作。最近，交换耦合式复合介质（ECC Media）的提出在一定程度上缓解了这种矛盾。（2）传统的溅射方法得到的磁性晶粒的尺寸分布很难满足高密度记录介质的要求（15%）。本实验室改进的溅射方法制备的薄膜，晶粒尺寸在纳米尺寸，并且尺寸分布最小为5%。本人将在此基础上，围绕软硬磁复合记录技术（ECC or SpringMagnet Media）进行系统研究，开发出具有自主知识产权的可以应用于记录密度为500 - 1000 Gb/in2的硬盘驱动器盘片。同时将培养出一批优秀的毕业生，以满足中国信息存储工业快速发展过程中对专业人才的迫切需求。
基于阵列码的分布式容灾存储系统	容灾存储;分布式存储;阵列码	网络上最重要最宝贵的资源是数据和信息,因此网络上承载的数据信息的安全性是网络容灾与网络可生存性的首要任务。特别是震惊世人的汶川大地震之后，数据容灾研究的紧迫性更加凸显。本课题将深入研究一种基于阵列码的大规模分布式数据灾难恢复的新技术体系，特别是其中的关键技术和原型系统研发，包括核心技术方法、技术体系、关键的软硬件原型系统、灾难恢复模拟试验、发明专利等关键点。课题拟解决的技术关键将是国际上挑战性的课题：对于由N台数据服务器（包括备用服务器）组成的一个网络分布式容灾存储系统，当其中任意r台数据服务器的数据遭到损毁时，可以利用其余N-r台未受损的服务器数据立刻自动恢复这r台损毁服务器的全部数据信息，并且要求：网络上每台服务器存储的数据信息冗余度均不超过2r/n 。
非均匀众核处理器通讯系统吞吐量优化设计与评估方法研究	高通量众核处理器;片上网络;芯片上制造偏差;非均匀网络	在芯片制造过程中由于工艺参数波动，导致芯片内部器件和互连线的实际性能与设计指标之间的漂移，即使初衷为构建同构众核处理器，实际生产制造将导致众核处理器呈现异质分布，同时，芯片上不可避免的故障、MPSoC集成不规则IP以及多任务并发映射导致构造均匀一致的众核处理器变得越发困难。当前，国内外学者虽然普遍意识到该问题，但是还缺乏对非均匀众核处理器通信系统全面系统的研究，本课题是国际上首次探索构成非均匀众核处理器的四种原因的共性特征，通过分析应用程序的通信需求和非均匀众核处理器通信系统资源，以提高系统吞吐量为研究目标，设计提出了综合路由算法、通信资源分配策略以及路由器微体系结构设计的解决方案，通过软件仿真和FPGA验证，完成对众核处理器通信系统性能的评估。并设计了开源的众核仿真器和FPGA仿真平台提供给学术界，用于众核处理器的研究。本研究对探索非均匀众核处理器通信系统具有重要的参考价值。
空天应用中自适应数据处理系统自主演化与容错机制研究	现场可编程门阵列;自主演化;自适应;数据处理;容错	为满足空天应用中数据处理系统对强自适应能力、高可靠性、小型化和先进处理能力的需求，借鉴演化硬件思想实现基于现场可编程门阵列的自适应数据处理系统。演化硬件可解决传统数据处理系统自适应能力不强，可靠性不高的问题，但目前仍存在演化时间长，复杂电路验证评估困难，在线自适应与自修复速度慢等问题。本项目从体系结构、自主演化与容错机制三个方面研究相应解决方法：（1）采用单芯片演化设计系统的体系结构，提高了自适应能力、可靠性和资源利用率；（2）提出了基于可变尺度演化构造块和构造块位流库的快速自主演化机制，提高了系统在线自适应速度和效率；（3）提出了基于演化硬件和多目标优化的选择性三模冗余系统容错设计方法，不仅提高了在线自修复速度，而且可以最少的资源代价，获取尽可能高的可靠性。研究成果可为新型智能电路和芯片设计及构建自组织、自适应、自修复的硬件系统提供理论依据，同时为可重构计算和高性能计算提供参考。
基于可靠性的节能并行存储系统模型的研究	并行存储系统;混合存储;节能;可靠性;SSD	目前并行磁盘系统越来越多地应用在超大型并行计算平台上并用以支持处理数据密集型计算。然而并行磁盘存储系统的能耗和环保问题成为越来越不容忽视的因素。除少数节能方案在系统可靠性方面的影响较小外，大多数广泛研究的节能方案都对磁盘系统的可靠性有较明显的负面影响。针对以上问题本项目致力于研究和设计基于系统可靠性的节能并行存储系统模型。本项目主要在研究现有节能技术对存储系统在不同文件访问负荷下可靠性的基础上，提出一个完备的分析框架用以分析影响因素及其影响幅度。在建立完善可靠性分析模型的基础上设计由固态硬盘和磁盘混合构建的基于可靠性的节能并行存储系统模型。该项目为研究节能存储系统的可靠性问题奠定了理论基础，同时，该项目研究突出了存储系统在节能技术下的可靠性问题为节能存储系统的设计提供了启发性的思路。
基于混沌的非连续频谱高速信道传输及性能研究	信道传输技术;混沌;高速数据传输;非连续频谱;理论性能	无线电频谱资源有限且日益紧缺，已成为高速数据传输的瓶颈。应用认知无线电技术，能智能识别、获取可用频带，从逻辑上产生足够宽的非连续频段实现高速数据传输。由于无线信道具有广播特性，认知网络仍易于遭受恶意窃取和攻击。混沌信号的物理特性使其能有效提高安全性能。然而，基于混沌及非连续频谱，实现高速、高安全性数据传输仍需解决如何使非连续频谱的传输波形呈混沌特性、高效传输参考混沌信号和可用频谱图样的问题。为此，我们将基于混沌，研究非连续频谱上的高速、高安全性信道传输新技术及理论性能，具体包括：（1）研究非连续频谱混沌多载波高速传输，并联合设计非相干高阶混沌调制方案；（2）研究非连续频谱图样检测方法及高码速LDPC混沌安全编码；（3）推导误比特率及保密容量等理论性能，并基于USRP构建原型系统加以验证。本课题的研究成果可用于非连续频谱上实现高速、高安全性混沌信息传输并可评估性能，从而为其应用提供理论基础。
比特图案介质的超高密度瓦记录关键技术研究	磁记录;比特图案介质;硬盘驱动器;瓦记录	因连续薄膜介质超顺磁效应问题，现硬盘记录密度已接近其理论极限。为突破磁记录密度瓶颈，亟需探索新的磁记录机理和实现技术。比特图案介质通过将记录位进行磁性分离，使记录介质获得更高的热稳定性，以应对超顺磁效应而有效缩小位间距；瓦记录则通过像叠放屋瓦似地在已写磁道上部分覆盖记录下一磁道数据，从而既保障写入场强又可实际减小道间距。在比特图案介质上采用瓦记录方式，有望实现2~10Tb/sq-in的面记录密度，但随之面临如下核心问题：1）如何实现比特图案的合理布局以适应瓦记录？2）如何认识比特图案介质的瓦记录过程以优化写磁头设计？3）如何实现瓦记录的数据组织以保证硬盘性能？为此，本项目拟对比特图案介质的布局策略、图案布局与形貌对瓦记录的信号影响、瓦记录过程模型和瓦记录的数据组织进行研究。目标是建立比特图案介质的瓦记录模型和实现方法，为硬盘记录密度大幅提高到现有技术理论极限的2~10倍提供理论支持。
LTE-A飞蜂窝系统的动态资源分配与性能评价研究	性能评价;飞蜂窝;资源分配;LTE-A;动态优化	在LTE-A网络中，引入低功耗飞蜂窝（Femtocell）能够显著增大室内无线信号覆盖范围和强度，减少宏蜂窝部署数量，降低运营成本。本项目拟对LTE-A飞蜂窝系统中的资源分配与性能评价问题进行建模、求解与分析，具体研究内容包括：（1）研究LTE-A系统的信道状态与用户行为特征，建立动态分配信道与功率的马尔科夫决策过程模型；（2）运用近似动态规划方法，通过状态空间聚合、Monte Carlo模拟以及值函数近似等方法，设计动态优化模型的近似求解算法；（3）针对资源动态分配策略，建立LTE-A飞蜂窝系统的随机Petri网模型，运用模型分解与精化技术近似求解平均性能指标，评价系统的稳态性能；（4）运用随机网络演算理论，构建LTE-A飞蜂窝系统中飞蜂窝移动站的分组到达模型，并通过矩母函数求解资源分配策略的信道服务模型，建立LTE-A飞蜂窝系统的随机边界性能评价框架。
云存储系统的安全评估优化技术研究	安全性评价指标;云存储;安全评估;态势感知;安全优化	随着云存储系统的兴起和逐步推广，云存储的安全受到极大关注，开展面向云存储系统的安全评估优化理论和技术的研究对构造云存储安全体系具有重要意义和实用价值。本研究通过安全评估能够明确云存储系统自身的安全缺陷和面临的安全威胁，并分析可能由此造成的损失或影响，为满足信息安全需求和降低云存储系统安全风险提供必要的依据。研究内容包括：分析云存储系统中的安全脆弱性；通过统计分析、神经网络和模式识别等方法确定云存储系统安全性评价指标；研究云存储中的安全态势感知方法；确立定量与定性相结合的安全评估方法和优化模型。本项目提出和建立的云存储安全评估优化理论和关键技术，可望对云存储安全评估优化的理论发展和应用实践有所贡献，在云存储管理和云存储安全领域有很大的应用前景。
NAND闪存系统中的纠错编码关键技术研究	NAND闪存系统;误码平台;门限值;空间耦合码;复杂度	随着存储密度的增加，单元间干扰和层间噪声日益严重，NAND闪存系统迫切需要功能更加强大的纠错码来保证数据的可靠性。空间耦合（spatially coupled，SC）码具有NAND闪存系统所要求的低复杂度编码结构、优异纠错性能、极低误码率等特点。鉴于此，本项目基于NAND闪存系统的分级调制（rank modulation，RM）结构，拟搭建"SC码-分级调制"迭代架构，并在此架构上利用有限码长EXIT方法和PEG算法设计SC码的编码结构，以进一步提高纠错性能；引入滑窗译码算法和最小和算法简化SC码的译码过程，以降低译码时延和复杂度，提高NAND闪存读写速度；同时借助软硬件仿真平台验证系统性能，并利用有限码长比例法则，从理论上分析NAND闪存系统中的SC码性能，探寻决定其性能的关键参数。本项目的研究成果为下一代闪存系统中的纠错码技术发展提供理论支持，并有助于推动SC码在实际通信系统中的应用。
支持可信计算的虚拟平台研究	虚拟技术;可信计算;CrossBit;先进计算	基于虚拟机技术实现可信计算平台，具有可编程、可扩展、可调试等优点。在虚拟机中借助二进制翻译引擎和虚拟I/O模块，可以捕捉每条指令的执行和I/O操作的每个操作步骤，所以理论上它可以实现最微观的安全机制。可信计算包含自身容错和对外防御等两方面。本课题的主要研究目标是基于自主开发的CrossBit虚拟平台原型，实现支持TPM可信认证结构和基于访问控制的入侵检测机制，使之成为一个兼备对内实现隔离、对外开放的可信计算平台。课题研究目标包括在虚拟机（VM）及其管理模块（VMM）中插入指令和资源访问采样点、访问控制、策略进程、TPM访问接口等组件，实现监控者和被监控之间执行环境的隔离，并通过认证机制保证同一执行环境内各层次之间的可信操作。此外，本课题还拟解决可信原语抽象、系统兼容、虚拟性能等研究课题。本课题的研究成果有可能会为新一代的、网络环境下先进计算模式，提供新的理论和方法。
多核异步数据触发微处理器设计关键技术研究	多核;数据触发;片上网络;功耗评估;异步电路	目前微处理器已经发展到多核时代，但是多核处理器的功耗是很难解决的问题。异步集成电路从本质上解决了同步集成电路在深亚微米和超深亚微米工艺条件下所面临的时钟扭曲问题，具有功耗低、电磁兼容性好、模块化和可重用性好等一系列优势。本课题针对多核处理器的功耗问题，研究异步多核数据触发微处理器设计关键技术，目的在于探索数据驱动的异步电路设计思想和数据触发体系结构相结合带来的低功耗特性在多核处理器设计上的应用。本课题将针对数据驱动的异步电路技术、基于数据驱动电路的多核异步数据触发体系结构及其编程模型、数据驱动的异步片上互连技术、多核异步数据触发微处理器的功耗评估和优化方法、性能评估和优化方法进行深入的研究，最终实现多核异步数据触发微处理器原型。本课题的研究可以为高能量有效性的多核处理器的设计和实现提供坚实的理论和技术基础，具有重要的理论意义和应用价值。
面向通信信号处理的众核处理器设计的可预测性理论研究	同步数据流;动态数据流;众核;移动通信长期演进系统;有限动态数据流	通信网络的发展呈现多网融合泛在，单网演进加速，带宽持续扩展的特点，这些造成通信信号处理对于计算能力和可编程性的渴求。多核/众核处理器具有满足通信信号处理这些要求的潜能，又顺应了集成电路发展和延续了摩尔定律，因此将众核处理器应用于通信信号处理是一个必然趋势。目前已有越来越多基于多核处理器的通信系统，但这方面的理论研究严重滞后，亟需开展。.    本项目从理论上研究面向通信信号处理的众核设计的可预测性这个迫切的问题。首先分析了通信信号处理的特点，在同步数据流模型和动态数据流模型的基础上，提出研究具有有限动态性的数据流模型，并根据信号处理各个模块对于动态性的不同要求组合建模。在这个组合模型的基础上，研究众核处理器的调度、存储空间、吞吐量和延时分析方法，解决目前只能依赖仿真分析众核设计的局限性，为面向下一代通信网络的应用奠定理论和技术基础。
非完整大数据的超网络拓扑推断理论与业务识别方法研究	拓扑推断;网络管理;业务识别;超网络;大数据	超网络具有大数据、复杂性、异构性、开放性的特点，由于其传感器节点能力和通信链路严重受限，超网络中大数据具有"信息不确定性"，不能准确和直接反映其态势的信息，对现有的网络管理技术提出了极大的挑战。本项目将通过对数据分析得到超网络空间的态势隐含知识的过程抽象为基于不完备信息的知识发现过程。首先对原始数据信息进行度量，度量数据对推断超网络空间拓扑结构的有效信息量；并以粗糙集这一处理不完备信息的知识发现理论为依据，结合数据多层多维多尺度的特性，利用相似性原理和数据融合等理论对多层网络拓扑进行关联分析，实现对大数据超网络空间中隐含节点和关联关系的发现，达到对网络拓扑结构推断的目的；并在此基础上，研究不同业务类型的识别和分类，发现可能存在的业务路径，形成一套业务路径识别方法。基于多平台构建仿真实验环境，建立测试评估体系，用于实验验证本项目所提出理论与方法的有效性。
多处理器计算系统能耗和服务质量约束的任务调度策略研究	能耗优化;多核;服务质量;任务调度;多处理器	能耗和服务质量问题已成为影响多处理器系统规模和可靠性的关键问题。传统的任务调度策略以任务完成时间、吞吐率等性能指标作为任务调度的依据, 较少考虑系统能耗与服务质量问题。在多处理器计算系统中，如果单纯降低系统能耗,则会影响系统的服务质量。本项目将以当前广泛应用的多处理器计算系统为背景,研究适用于多处理器及多核架构下的能耗和服务质量约束的任务调度策略。通过研究任务负载的能耗特性及细粒度的线程能耗模型,应用模糊集理论对服务质量进行表达、强制和检验，提出多处理器系统能耗和服务质量可感知的任务调度模型和启发式算法,在满足指定的服务质量和资源服务能力的前提下,合理分布不同处理器或处理器核之间的能耗，使计算系统的能耗最小化。此种任务调度策略在传统任务调度策略的基础上,实现系统能耗和服务质量的多目标联合优化与折衷。这对于降低多处理器及多核计算系统的能耗,提高系统服务质量,具有重要的理论意义和应用价值。
云计算环境下键值存储系统查询优化技术研究	索引技术;查询规划;键值存储系统;云计算;No-SQL数据库	大数据、大流量、高并发是云计算的主要应用特征。分布式键值存储系统已经逐步取代传统关系型数据库，作为海量结构化数据的云存储平台。课题解决键值存储系统在云计算环境中查询多样性差、查询效率低的问题。研究内容包括面向云环境的全局最优查询计划选择机制；基于工作流感知的动态多层缓存结构设计；以及面向键值存储系统的高效索引数据存取、组织技术等，课题研究目标是建立面向云计算的海量结构化数据在线的实时存储、分析平台。
超低飞高条件下硬盘磁头的运行姿态建模与测量方法	建模;磁盘存储器;磁头;飞高;纳米测量	硬(磁)盘存储器是未来仍将长期采用的大容量快速信息存储主导载体形式。与半导体存储、光存储等方式相比，磁盘存储器具有更高的性价比，技术潜力巨大，存储密度的提高已经超过摩尔定律的预言，呈年翻一翻的增长态势。伴随着磁头飞高(即头盘间距)突破5纳米极限设计理论的提出，磁盘存储密度由现在的100Gb/sq-in提高到1000Gb/sq-in(即1T位/平方英寸)成为可能。由于磁盘的道密度、位密度、磁头飞高皆已进入纳米尺度，在超低飞行状态下磁头与介质磁粒、微尘、盘面间处于微磁场/流场环境中分子级的复杂相互作用，从而产生了磁头纳米级精度的设计、制造、运行与测控问题。本项目旨在研究超低飞高条件下磁头的运行姿态建模和测量方法，主要研究内容包括：磁头飞行姿态参数确定、磁头与介质磁粒及微尘间的分子级相互作用机理、磁头的飞行姿态建模与纳米精度测控方法。研究结果可为新型超高密度磁盘的磁头设计与运行测控提供理论基础。
面向数据密集计算的超高性能多核处理器体系结构研究	多核处理器;系统结构;数据密集计算;处理器	单纯依靠提高主频已经很难进一步提高处理器性能，采用以多核处理器为代表的先进体系结构已逐渐成为当前提高处理器性能的主要技术途径。面向数据密集计算，本课题将提出一种基于同步数据触发机制的高性能低功耗多核处理器体系结构，并深入研究其设计实现关键技术，主要包括：计算内核和多核处理器计算模型和虚拟机模型；结构简单、计算资源利用率高、计算能力强、可扩展性好的同步数据触发计算内核体系结构；以较小硬件开销满足数据密集计算对多核并行处理的通信带宽和存储带宽要求的带宽平衡系统结构；在体系结构、编译、任务调度、电路实现等多个不同层次上的多核处理器低功耗设计优化技术；多核处理器编程模型，以及以半自动方式辅助程序员进行程序开发的多核处理器并行程序设计等关键技术。本课题的研究可为千亿次/万亿次级超高性能多核微处理器芯片的设计与实现提供坚实的理论和技术基础，具有重要理论意义和应用价值。
高效能众核异步微处理器设计关键技术研究	微处理器;众核;高效能;异步;效能评估	目前，微处理器发展已经开始进入众核时代，单个芯片上处理器核的数目呈现指数增长，然而功耗问题和互连问题一直是制约众核微处理器发展的瓶颈问题。异步电路相比同步电路而言具有功耗低、电磁兼容性好、模块化程度高一系列优势，可以从根本上解决当前众核微处理器研究遇到的一些问题，在未来众核微处理器发展过程中引入异步技术已成必然趋势。本课题将针对众核微处理器的功耗问题和互连问题，研究众核异步微处理器设计关键技术，目的在于将异步电路的低功耗优势与众核微处理器丰富的计算资源优势结合，建立高效能的众核异步微处理器原型系统。本项目将针对高效能的众核异步微处理器体系结构与计算模型、高效能的异步电路设计技术、高效能异步单核微处理器设计技术、高效能异步片上网络、众核异步微处理器效能评估与优化等展开深入研究，最终实现众核异步微处理器原型。本项目的研究将为高效能众核微处理器的研究与实现提供坚实的理论和技术基础。
面向云存储服务的文档保护与组合关键技术	细粒度访问控制;分级属性加密;文档自毁;文档组合;云存储服务	本项目针对云环境中电子文档与组合文档的存储服务及细粒度访问控制需求，重点研究基于属性的文档分等级加密存储与共享机制和文档组合模式问题。通过提出文档属性集的等级划分和受限条件概念，融合到文档属性加密的新方法研究以支持文档的安全云存储服务，并应用多重签名和聚合签名减少文档在云端的存储和通信开销，实现文档完整性验证的高效性。延拓多方计算理论的领域应用，提出基于扩展的安全多方计算的文档组合模式，攻克云环境中安全文档组合难题。研究属性集拆分和多授权机构的属性加密机制，以解决随属性集变化的可伸缩文档授权管理问题。通过最小属性集提取、密文耦合和分布哈希网络分发的方法，实现文档使用后的自毁以保障数据安全与隐私保护。本项目研究将强化面向文档工程的文档安全体系架构，相关内容处于当前国际研究前沿，安全文档组合及可伸缩授权等内容未见报道，研究成果将创新文档保护、组合方法和技术。
光片上网络关键问题的研究	光互连;路由算法;路由器结构;三维拓扑;众核系统	高性能计算正从千万亿次运算迈入百亿亿次运算的时代，单一芯片上将集成成百上千的IP核，而对于这种大规模核间互连的需求，当前电片上网络存在带宽、时延、功耗等瓶颈，因此在未来高性能芯片设计中，人们更趋向于用光片上网络来解决这些技术瓶颈。当前光片上网络的研究主要集中在中小规模的网络的二维结构设计上，以提高吞吐，降低时延和功耗为设计目标。当网络规模扩大时，将加剧现有设计方案阻塞概率大、可靠性低、可扩展性差等问题。本项目拟采用三维拓扑结构设计，通过时分复用、波分复用、空分复用相结合的方法，实现混合高效复用技术，在降低阻塞、提供高带宽的同时，充分利用片上资源，有效降低成本，设计实现具有多播功能的光片上网络结构，从而支持多种通信模式，设计多目标路由算法，从阻塞、容错、损耗和串扰出发，确保路由的高效性和容错性，提高信号的信噪比，从结构设计出发，改善光片上网络的可靠性。最终实现面向千核的光片上网络架构。
面向超深亚微米工艺的MRAM高速缓存架构技术	工艺参数波动;高速缓存;磁电阻随机存储器;可靠性;计算机存储系统	磁电阻随机存储器MRAM是一种新型非易失性存储器，具有可微缩能力强、存储密度高、静态功耗极低和不易受射线影响等特点。使用MRAM设计处理器片上缓存有望突破传统SRAM在超深亚微米工艺下的技术制约，显著缓解计算机系统的"存储墙"问题。本课题以MRAM存储器的技术发展为背景，研究MRAM在处理器片上高速缓存中应用的架构电路设计技术及其可靠性评估策略：采用器件建模和概率分析的方法，量化分析MRAM工艺参数波动问题及其对高速缓存的性能影响；结合MRAM工艺参数波动效应，提出一种能容忍工艺参数波动的MRAM缓存架构与电路设计技术；提出一种能改善MRAM器件写容限的高速缓存架构设计技术；研究SRAM与MRAM混合的缓存系统可靠性改善问题，提出其对软错误的可靠性评估策略。本研究采用器件工艺、电路与系统等多个层次协同的研究方法，对超深亚微米工艺下的低功耗高可靠缓存系统设计具有重要的研究价值。
面向高精度计算领域动态可配置加速器体系结构研究	动态可配置;加速器增强型体系结构;高精度计算;并行计算	随着半导体技术的发展，芯片晶体管数量持续增加，功耗成为限制处理器性能提升的主要瓶颈。面向计算密集型应用领域定制加速器是保证芯片性能延续摩尔定律的发展趋势、提高芯片计算效率、缓解功耗问题的一种有效方法。然而，除了计算性能外，大规模科学和工程计算对处理器的计算精度也提出更高要求，迫切需要对高精度算术提供有效的硬件支持。.   课题结合处理器的发展趋势和计算精度需求，研究面向高精度计算阵列加速体系结构在性能、效率和扩展性方面所面临的挑战，研究基于VLIW可编程统一高精度基本功能计算引擎架构、支持延时容忍的可配置循环类加速策略和流水线折叠的可编程非规则类加速策略的计算加速核结构和存储架构、支持多种访存模式的自适应配置DMA数据引擎架构、支持SPMD数据并行模式和Systolic阵列功能流水并行模式的可配置多核阵列体系结构等关键技术，为下一代高效能计算机提供有力的计算能力和计算精度支持。
基于硬件时分复用的大规模脉冲神经网络处理器	脉冲神经网络;大规模网络;时分复用;类脑计算;处理器	在类脑计算的浪潮中，作为一种对大脑工作方式有效模拟，脉冲神经网络被认为是是实现脑计算根本性突破的几个可能性方向之一，从而成为研究的热点。对于大规模脉冲神经网络的加速也变得至关重要，然而由于脉冲神经网络的特殊性，现有的结构设计通常使得物理上的神经元和逻辑上的神经元一一对应，而对于构建大规模脉冲神经网络专用的处理器运行平台，这种方式变得十分困难。本项目拟设计通过时分复用支持任意规模的脉冲神经网路处理器，主要从算法优化、结构设计和验证三个方面展开具体研究，从而使得任意规模的脉冲神经网络算法可以运行在有限的硬件资源上，并以此为基础构建一整套完成的加速平台。本项目力争形成脉冲神经网络处理器平台及配套开发工具，为我国智能芯片研发添砖加瓦，提供参考和支持。
跨平台的操作系统安全机制形式化验证方法研究	测评;操作系统安全;形式化方法;模型检测	针对现阶段操作系统安全机制分析中出现的困难和现有安全策略分析方法中出现的问题，从操作系统的安全机制实施和安全策略配置方法入手，研究通用的访问控制元数据和安全需求的形式化表述方法，建立用于描述系统安全策略配置的数学模型； 同时利用严格的逻辑和验证工具，分析和验证安全需求与安全策略之间的一致性，在此基础上建立安全策略漏洞报告机制，从而以数学模型和相应的验证工具帮助操作系统设计者、管理者和用户更好地理解和分析安全策略，完善系统安全机制的实现和优化。
搜索引擎在线算法的GPU优化关键技术研究	CUDA;归并;GPU;搜索引擎;分数计算	搜索引擎服务是最重要的互联网服务之一，它必须能够在秒级内完成用户查询请求并返回给用户所需的结果。为了达到这一要求，最新、最好的IT技术都会被应用到搜索引擎中来。本课题的研究动机正是来自搜索引擎企业的实际需求：如何降低单个搜索服务器的CPU负载。针对这一问题，本课题的工作是研究一种CPU与GPU混合方法来优化搜索引擎在线算法，通过将大计算量任务迁移到GPU上来达到降低CPU负载的目标。在具体方法上，课题提出了适合于GPU的数据压缩、索引组织、批次处理和Cach等方法，并将其应用于归并计算、分数计算和TopK等大计算量算法，从而达到降低服务器CPU负载的研究目标。课题的成果不但能够直接应用于搜索引擎服务，而且对于有类似需求的计算密集型应用也具有重要的参考价值。
以协作中继为核心的无线网络多资源联合分配问题研究	网络吞吐率;联合资源分配;协作中继;协作通信;能量有效性	限制无线网络应用的技术瓶颈是无线传输的效率和性能，而信号衰落是降低无线传输效能的主要因素。近年来，研究者提出了新颖的协作通信技术来克服信号衰落效应，即多个单天线节点通过某种方式来共享彼此的天线，以获得类似于MIMO系统的空间分集与复用增益，提高无线传输效能。由于协作通信要求中继参与传输，因而协作中继是实现该机制的核心要素。国内外已有针对中继分配的研究往往不能满足无线网络资源受限等特性，因此研究协作中继与其他资源的联合分配对提高无线网络性能至关重要。本课题将以无线网络特性为出发点，以降低网络能耗或提高服务质量为优化目标，紧紧围绕协作网络多资源联合分配建模与实现机理这两个科学问题，设计以协作中继为核心的无线网络多资源联合分配算法和机制，使其适用于资源（如能量、带宽等）受限的无线网络环境，满足不同的应用需求，并在相关测试床上进行算法与协议验证，推动协作通信技术在下一代无线网络和移动通信中的应用。
基于零星请求的低能耗磁盘调度策略研究	存储;调度;能耗;磁盘	基于磁盘的不同能耗状态切换，磁盘I/O中的非突发时段为降低其能耗提供了可能。但非突发时段之间的零星请求会导致高昂的时间和能量开销。本项目提出利用I/O的块关联性和频繁序列挖掘技术来定位该零星请求并进行调度，以尽可能延长磁盘处于低能耗状态的时间，从而在保证服务质量的前提下最大限度地降低其能耗。本项目将研究磁盘I/O中的非突发性数据访问行为，揭示其内在规律并构造合适的描述模型，探索该行为和零星请求的潜在联系并阐明其机理，设计零星请求的定位方法；利用多目标优化理论来研究并设计基于零星请求的低能耗调度策略的代价模型，明确磁盘Cache对该调度策略的影响，设计自适应的磁盘Cache分配策略以最大化该策略的能量节省；研究并设计基于该调度策略的磁盘服务质量保证机制。实现一个基于该策略的原型系统来验证理论成果。相关研究结果，对于降低多盘存储系统、分布式存储系统、数据中心等的能耗都将具有重要的参考作用。
GPU虚拟多通道不对称并行模型与公平调度策略研究	公平调度策略;GPU;不对称并行模型;虚拟化技术;虚拟多通道	GPU域已经更像一个完整复杂的并行计算系统，域内资源种类繁多，在满足虚拟计算环境多样化的GPU应用负载和不对称的性能需求前提下，如何组织与管理GPU内部资源，提高资源利用并行潜能,是高性能GPU虚拟化设计面临的关键问题。通过分析虚拟计算环境应用混合负载特性，提出GPU虚拟多通道不对称并行模型，将GPU域分离为性能不对称的多个虚拟GPU子系统，分别服务于多种性能需求不对称的异构应用；研究GPU资源的不对称并行划分方法、与不同应用匹配的虚拟GPU子系统之间的资源借用策略。在此基础上提出多通道虚拟GPU子系统公平调度策略，将虚拟计算环境多类GPU应用负载分派到不同性能的虚拟GPU子系统，以合适的内部资源来服务适当的应用需求，在满足应用性能需求前提下，提高GPU内部资源并行潜能及最大化资源使用公平性。本项目预期成果将对虚拟计算环境GPU子系统建模、分析、设计与开发提供新的研究思路。
基于测度属性关系网络分析的大规模分布式系统生存性异常检测技术研究	关系网络构建;关系网络划分;大规模分布式系统;测度属性	针对大规模分布式系统环境，尚没有完整的生存性模型的研究成果．缺少对运行系统进行在线、实时的生存性异常事件检测方法，而分布式系统的规模逐渐增加使得传统的基于复杂数学模型的方法无法应用。本项目以大规模分布式系统生存性异常检测为目标，以分布式系统测度属性关联分析为研究对象，针对目前的研究成果中缺少分布式系统测度属性关系网络构建和测度属性关系网络增量式划分模型等问题，重点研究：基于时空关联分析的分布式系统测度属性关系网络构建技术；基于隐马尔可夫模型的测度属性关系网络增量式划分方法，基于测度属性关联分析的多粒度生存性异常检测模型。最终得到分布式系统测度属性之间的关联关系，利用动态划分结果分析、划分类内关联度计算、特定属性关联度计算等手段确定系统生存性异常事件，并给出多粒度的检测结果。该研究对于在分析大规模分布式系统环境下的服务生存性具有重要的理论意义和应用价值。
基于智慧的下一代网络路由关键技术	下一代网络;单播路由;智慧;组播路由;网络架构	随着信息社会的发展进步，Internet网络已渗透到国民经济各个领域，起到了不可替代的作用。然而，作为Internet网络基础支持的IP路由体系面临着十分严峻的挑战。传统Internet网络在单播路由方面主要存在可扩展性、可靠性和收敛性等问题，组播路由主要存在可扩展性和QoS保障等问题。这些问题出现的根源是原始Internet网络架构设计缺陷，需重新构建Internet网络架构及核心协议。为此，本项目将首先采用随机集合理论及超图理论构建智慧的下一代网络架构，它主要由三个平面构成：用户平面、控制平面和智慧平面，三平面协同工作完成一体化服务功能。然后利用随机微分博弈理论分析和建立单播路由、组播路由模型，主要采用的方法是建立各自的效应函数，求解其最优值。本项目目标为构建一套完整的未来网络系统，解决传统Internet网络存在的单播可扩展性、可靠性和收敛性及组播可扩展性和QoS保障。
多微通道内存控制器关键技术的研究	访存特性;多核处理器;多微通道;虚拟化;内存控制器	多核处理器的发展对内存系统的延迟、带宽和功耗等方面提出了更高的要求。针对这种情况，本课题申请人提出过一种多微通道内存控制器设计方法。该内存控制器可以在提高内存系统带宽利用率的同时，缩短访存请求延迟，并具有较高的性能功耗有效性。在后续的研究中发现，现有的多微通道内存控制器不能实时侦测应用程序访存特性的变化趋势，从而动态的在应用程序间分配微通道，在实际应用中具有一定的局限性。本课题旨在三个方面对多微通道内存控制器设计进行优化：(1)引入动态通道分配机制。该机制可以更加灵活的在进程间分配微通道，具有更高的实用价值。(2)提出一种适用于多微通道内存控制器的协同访存调度算法。该算法不但可以使不同的微通道对进程优先级有相同的认识，还有低通信量和高扩展性等特征。(3)提出一套多微通道内存控制器虚拟化方案。该方案可以对多微通道内存控制器的通道资源进行有效管理，具有多实例、隔离性和高性能等特征。
混合云环境下面向跨域服务联动的协同调度建模与优化研究	协同调度;混合云;任务调度;分布式系统	云计算即将进入混合云的时代，协同调度是混合云环境下实现互操作的关键。在混合云环境下调度受到经济学利益驱动，由于这种自利性，给协同调度带来了一定的挑战，本课题深入研究面向经济学协同调度相关模型及算法。首先，利用灰色模型预测资源可用度，从流量特征、服务收益成本等指标刻画系统可用能力，并基于排队论研究系统可用性的评估方法。基于可用性评估结果研究对资源进行逻辑服务簇的分类和筛选方法。然后，针对协同调度，研究集中和分布式结构相结合的调度器分层组织架构；在可用性评估基础上，基于云模型研究面向经济学的参数量化和效用模型。最后，利用马尔可夫决策过程（MDP）将调度从最优化问题转化为决策问题，然后基于强化学习理论分别针对合作型和自利型两种调度环境设计相应的在线协同调度算法，实现多任务流的自适应、多调度策略的协同。
分布式存储系统基于纠删码的可靠访问关键技术研究	降级读;分布式存储系统;数据修复;纠删码	分布式存储系统规模扩张导致数据失效成为常态。纠删码通过保存少量冗余数据以实现数据容错，被广泛应用于分布式存储系统之中。然而纠删码的引入也改变了系统的读写访问和失效修复方式。如何加快基于纠删码的数据修复并降低其对系统读写访问的影响，成为当前分布式存储系统亟待解决的重要问题。基于此，本项目针对分布式存储系统基于纠删码的可靠访问关键技术展开研究，具体内容包括：1） 设计并实现面向异构分布式存储系统的节点修复优化方法；2）设计并实现面向降级读优化的数据布局和纠删码高效切换算法；3） 设计并实现面向纠删码的数据更新优化方法。本项目主要目标为加快分布式存储系统中基于纠删码的节点修复，并提升其读写访问性能，从而为纠删码在分布式存储系统的应用提供技术支持。
面向异构计算系统的非对称容错架构	冗余容错计算机;加速器架构;可靠性优化;异构计算系统	在如今异构计算时代，传统的CPU和大量加速器一起为主流应用提供性能保障。性能、能效和可靠性是处理器的三大重要指标，而且三者相互制约。加速器的主要优势在于性能和能效，而非可靠性。鉴于芯片制程的可靠性问题日益凸显，我们亟需解决加速器的可靠性问题。此课题重点研究设计可扩展和有效的可靠异构架构。传统思路是单独地优化加速器可靠性。然而加速器的种类各不相同，对其进行单独优化将导致极大的工程量，使得系统无法快速地集成这些加速器。因此这种传统的优化方案是不可扩展的。申请人提出了一种新颖的可靠性设计方案，即非对称容错架构。该设计方案的目标在于找到一种"即插即用"的以加速器为主的系统架构，无须单独地优化其可靠性，而是在系统的层面上解决此问题。非对称容错架构以CPU为核心进行可靠性优化，当加速器发生错误时保障整个系统仍然能够可靠地运转。申请书中论证了该架构的可行性，并详细地描述了其设计思想和其中的关键科学问题。
领域本体服务组合的Petri网建模和非功能特性分析	非功能特性;领域本体服务组合;随机Petri网;OWL-S	服务组合是将服务看成构件而进行重用的技术，已逐渐成为SOA构架下搭建分布式应用和企业业务流程的主要手段。在诸多服务合装标准中，领域本体组合标准因能实现语义WEB服务的自动发现、匹配、组合、执行而成为近年的研究热点。目前该领域的理论研究重在其形式化和性质验证，而其非功能特性和量化分析方面却相对薄弱。本项目立足深入分析服务组合和领域本体的本质属性，对以OWL-S为代表的组合标准的非功能特性进行研究。重点研究内容包括：（1）运用随机Petri网对组合流程进行量化建模；（2）设计基于等效结构约简的方法，对性能、可靠性、执行代价等多个非功能特性指标进行求解和分析；（3）获取真实语义WEB服务运行数据，并运用置信区间分析对预测结果进行检验；（4）运用灵敏度分析技术，对非功能特性的瓶颈因素进行检测。本项研究能为语义网络WEB服务可信性研究提供模型支撑和分析手段，并有望在量化分析这个薄弱领域取得突破。
基于志愿计算的信息元数据应用关键技术研究	志愿计算;编程模型;数据属性;形式化	志愿计算利用自愿者提供的空闲软硬件资源，为实际应用提供强大便利的共享.计算能力。由于资源的异构，动态及随机性等增加了该平台进行数据管理的难度。本研究.拟汲取网格、动态资源管理、云计算、随机过程和形式化方法等研究的成果，对应用的信息管理常用操作进行归纳和分析，抽象出共有的被操作信息的属性及接口，即信息元数据，提.供丰富的构件，研究支持把众多应用快速迁移到志愿计算平台上的关键技术。提高平台服务质量，完善其支持多种数据易用的能力，增加分布式应用及目录管理和使用多种传输协议的功能。拟研究一种适合志愿计算平台下的编程模型，并进行形式化论证和性能分析，再对特.定的应用实例进行定量分析，探讨其通用异步机制的实现方式。旨在使其能够便利的实现.MapReduce 和多变量的生物计算等应用，从理论上研究志愿环境中管理信息元数据来实现信.息优化方法和编程模型，提高平台的可扩展性和容错机制，提高服务质量
面向虚拟化云服务器的智能高速缓存管理	多核处理器;高速缓存;虚拟化;云计算	随着云计算的发展，当代数据中心日渐使用虚拟化技术来支持大量异构负载在多核服务器上的并发执行。然而虚拟机间对共享高速缓存的隐式竞争会破坏虚拟机的性能隔离，现有技术又忽略租户多样化的服务需求及虚拟机固有的特征。为此本课题将重点完成以下研究内容：研究面向高速缓存SLA优化的虚拟机间可伸缩Cache结构，包括高速缓存SLA度量的定义，可保证SLA的Cache容量划分算法的设计；研究面向并行负载中共享私有数据局部性优化的虚拟机内数据感知Cache管理策略，包括数据效用监控硬件的设计，基于数据插入位置控制的隐式划分算法的设计；研究面向重定位虚拟机高速缓存访问延迟优化的组间协作单体Cache结构，包括Cache组访问压力的剖析，重定位虚拟机工作集保留策略的制定。本课题将极大改善虚拟化环境下高速缓存的服务质量和性能，预期在国内外期刊和国际会议上发表8-10篇高水平论文，并申请1-2项专利。
DNA计算与分子尺度逻辑体系研究	自组装;DNA计算;分子逻辑系统	构建分子尺度逻辑体系对认识纳米尺度下的能量转换规律、分子相互作用与调控机制、发展高性能计算有重要意义. DNA 逻辑门是DNA 计算机体系结构的产生基础和DNA 计算机实现技术的硬件基础. 本项目重点研究: (1) 将DNA 自组装与链置换技术相结合, 构建DNA 分子逻辑计算模型; (2) 设计含有非结构化DNA、G 四链体、i-motif 和发夹结构的硼配位物/DNA探针. 通过输入不同的信号分子来调控体系内的荧光能量共振转移（FRET）过程, 实现以荧光强度变化为输出方式的DNA逻辑门(NOR、AND、INH 和分子开关); (3) 设计结合DNA计算特点的生物操作; (4) 建立实用化的大规模型求解NP-完全问题的逻辑计算模型. 计算模型能通过DNA 自组装结构电泳迁移率的改变输出计算结果. 本项目体现了数学、生物和计算机科学等学科的交叉与融合.
基于云计算平台的网络虚拟环境关键技术研究	网络虚拟环境;云计算;可扩展性;系统负载测量;资源调度	随着网络及虚拟现实技术的发展，以网络游戏、网络会议为代表的网络虚拟环境已成为一类重要的互联网应用，有着显著的用户及经济效益。网络虚拟环境用户数量多且多变、其用户行为多且多变，这为分布式计算带来极大的机遇与挑战。学术界对此有广泛兴趣并提出了不同可扩展及资源调度方法以可扩展、高效地服务行为多且多变的海量用户。但是这些方法只解决了部分挑战，比如事件同步模拟技术能有效降低网络开销，但CPU开销较高。本项目首先测量分析网络虚拟环境海量用户行为及系统负载，深刻理解其规律；其次，结合发现的规律及传统可扩展技术的优点，设计可扩展的区域模拟技术以支持海量用户多样多变的行为；最后，充分利用云计算平台不同计算资源及计费模式的特点，设计云资源调度技术以较低成本服务海量且多变的用户。本项目提出的可扩展技术及云资源调度技术可以有机地结合起来一起解决上述挑战，将对拥有巨大用户基数及重要产值的网络虚拟环境产生重大影响。
异构嵌入式系统的能耗感知调度机制与方法研究	时间约束;能耗感知;任务调度;混合调度;异构多核	据不完全统计，全世界98%的计算机设备都是嵌入式系统。一直以来，嵌入式设备都追求低功耗、低成本和高性能等。异构多核处理器具有高性能和灵活性以及低成本和低功耗等特点，使其在工业、国防、医疗和通信等诸多领域得到了越来越广泛的应用，它被普遍地认为是未来的多核处理器的主要发展趋势。为了满足人们不断增长的高性能需求，越来越多的核被集成到一块单一的芯片上，使得芯片上单位面积的功耗增加。功耗的增加加速了计算机系统内部的温度的升高。高温对计算系统有着严重的影响，它降低晶体管的寿命，减慢信号传输速度，增大永久故障和瞬时故障率，以及带来巨额冷却费用等。这些因素反过来又降低计算系统的性能。此外，随着"绿色计算"需求的提出，如何减少各种计算系统在执行应用时所消耗的资源、能量和预算等成本问题已经成为当前工业界和学术界共同关注和研究的热点。高效的任务分配和任务调度策略不仅可以减少计算系统所消耗的能量和成本，还可以降低温度和提高计算系统的可靠性。因此，本项目将围绕异构嵌入式系统的能耗感知调度机制与方法展开研究。
逻辑虚拟域中软件执行的可信确保机制研究	安全控制;行为监控;逻辑虚拟域;软件故障容忍;虚拟化技术	以资源租用、应用托管为特征的云计算有望成为企业/机构应用信息化的主流模式。逻辑虚拟域是由云计算环境中不同服务节点上的多个虚拟机为满足企业/机构分布式应用的资源需求或协作处理需求而组成的动态联合体，也称为"基于虚拟机架构的虚拟组织"。逻辑虚拟域为云计算应用提供了高效执行环境，但如何确保逻辑虚拟域中软件的可信赖服务能力还面临着严峻挑战，如：软件执行是否安全？软件出错能否恢复？软件更新是否合法等。本课题拟研究：1）基于逻辑虚拟域的行为监控机制，解决软件执行过程和用户访问在逻辑虚拟域中的监控问题；2）基于逻辑虚拟域的软件安全控制机制，实现逻辑虚拟域中软件的执行安全及用户访问控制；3）基于逻辑虚拟域的软件故障容忍机制，保证逻辑虚拟域中软件执行的正确性；4）基于逻辑虚拟域的软件可信更新机制，确保软件更新的合法性和高效性。本课题研究形成的理论和方法有助于保障逻辑虚拟域中软件执行的可信性，具有重要意义。
Petri网系统的子系统相关性研究	语言;建模;分析;性质;Petri网	本项目针对复杂系统Petri网建模与分析的关键科学问题，从系统综合的角度，开展Petri网系统的子系统相关性的研究。在分析和归纳子系统交互模式基础上，提供系统混合建模方法，构建系统综合的基础模型。提供子系统相关性子类的判定算法和语义表达，建立子系统相关性的分类谱系。给出子系统相关性的语言和性质关系，揭示子系统相关性的行为机理。进而提供一套有效的系统行为分析、性质验证、性能评价和优化控制方法，实现系统综合的有效分析、验证和控制。。建立高级网系统的子系统相关性理论，为实际系统的Petri网建模与分析提供可行方法。基于上述理论成果，为服务动态组合系统建模、行为分析和QoS评价提供解决方案，研制相应的服务动态组合软件支持工具。本项目的研究，将为复杂系统的Petri网建模与分析提供解决方案，丰富和发展Petri网科学理论，为计算机系统协作和集成提供理论指导。
面向软件管理片上存储器的编译优化技术研究	流寄存器文件;图着色;存储管理;区间着色;便笺存储器	存储墙问题是高性能计算研究的核心技术问题，软件管理的片上存储器被认为是解决存储墙问题的一个有效途径而得到广泛应用。然而，软件管理片上存储器性能的发挥依赖于编译时分配算法的性能。目前国际国内的相关研究，主要面向嵌入式系统和多媒体处理，面向高性能计算的研究还不够，算法的性能还不够高，测试用例也多为简单的嵌入式应用程序。此外，对于多级软件管理存储层次分配的研究尚在起步阶段，还有很多待研究的问题。因此，面向高性能计算开展软件管理片上存储器的编译优化技术研究，具有重要的理论和实际应用价值。基于此，我们拟在如下方面开展研究：(1) 面向非规整流应用的流寄存器文件分配算法，能够提高非规整流应用在流处理器上的性能；(2) 一般化的基于图着色/区间着色的便笺存储器分配算法，能够提高片上存储空间的利用率；(3) 面向多级软件管理存储层次的分配算法，能够有效分配热点数据，并充分开发复用和并行。
面向高性能可视化计算的体系结构优化设计关键技术研究	可视化计算;体系结构优化;众核处理器;通用图形处理器	面向可视化计算的图形众核处理器发展正处于一个新的转折时期：为了能够支持各种高级可视化效果的表现，图形处理器的主要运算负载正由基于顶点与像素的传统图形计算转变为全局光照计算、碰撞检测、近似物理仿真、图像处理甚至人工智能等非传统高性能可视化计算。而当前图形众核处理器无论是在并行计算模型还是体系结构设计方面，均难以满足未来高性能可视化计算的需求。本课题紧跟高性能可视化计算的发展趋势，深入研究支持各种非传统高性能可视化计算的并行计算模型与相应体系结构支撑机制，并研究各种基于体系结构的优化设计技术，解决当前众核图形处理器在任务与线程管理、片上存储、功耗管理等方面存在的问题。本课题研究工作的成果对面向未来高性能可视化计算众核处理器体系结构技术的发展有着重要的理论意义和实用价值。
多维操控器力觉感知机理与动态位姿耦合信号实时解耦方法研究	多维操控器;解耦;位姿信号;冗余;力觉感知	多维操控器是虚拟现实技术中重要的交互设备，在机器人遥操作、三维设计等领域有着广阔应用前景。本项目采用具有冗余传感的宏动并联机构作为多维操控器力感机构，研究力觉感知的实现方法和空间动态位姿耦合信号实时捕获与解耦机理。通过基本单元运动螺旋的叠加，获得操控器输出运动螺旋的精确数学表达，结合集合论的思想探索操控器拓扑综合规律，解决三至六维不同维数的多维操控器力感机构选型问题。通过协调曲线法建立位姿集任务空间与动态参数、多维耦合反馈力与关节驱动力之间的逆向映射模型，创新性引入冗余传感解决并联机构位置正解解算的难题，采用多目标粒子群优化方法研究冗余传感器及驱动电动机的分配策略，揭示冗余传感对多维动态耦合信号解算速度的影响规律，以及通过独立驱动力实现耦合反馈力的解耦机理。最终形成一套适用于三维至六维通用操控器设计方法和理论，为其在虚拟现实等3D相关领域的普及应用奠定理论基础。
云计算环境下数据感知的大数据管理优化策略研究	数据密集;数据感知优化;云计算;大数据管理;大数据	当前，大量数据密集型应用开始部署到各类云计算平台中，大数据管理呈现了一系列新的特征和需求，如何针对数据特征实现数据感知的大数据管理优化已成为亟待解决的关键问题之一。对此，本课题以大数据应用的数据需求为研究起点，提出"基于有向超图结构变换的数据约简模型"；随后，在数据管理层面向非结构化数据管理系统提出一种"基于不稳定分区动态管理的数据布局策略"；最后在应用管理层面向混合负载的大数据应用，利用不同类型性能计数器提出一个"基于负载区分路径规划的虚拟机部署策略"。本课题提出的"基于有向超图结构变换的数据约简模型"有望获得数据能力特征感知的数据需求，"基于不稳定分区动态管理的数据布局策略"有望实现数据上下文特征感知的数据布局；"基于负载区分路径规划的虚拟机部署策略"有望实现数据负载特征感知的虚拟机部署，以期降低中间数据开销、提高大数据应用的执行效率和性能。
基于算法特征自动化提取分析的机器学习处理器关键技术研究	面向应用的定制处理器;特征提取;机器学习芯片;结构优化;机器学习	随着机器学习应用日益广泛，业界对机器学习处理的性能提出了越来越高的要求，机器学习专用处理器已经成为研究热点。然而，现有的机器学习处理器支持的算法有限，往往只支持某个或某类机器学习算法，领域通用性不够；即使支持多种算法，往往也是以大大牺牲效能为代价；在设计过程中，无论是算法特征提取还是结构优化，都是基于专家的直觉和经验，造成整个设计过程缓慢和低效。机器学习处理器设计目前面临领域通用性、专用高效能、以及结构最优化三方面挑战。.针对这三方面挑战，本项目拟引入机器学习技术来加速机器学习处理器的设计并提高其领域通用性。具体来说，本项目研究基于机器学习算法特征自动化提取和分析、基于核心运算和共性访存的专用运算单元和访存结构设计、基于机器学习的专用处理器性能建模和结构优化等关键技术。并构建机器学习处理器原型系统，形成一套专用处理器设计的开源工具，服务于智能处理器为代表的领域专用处理器研发。
面向多用户共享云计算平台的大数据处理公平性研究	公平性资源分配;数据密集型计算;云计算;海量数据处理;MapReduce	云计算作为一种新的计算模式，已成为大数据计算的首选平台。云服务商通过按时收费服务方式让用户通过租赁云计算资源进行大数据处理。在云计算中保持一个高的资源利用率对于提高程序性能和资金使用性价比意义非常重大。然而，由于用户对资源的需求往往是随时间而不断变化的，一直保持一个较高的资源利用率对于单个用户而言是一件非常困难的事情。多用户资源共享是一个非常经典有效的提高资源利用率的方法。满足公平性资源分配是资源共享的前提和保证。本课题从经济学角度研究分析发现，传统的高性能计算、并行计算和网格计算中已有的公平性资源分配算法和策略在共享云计算平台中存在几个严重问题（包括：自私用户提交无用任务问题，狡诈用户受益问题和付出与回报不成比例问题）使得其并不适用于共享云计算环境。据此，本课题通过研究云计算的一些基本特征，提出一套面向共享云计算平台新的单类型和多类型资源公平性分配算法，并将其以插件方式实现于Yarn中。
基于FNN的国家电力网战略安全防御体系模型与仿真方法研究	战略信息战;国家级战略电力网安全防御体系模型;仿真方法	现代战争中攻击电力系统已经成为新的作战样式、它与地震、雷击等自然灾害一起构成了国家战略电力网广域安全防御体系的重大隐患。.以此为背景，本研究目的是，创立一套"基于FNN的国家级战略电力网安全防御体系模型与安全状态模拟方法"，为实现国家战略级电力网安全状态预警、防御，战略信息战能力评估奠定理论技术基础。.它将根据因素神经网络理论（FNN），把战略级电力网络安全防御体系模型与模拟仿真问题看作一个具有模糊特性、神经网络特性的多点、多层、多因素开放式复杂巨系统模拟仿真问题,提出了安全状态模拟7项研究方面，评估方法8项研究内容的研究方案。把为国家相关部门提供战略级电力网络安全预警、评估方法与手段，为军委总部机关提供战略级电力网络防御能力评估方法与手段，为提高国家战略信息战能力评估奠定基础作为研究基本着眼点。其对国家安全及国民经济发展具有重大战略意义。是刻不容缓的重要任务。
基于合成基准测试程序的多核处理器模拟技术研究	多核;模拟;微体系结构;合成程序	处理器微体系结构模拟是现代处理器设计中不可缺少的重要环节。然而在模拟器上运行基准性能测试程序（如SPEC CPU2006）需要极长的时间。为此，研究人员进行了大量的研究，但这些研究大都集中在如何在基准测试程序的动态指令流中选取部分指令进行详细模拟，其他指令进行功能模拟或功能预热方式运行的方法上，他们能够取得明显的模拟时间减少，但无法满足多核体系结构模拟的要求。本课题提出基于合成基准测试程序的多核模拟技术是解决上述问题的一个新途径。主要的思想是合成一个比SPEC CPU2006小得多但能代表它的性能特征的程序，从而极大地减少模拟时间。另外，能耗和发热模拟也是目前微体系结构评估的难点问题，主要体现在模拟精度低和操作复杂等方面，本课题将设计一套自动生成合成测试程序的框架和算法以解决上述问题。
异构多核片上系统自适应实时任务调度机制及算法研究	实时;任务调度;自适应;MPSoC;异构	低功耗实时任务调度是异构多核片上系统（MPSoC）面向实际应用的核心支撑技术。本项目以无人驾驶汽车应用为背景，针对制约异构MPSoC推广的关键的任务调度问题进行研究，分析计算任务与异构平台之间的相互关系，研究异构MPSoC平台建模的理论、技术和方法，综合在线和离线调度机制，建立多核任务调度模型，将处理器及任务的实时反馈加入调度算法之中，考虑任务优先级及其在不同内核上运用的性能、功耗差异，设计低功耗实时自适应异构MPSoC任务调度算法，对整体调度框架进行仿真验证和实测评估。通过本项目研究，可望在理论上揭示异构多核任务调度中多目标优化的瓶颈和新的解决思路，阐明任务调度策略与系统平台之间的相互关系，建立低功耗实时任务调度框架；在应用上为面向特定需求的异构多核任务调度问题提供系统解决方案，促进异构MPSoC在嵌入式系统中的推广应用。
嵌入式可信网络关键技术研究	可信计算;软硬件协同设计;嵌入式系统;可信网络;可信密码模块	可信网络（Trusted Network）是可信计算平台与网络接入控制机制的结合，引起了工业界和学术界的广泛关注。其核心问题是如何将信任链从终端扩展到网络，使得网络成为一种可信的计算环境。本项目拟采用软硬件协同的方法，自底向上开展研究，构建基于可信密码模块的嵌入式系统硬件可信平台；探究高可信嵌入式操作系统模型，对可信硬件平台进行管理，并向应用程序提供可信评估服务和可信连接服务；设计基于IPv6和IP Sec协议的高可信嵌入式TCP/IP协议栈；研制高可信电网信息智能感知终端与嵌入式可信网络原型系统。本项目将提出一套完整的嵌入式可信系统解决方案，为改善国家安全攸关的可信平台及可信网络的研发提供科学支撑。
多维气候大数据存储与处理关键技术研究	数据索引;多维数组;大数据;Hadoop;查询处理	科学仪器设备的不断发展和研究方法的持续进步促使气候变化的研究步入了大数据时代，给科学研究自身以及数据的存储和处理都带来了巨大的挑战。项目面向实际需求，针对当前气候大数据中多维数组存储和处理所存在的查询效率低、索引开销大、导入转换时间长、空间占用多等不足，结合开源的Hadoop、Impala等分布式存储和处理框架，探究针对查询处理的多维数组的高效存储方法、多维数组数据集的自适应分布式索引技术、面向多维数组的SQL查询支持与优化等关键理论和技术，在此基础上，进一步探究系统的实现技术和评估方法，开发完成一个多维气候大数据的云存储和查询平台，并进行实际应用评估。通过上述工作，为科学研究领域大数据的存储和处理提供良好的参考和借鉴，推动大数据研究和应用的进一步发展乃至科研的进步。
重金属材料第一原理模拟的并行算法研究	平面波;第一原理;重金属;Kohn--Sham方程;特征值和特征向量	基于密度泛函理论的第一原理方法是当今材料科学不可或缺的理论研究手段，Kohn--Sham方程特征值解法器是其中的计算核心。重金属材料第一原理模拟的计算量可达E级，对Kohn--Sham方程解法器提出了苛刻的性能要求。平面波方法是求解Kohn--Sham方程最成熟的数值方法之一。然而，一般的平面波方法难以扩展到数千处理器核，不能满足重金属材料高精度数值模拟的要求，亟需发展新方法、新技术改变现状。本项目面向数万核上的重金属材料第一原理模拟，从权衡负载平衡与通信开销的角度研究具有良好并行扩展能力的平面波算法，并结合多特征向量并行迭代发展Kohn--Sham方程的二级并行算法，研制高效并行的Kohn--Sham方程解法器。在此基础上，本项目瞄准若干典型重金属的物性研究，支撑上千个重金属原子的第一原分子动力学模拟扩展到数万核。
面向云存储系统的硬盘故障预测方法研究	系统可靠性评价;新评价指标;硬盘剩余寿命预测;云存储系统;主动容错	作为大数据和云计算的基础支撑设施，云存储系统规模不断增大，引发系统中硬盘故障频发问题。硬盘故障预测通过对危险数据的提前预警和迁移，能够避免或减少故障带来的损失，保障系统的可靠性和可用性。然而，现有预测方法存在一些缺陷，未能真正应用于云存储中。因此，本项目拟面向大规模云存储系统，设计适合实际应用的硬盘故障预测方法，包括：（1）通过挖掘故障预测对云存储的影响，提出基于"数据保护"和"资源消耗"的硬盘故障预测模型评价指标，直接反映预测模型对系统性能的影响；（2）围绕新评价指标，研究GBRT硬盘剩余寿命预测模型和优化算法，量化指导预警处理；（3）采用组合分析、蒙特卡洛仿真和概率分析等方法，构造主动容错云存储系统的静态/动态可靠性评价模型，指导系统创建和预警处理。三者相互融合，弥补现有预测方法的缺陷，促进硬盘故障预测技术在云存储系统中的应用，从根本上提高系统可靠性和可用性，降低系统成本。
高层处理器设计流程中延时错误建立及传输机制的研究	可靠性模型;可靠性分析;故障预测	随着集成电路产业的迅速发展，可靠性与容错技术已经成为芯片领域的重要问题之一。现今容错技术的探索依赖于在芯片仿真阶段的大规模统计错误注入实验，然而基于该类实验的可靠性分析有三点缺陷：第一，缺乏对错误产生机制的模拟；第二，缺乏对错误在芯片中传输机制的分析；第三，注错实验通常建立于寄存器传输级及其以下级别，无法快速对错误在高级别的影响进行估计。为解决上述问题，本项目首先通过分析纳米级别芯片中延时错误的产生机制，运用动态时序分析技术进行有物理依据的延时错误注入实验。其次，本项目通过研究芯片中逻辑单元的湮没效应对错误的传输进行动态追踪，从而减少统计注错实验的巨大成本。其三，本项目将错误建立与传输分析集成于高级别处理器设计流程，从而在芯片仿真阶段进行快速故障预测。本项目的意义在于建立一套在芯片高层次设计阶段进行准确可靠性分析的机制，从而辅助设计人员探索针对体系结构及应用程序的容错技术。
多核多线程DSP适应性存储结构研究	多核多线程;高性能DSP;集成电路;存储结构	嵌入式应用和市场的迅速发展迫切需要更高性能的DSP，多核多线程DSP将成为未来DSP的主流结构。实现高效且适应性好的并行数据访问与存储结构已成为未来多核多线程DSP发展的关键。本项目将构建一种新的适应性片上存储结构，能够同时高效地支持流数据、向量数据以及标量数据的存储与访问，满足不同数据流特征的应用程序对高数据吞吐率的需求，缓解"存储墙"问题对多核多线程DSP性能的限制，有效平衡存储效率和适应性，为研制新一代超高性能DSP芯片探索新的结构和技术，具有重要的理论意义和应用价值。.    主要研究内容包括：1）适应性共享存储结构，包括基于微编程的共享存储管理方法、适应性可扩展环形协同Cache结构和适应性数据传递方法；2）适应性片上缓存优化技术，包括适应性数据流控机制、公平性策略和适应性线程动态交换方法；3）适应矩阵操作的并行数据访问模式。
嵌入式图编程平台中公共化构件的构造方法研究	生成系统共享;构件拆解;图编程;构件组装	本项目以构件技术在嵌入式图编程方法中的应用为背景，以归纳构件共性化问题为切入点，构造可公共化的微小构件为元构件，探索不同的图编程平台共用同一软件自动生成系统的方法。研究拟通过1、研究适合元构件及领域构件描述需要的手段和构造方法；2、建立基于元构件的领域构件组装规则，解决构件可组装性验证和正确性证明；3、寻找一种基于领域构件的非规范描述向以元构件为基础的规范描述的转换方法，重点解决转换过程中的构件拆解，论证拆解前后的语义一致性。该研究的结果将为跨领域的嵌入式图编程平台的构件归一化生成模式提供依据，有助于促进各领域软件自动生成中图编程平台的应用，进而提高嵌入式软件的可靠性和开发效率。
随机门设计及其体系结构在机器学习中的应用	随机计算;图模型;蒙特卡罗采样;随机门;并行计算	许多机器学习和推理问题的一个特点是存在着不确定性。在这种不确定的环境下对推理问题的解决依赖于将它转换为概率性的模型,并求解概率分布。为了降低计算的复杂度,机器学习和推理问题的解决现在多依赖于基于图模型的采样算法。进一步提高这一算法的运行性能将提升许多学习和推理问题的求解速度,增大计算机能够处理的问题的规模。然而,利用传统处理器来实现这种采样算法具有局限性。主要原因在于:1)图模型采样算法的随机性和硬件的确定性之间存在着一定的矛盾;2)基于传统处理器的实现不利于发挥算法所具备的并行性。在本项目中,我们将探索一种更高效的硬件平台来实现基于图模型的采样算法。针对算法需要以任意概率分布产生随机输出的特点,我们提出设计一种称为"随机门"的数字电路来更自然地产生随机输出。进一步,我们将探索如何设计基于随机门的可重构的体系结构来实现这一算法,以充分利用算法具有的并行性,并能够对任意的图进行采样.
拓展众核有效并行源的微线程式软硬件支持机制	线程划分;并行编程模型;数据依赖;主动同步;众核体系结构	如何在众核结构上实现主流应用软件可并行性的透明扩展，是当前研究中的热点问题。针对目前众核加速串行程序均着眼于"大"热点循环结构而有效并行源不足的现状，本项目提出一种微线程式软硬件支持方法，通过拓展"小"热点循环结构为新有效并行源，使"微"线程划分深入到基本块内部，将计算和访存依赖不规则的串行程序划分成符合众核体系结构特征的微线程独立并行执行。主要研究内容包括：两层化微线程式串行程序表示形式的编译支持、拓展有效并行源的众核微线程划分方法、主动式微线程间同步通信的软硬件支持和符合微线程并行执行特征的动态资源分配机制。本项目的预期目标是提出一种拓展众核有效并行源的微线程式软硬件支持机制,平衡并行编程的易编程性和并行程序的执行效率，改善众核研究当前面临的有效并行源不足、片上资源有效利用率不高和负载不平衡等问题。对于探索2017-2025前后高效能通用微处理芯片体系结构的发展道路具有重要意义。
面向大规模并行计算机系统的应用级检查点关键技术研究	应用级检查点;可靠性;大规模并行计算机系统;容错开销优化	随着并行计算机系统规模的增加，系统的平均无故障时间远低于许多大规模科学计算程序的运行时间，系统可靠性问题已成为大规模并行计算机系统发展的重大挑战之一。应用级检查点技术是应对这一挑战的关键技术之一。为了充分发挥应用级检查点技术的性能优势，应用级检查点的容错开销优化已成为并行计算机系统容错技术的研究热点。本课题基于大规模并行计算的特点，展开面向大规模系统的多项应用级检查点容错开销优化的关键技术研究。这些关键技术的研究内容包括以下三项：面向数组的状态保存开销优化、快速高效的故障恢复以及多检查点的优化设置。本课题的目的是通过对上述关键技术的研究，有效降低应用级检查点技术的容错开销，使之能够更加有效地解决大规模并行计算机系统的可靠性问题。
多形态嵌入式系统智能化实时服务模型及实现方法	泛环境自适应;实时服务模型;能力演化;智能化软件;多形态嵌入式系统	继网络化发展，智能化已成为当代嵌入式计算技术的又一重要趋势。自主运行于动态环境中的应用系统，其应用行为过程、系统运行模式及平台资源构型均呈现出形态多样化、环境自适应的特征，这对传统嵌入式系统设计模型与方法提出了新的挑战。为此，本课题将重点围绕多形态嵌入式系统的智能化实时服务设计方法展开研究，着力解决嵌入式智能化服务软件结构、多形态实时服务统一模型、泛环境感知的可演化智能实时服务机制以及运行时模态自主决策等关键问题，并力求在多形态智能计算模型、可演化智能实时服务机制等方面取得创新成果。进而，设计、形成智能化实时服务软件原型及其部署规范，为多形态嵌入式应用的智能化软件设计提供方法及能力支撑。在上述研究基础上，研究者将首先把所研究方法与机制部署在新型无人装置的多形态嵌入式系统中，通过理论分析、综合仿真等方法对智能化实时服务能力进行验证。
基于闪存的数据缓存关键技术研究	缓冲区管理;固态硬盘;闪存;置换;磁盘	虽然闪存以其小巧、轻便、无机械延迟等特点被认为有望替代磁盘作为未来的二级存储设备，但是基于闪存的数据管理技术远未发展成熟。在未来相当长一段时间内，根据不同应用的特点和要求，实际中将出现各种基于闪存的存储系统。在这些系统中，缓冲区是其核心组件，缓冲区置换算法的高效与否直接决定了系统的效率。虽然研究者已经提出了各种基于闪存的缓冲区置换策略，但其基本假设是闪存的随机读代价远小于随机写代价。这一假设和实际不符，实际中不但单一闪存存在读写代价不对称的问题，而且不同型号闪存读写不对称的程度之间存在巨大差异性。本课题基于这一背景，重点研究基于闪存的缓冲区管理中涉及的关键技术问题，包括单介质、多介质、多级缓冲区管理机制以及混合式系统中的数据放置策略。本项目的研究有助于系统阐明在各种基于闪存的存储系统中的自适应置换策略、相关算法及模型，具有重要的理论意义和应用价值。
复杂流动的格子Boltzmann建模与计算机仿真	格子Boltzmann建模;高性能计算;计算机仿真;复杂流动	流体计算是具有挑战性的研究领域，是联系基础计算方法和软件实现技术到具体重大实际应用问题的一个重要纽带和桥梁。本项目以复杂流动和物理中的非线性问题为背景，直面湍流、非线性及高维问题，从"微观"机制出发，综合运用演化数学思想和自然演化规律，以高性能计算为手段，探索复杂流动的演化规律和建模方法。拟以与高性能计算机匹配的"自底向上"的建模方法：格子Boltzmann（LB）方法为重点展开研究工作。探讨LB方法的普适的演化机制和高效的建模方法；建立通用、高效和稳定性好的LB模型和算法以及与之匹配的边界处理格式；在进一步完善和发展基本LB方法的基础上，将LB方法扩展至热流动、磁流体力学和非线性动力系统等领域；研发实用、高效的算法软件，并应用于典型流动的模拟和仿真。研究结果不但对探索复杂流动的演化规律和建模方法有重要的学术意义和广泛的应用前景，而且对非线性科学及复杂系统研究有借鉴作用。
安全多方量子计算基础协议的研究与应用	安全多方计算;量子信息;体系和系统;基础协议;无条件安全	量子信息技术与安全多方计算技术分别是物理学与计算机科学中的热门研究领域，二者的结合产生了更新的研究热点- - 安全多方量子计算。相对经典环境下的安全多方计算协议，安全多方量子计算协议在安全性、健壮性、通信效率等方面都有很大的提高，特别是窃听检测方面，后者更是前者望尘莫及的。.有鉴于此，本课题的主要研究内容包括：1）系统研究安全多方计算原子协议的特性，以原子协议为基础构造更复杂的协议和应用；2）深入研究量子计算和量子信息技术，特别是与构建安全多方量子计算基础协议相关的技术；3）构建安全多方量子计算的协议体系和系统，并探索其应用的可能性；4）研究量子环境下关于安全协议的可靠、完备的证明方式，特别是对于协议的抗攻击性与抗窃听的分析与证明。
虚拟集群Live迁移关键技术研究	Live迁移;虚拟机;并行作业;虚拟集群;虚拟网络	随着虚拟网络和虚拟机技术的结合，用于运行大规模并行或分布式应用的虚拟计算集群技术，作为新的高性能计算环境，逐渐得到人们的关注。但是，分布式共享环境中资源的动态性、自治性和资源竞争往往会影响虚拟集群所在物理主机的可用性。而虚拟集群的动态Live迁移技术则是保证分布式应用服务能够高效、正确执行的关键。拟申请课题将研究虚拟集群Live迁移的关键技术，使虚拟计算集群在运行过程中能够动态绑定更佳的物理宿主机、动态优化虚拟网络的拓扑结构，更好的保证用户对计算性能的要求，并保证迁移过程对并行应用程序的透明性。虚拟机迁移技术的研究日益成为国际上新的研究热点，但当前的研究工作大多局限于单个虚拟机和简单网络应用的迁移。针对集群整体计算环境和支持分布并行应用的虚拟集群迁移技术的研究还处于起步阶段，因此该项研究具有重要的理论和应用价值。
一类CPS中基于模糊理论的系统建模方法研究	可组合性;系统建模;自适应性;模糊理论;信息-物理融合系统	支持离散-连续动态性能分析及二者间交互的系统模型数学理论是目前信息-物理融合系统(CPS)的应用研究领域尚需探明与深入研究的关键理论基础之一。本项目以车用开关磁阻电机(SRM)驱动设备与其车载计算控制单元(SRM-ECU)二者间的相互影响与交互为应用对象，将模糊系统和模糊控制引入CPS建模，开展一类CPS中基于模糊理论的系统建模方法研究。提出离散-连续混合的新型自适应模糊系统模型，发展其稳定性理论，解决CPS的异构模型组合问题；构建基于模糊理论的分层智能控制系统结构，发展模糊系统的智能控制方法，建立预测式自适应输出反馈混合控制算法，解决CPS的自适应性问题；研究用于系统安全验证的系统状态稳定区域量化计算和可达性分析与综合的方法。揭示信息组件通过网络控制物理过程的动态融合反馈机制，为CPS的应用领域奠定理论基础。最后，以实验室电动汽车平台为基础展开CPS系统的仿真与物理实验，验证其理论结果。
混合式瓦记录磁盘阵列系统理论、结构和实现技术研究	磁盘阵列;固态硬盘;缓存管理;瓦记录	针对瓦记录磁盘技术存在的问题，提出了一种融合固态盘和瓦记录磁盘的新型混合式磁盘阵列系统（简称SS-RAID），其研究内容包括四个方面：1).设计一种高性价比混合式瓦记录磁盘阵列系统方案。采用固态盘充当瓦记录磁盘阵列缓存，利用瓦记录磁盘上的空闲空间组成逻辑磁盘阵列，采用日志写方式镜像固态盘缓存中的脏数据，兼顾了固态盘的高性能和瓦记录磁盘的低成本；2).提出新颖的RAID写优化方法，利用固态盘中的无效旧数据，以减少RAID5/6更新校验时对磁盘的读访问，加速SS-RAID数据写过程；3).设计新的"区-段-块-扇区"瓦记录磁盘四级编址方式。一方面，采用对称堆叠式分区以提高空间利用率；另一方面，采用径向分段方法以降低写覆盖；4).设计高效的"段-块"结构缓存管理算法，以进一步优化SS-RAID性能。本项目的研究成果为瓦记录磁盘的广泛应用提供理论基础和技术支撑，具有重要的理论和经济价值。
基于三维堆叠NVM-DRAM混合介质的内存大数据处理体系结构关键技术研究	面向大数据处理的计算机系统结构;大数据可定制加速计算系统;并行计算模型	大数据应用中有近一半的任务需要实时处理，但基于磁盘的处理系统由于I/O性能低下而难以满足要求。内存计算模式为解决这个问题带来了新的希望。但内存计算对计算机系统结构也构成了新的挑战。主要体现在：（1）当前普遍采用的DRAM内存由于其在存储密度、能耗、和可扩展性等方面的限制使其难以跟上数据增长的步伐；(2) 现代处理器体系结构设计没有考虑内存大数据程序的特点，导致处理器资源利用率不高且性能也不高的局面。为此，本课题根据对已有大数据处理程序在体系结构层次特征的分析，提出基于三维堆叠NVM-DRAM混合介质的新型体系结构。主要研究内容为：（1）三维堆叠NVM-DRAM混合介质环境下访存行为理论模型；（2）处理器总体架构；（3）能效模型；（4）运行时系统。课题期望通过对这四个内容的研究，解决内存计算在体系结构层次所面临的挑战。
云环境隐私泄露恶意行为应急响应关键技术	恶意行为;云计算;计算机系统安全;隐私泄露;应急响应	课题研究云计算环境隐私泄露恶意行为应急响应技术，对云计算的可持续发展将提供重要保障。针对云计算环境联动监控体系、隐私泄露可疑行为特征分析与检测、云环境隐私泄露恶意行为快速定位与评估、隐私泄露恶意攻击免疫技术四个关键技术问题，课题首先从安全领域、安全威胁和安全隐患三个方面系统分析云计算的安全差距问题；然后重点从云计算服务器探针部署、基于核心节点信誉证书的联动监控体系、基于恶意代码样本空间关系特征的云计算恶意行为检测方法、隐私泄露行为定位和现场处理技术、虚拟机自省技术、基于交叉熵的恶意代码评估技术、基于树突细胞免疫的网络恶意代码免疫技术等方面，研究云计算环境隐私泄露恶意行为主动响应机制。最后实现云环境中隐私泄露恶意行为监控示范验证系统。从而解决云计算隐私泄露恶意行为应急处置的完整过程的关键技术问题，提高我国在云计算环境中隐私泄露恶意行为主动防御上的整体水平，并带来很好的市场前景和经济效益。
效能驱动的光互连视频阵列处理器动态自重构体系结构	阵列处理器;动态自重构;三维集成;分布式存储;片内光互连	超高清视频编解码、计算机视觉等新应用、新算法层出不穷，计算量迅速飙升，功耗不断激增，急需兼顾计算高效性和应用灵活性的新型体系结构。首先，面向视频应用的多样化场景，针对CGRAs等重构结构灵活性不足、资源利用率低、性能提升困难等问题，基于三维邻接寻址的大规模轻核阵列结构，探索效能驱动的动态自重构机制。重点研究视频处理器通过"软件编程"动态实现"硬件重构"的新方法。其次，针对自重构阵列中PE内和PE间的高性能互连需求，尝试提出局部电交叉互连全局光多环嵌套互连的新型三维拓扑结构，解决"通信墙"问题。最后，着手建立统一寻址的分布式共享片上存储结构，解决视频处理器海量数据和配置信息的高效存取问题。预期突破可编程动态自重构、三维光电混合互连等新技术，建立适应纳米工艺发展的视频阵列处理体系结构及其原型系统，兼有软件编程的灵活性和专用硬件计算的高性能，为突破后摩尔时代处理器体系结构的效能瓶颈提供有益参考。
异构众核处理器的功耗分析及管理优化	任务调度;众核处理器;功耗谱线;异构;功耗管理	随着单片微处理器芯片的规模及集成处理器核数目的不断攀升，处理器体系结构正在朝着"众核"迈进。然而，趋势研究表明未来处理器的功耗需求的增长速率要远高于供电系统的功率供给增长速率。因此，如何有效的分配、管理有限的功耗以尽可能发挥处理器性能将是体系结构设计上的重要挑战。本项目针对这一挑战，从以下两点寻求突破：（1）研究众核处理器的功耗特征谱线。谱线的分布体现了不同处理器结构的各个功耗状态的分布，而分布的范围大小反映了功耗调整的潜力；解决如何构造具有较强功耗可伸缩性的多核处理器的问题。（2）研究运行时的功耗管理策略。通过任务与处理器核的多样性构造不同的功耗状态，通过任务调度完成功耗状态的迁移；解决了传统基于动态电压、频率调整的功耗管理方法部署开销大的缺陷。本项目充分结合了申请人在低功耗设计及利用任务调度优化多核体系结构的研究基础，将为众核处理器功耗管理提出解决方案。
车载自组织网络数据安全聚合机理与方法研究	上下文;数据聚合;安全;车载自组织网络	车载自组织网络是未来智能交通的关键技术，有效数据分发是其应用实现的核心问题。本项目聚焦于车载自组网数据分发的数据聚合阶段，研究车载自组织网络安全数据聚合机理和方法。主要研究内容包括：数据过滤抽取方法；数据异常检测方法；数据安全聚合方法；数据安全聚合模型及机制。利用超图理论对车载自组网进行建模描述,基于子图挖掘研究数据过滤提取方法，抽取重要信息；基于逻辑推理理论研究数据异常检测方法，保证数据的正确性；基于启发式算法研究尺度可变的层次化数据聚合计算方法，利用通用可组合理论设计安全认证机制，并实现与聚合计算的融合，保证数据聚合的尺度、精度和数据的完整性和可认证性；建立通用的数据安全聚合模型及评价标准，设计面向应用的高效数据安全聚合机制。提出一套创新的车载自组织网络数据聚合理论、技术与方法。希望研究成果能够为车载自组网的大规模部署和应用研究提供理论和技术支撑。
基于业务感知的矿山物联网自治QoS保障机制研究	服务质量;物联网;自治网络;业务感知	矿山物联网技术为煤矿安全生产与预警救援新体系提供了新思路，但大流量多业务矿山物联网的服务质量保证是亟待解决的问题。针对矿山物联网服务提供的自适应需求及煤矿恶劣环境下的网络终端QoS保障问题，项目采用跨层认知设计思想，融入认知元素，引入自治控制环技术，建立满足矿山物联网业务需求的QoS体系结构模型；引入本体论对业务特征参数进行语义描述，利用粗糙集理论和语义距离方法进行多维、多粒度网络业务感知；采用Vague集描述QoS参数，提出基于证据理论ER算法的Vague集多QoS属性融合方法与QoS动态自配置机制，进行终端业务QoS优先级动态自适应调整；采用业务源端控制层面和分布式认知链路控制层面的端路协同控制策略，结合分数阶PID的主动队列管理方法进行矿山物联网QoS自适应控制，探索基于业务感知的矿山物联网服务QoS保障机制。建立基于业务感知的矿山物联网自治QoS系统，为实现感知矿山奠定基础。
面向认知网络的自律计算模型及评价方法研究	PEPA评价模型（PEPA;自律计算模型(Autonomic;Networks);Evaluation;MDE环(Monitor-Decide-Execute;认知网络(Cognitive;Computing;Model）;Circle);Model)	认知网络是一种具有自律特性的主动网络，代表着下一代互联网络的发展方向，正成为国际上一个前沿且具有多学科交叉性质的热点研究领域。对于"认知网络是否具有自律性？"、"认知网络具有那些自律特征？"、"认知网络的自律程度如何？"等工作是认知网络研究亟需解决的问题。然而，现有文献仅从认知网络定义、框架结构等展开，缺乏理论模型、定量评价等基础性工作，难以有效地引导研究的开展。基于此，本课题提出"四个一"的研究目标，提出一种基于多级反馈MDE环结构的认知网络自律计算模型；构建一套自律性综合评价指标体系；提出一种基于性能评估进程代数的形式化评价方法；搭建一个支撑评价验证的仿真环境。承接前期在网络感知和自律计算方向的研究基础实现"网络感知→自律计算→认知网络"研究的升华和延续，最终形成一条完整的学术研究链。课题的开展将为认知网络进一步研究奠定基础，为深化和完善认知网络研究提供重要的理论依据和方法参考。
阵列化极宽频无线通信处理的关键技术研究	可伸缩处理;阵列化处理;极宽频通信;并行计算;频谱感知	无线网络通信性能的提升，一直是学术界和产业界广泛关注的焦点问题。根据香农定理，无线信道的通信带宽受限于其频宽和信道上的信噪比。当前基于OFDM、MIMO等技术开展的研究工作，大多是致力于以提高信道信噪比、优化传输效率的方式提高无线通信性能，其性能提升空间日趋有限。本项申请从突破固定频宽对无线信道通信性能的限制入手，探索利用可扩展频宽的无线信道以大幅提升无线通信性能的新机制，设计可扩展的无线通信框架，为新一代无线通信的发展提供了创新思路。针对当前无线通信终端只能在固定频宽条件下工作的局限性，我们创新性地提出了利用固定、有限频宽的无线通信终端实现可扩展频宽的无线信号接收技术，给出了极宽频无线信号接收的技术方案；针对极宽频无线信号的高性能计算需求，设计了阵列化、可扩展的计算框架。本项申请的预期研究成果，有望为高性能无线通信提供原始创新性的技术突破，具有重要的理论意义和应用前景。
智能制造物联网络自组织弹性增强策略研究	智能制造;网络弹性;服务弹性;自组织	智能制造物联网络将生产设备、人和智能产品连接在一起，持续进行数据采集、分析和控制，提供端到端的数据流和控制流服务，具有大规模、异构和动态演化的特征，常态化的组件随机失效和针对工业系统的攻击已经成为最严重的国家安全挑战之一。.      本项目以提升智能制造物联网络的承受能力、吸收能力和恢复能力为目标，研究利用自组织特性增强其网络和服务弹性的相关理论和关键技术。建立智能制造物联网络异构多域自组织弹性模型；研究网络和服务多维弹性度量和评价方法；研究高效快速自组织弹性增强策略。本项目在智能制造物联网络和服务弹性度量和评价方法及利用网络自组织特性增强服务弹性的策略上将产生创新成果。.      项目成果对于保障智能制造系统安全、对于我国智能制造2025战略的实现具有重要的现实意义。
面向多核/众核体系结构的评估方法研究	评估方法;模拟模型;加速;可扩展;可信评估	由于存储墙和功耗墙的限制以及应用对计算需求的持续增加，多核乃至众核已成为各种处理器的主流设计。而随着处理器产品竞争压力的不断加剧，对产品的设计周期和设计的有效性提出了越来越高的要求。在处理器设计过程中，设计进度和设计有效性直接受制于评估方法的有效性。然而，现有体系结构评估方法在高效性、可扩展性和可信性等方面面临诸多挑战，无法满足多核/众核处理器设计对评估方法的需求。.    针对体系结构评估方法面临的挑战，本课题从评估方法的两个关键环节(模拟评估平台的有效性和结果分析的可信性)着手展开研究。针对已有模拟评估平台使用的传统模拟模型性能和可扩展性存在的限制，开展新型模拟模型以及相关可扩展和加速技术研究；结合并行应用执行不确定对评估准确性造成的挑战，研究可信的评估方法。研究成果有望解决体系结构评估方法的基础问题，对提升多核/众核体系结构评估方法的高效性、可扩展性和可信性将具有普遍的适用意义。
面向弹载微波前视三维成像制导的异构可重构计算研究	弹载微波前视成像;导引头;异构可重构计算	弹载雷达前视成像是实现导弹精确制导亟需解决的关键问题之一，在军事领域有着重大需求。随着新体制雷达的不断发展，弹载前视成像算法的研究逐渐深入。然而在资源受限的弹载平台，如何在确保设备小负荷、低功耗的前提下实现高计算复杂度的前视成像实时处理，这方面的研究仍非常欠缺。为了突破此瓶颈，本项目拟开展弹载异构可重构新型计算体系与方法研究。本项目的主要研究内容包括：1）复杂运动条件下弹载前视计算成像算法实时优化与核心子电路IP设计；2）面向前视成像制导的弹载异构体系软硬件协同设计与验证；3）基于自主任务规划的动态可重构策略和低功耗与容错设计。本课题将异构可重构计算体系引入弹载信号处理平台，重点突破弹载微波前视成像算法实时优化、异构体系软硬件任务调度以及可重构资源高效配置等基础问题，为促进雷达导引头前视成像方法完善以及高能效计算体系结构的发展提供理论依据和技术支撑，具有重要的研究意义和实用价值。
网络视频定向广告关键技术研究	定向广告;对等网络;隐私保护;网络视频	定向广告在视频服务领域的应用对网络视频的推广和整个行业发展至关重要。现有网络视频服务的定向广告系统通过服务器端收集用户个人隐私信息来实现广告内容的定向投放，无法从根本上解决网络广告定向投放和用户隐私保护之间的矛盾，存在投放内容缺乏针对性和影响用户的视频观看体验等问题。本课题从问题的根本矛盾入手，变传统以服务器为关联中心的集中式系统模型为以客户端为关联中心的分布式系统模型，让用户来创建和管理自己的个人隐私数据，将广告内容预先推送到客户端，从而在客户端来实现广告的定向关联，使得网络视频服务的定向广告系统可在不侵犯用户隐私的前提下实现广告和用户之间更准确的关联。以此为基础，本课题进一步研究新模型下定向广告的数据分发策略、定向关联算法、以及显示时机检测等关键问题，其研究成果具有重要的理论意义和应用价值。
面向多核异构平台的协同设计虚拟机关键技术研究	多核;软硬件协同设计;动态二进制翻译;协处理器;虚拟化	计算系统虚拟化技术隔离软硬件支撑环境之间的紧耦合性，实现按需构建计算系统虚拟映像，展示了新的计算机理与计算模式。课题结合当前计算系统多核化、异构化的发展趋势，深入研究面向多核异构平台的协同设计虚拟机关键技术，通过将实现异构平台兼容性的动态二进制翻译与支持多虚拟机环境的VMM有效融合，实现多核异构平台下的计算系统虚拟化，并通过软硬件协同设计的方式提高虚拟化系统的性能。课题研究内容包括：虚拟化计算系统以及协同设计虚拟机运行机制分析、协同设计虚拟机支撑理论、支持多虚拟机环境的协同设计虚拟机体系结构、高效动态二进制翻译技术、提高虚拟机执行效率的软硬件协同设计方法、应用程序目标代码在多核平台上的并行化方法以及支持虚拟化的异构多核处理器体系结构等，本课题的研究成果能够为实现基于多核异构平台的高效透明虚拟化系统提供一种有效的解决途径，具有重要的理论意义和实用价值。
基于PRAM的主存储器在虚拟化环境下的关键技术研究	老化均衡;生命周期;可靠性;主存储器;相变存储器	相变存储器（PRAM）可有效缓减数据中心的内存容量和功耗问题，研究相变主存储器在数据中心虚拟化环境下的关键问题和关键技术具有重要意义。生命周期和访存性能优化是相变主存储器设计研究的重点，而虚拟化技术对此提出了严峻的挑战：1）虚拟化技术破坏了程序在物理内存访存中的统计性规律；2）虚实空间的划分导致了已有内存调度方法的局限性；3）虚拟化技术的公平性和隔离性需求严重增加了内存优化方法的设计维度和问题复杂度。针对以上问题，本项目拟研究：1）多层次融合性访存行为的特征分离与提取方法，以提高访存行为预测的准确率；2）兼顾空间全局性与虚拟机隔离性的内存调度方法，为相变主存优化设计方法提供高效的支撑技术；3）结构相关的多维度相变内存优化方法，根据内存系统的硬件结构进行相应的生命周期、访存性能、和虚拟机公平性的协同优化方法设计。本项目拟搭建1套模拟实验平台，发表4-5篇SCI/EI检索文章，申请专利3-5项
基于DNA自组装并行计算模型的二元域圆锥曲线密码编码机理研究	圆锥曲线密码;分子计算;二元域;自组装模型	基于DNA计算的密码技术是信息安全领域未来几年内的热点和重点研究课题。如何利用DNA计算来实现二元域密码还是一个开放性的问题，如何有效解决"DNA密码并行计算模型计算复杂度高的问题以及计算参数长度不匹配的问题"是DNA密码技术进一步发展的关键技术和首要前提。本项目拟从国内外相关研究中存在的问题入手，深入分析DNA自组装模型的特性，研究二元域基本运算按比特位移位进行自组装的规律，揭示二元域运算的DNA自组装模型编码特征，以此为基础研究二元域圆锥曲线密码各种点操作的编码机理，提出以单一比特位为基本计算参数的二元域圆锥曲线密码低复杂度并行计算模型，拟采用Matlab工具箱对计算过程进行模拟验证。DNA密码技术属于并行计算、信息安全和生物数学等领域的交叉学科，本项目的研究旨在丰富这一新兴科学领域的研究内涵，探讨增强、改进和创新DNA密码技术的新理论和新方法。
多租户多数据中心的容器云平台虚拟资源调度研究	深度强化学习;容器云平台;协同;自适应;资源优化调度	以容器为新型虚拟化单元的大规模多租户多数据中心云平台中的主要两类虚拟资源——计算资源和网络资源之间存在综错复杂的非线性动态不确定关系，客观上要求两者以协同方式进行自适应配置。本项目利用高性能机器学习方法和多agent技术，以容器簇为粒度，研究多租户多数据中心的云服务环境下虚拟计算资源和网络资源协同自适应调度的理论问题、关键技术和方法，以确保在满足用户服务等级协议前提下，最大程度实现云服务供需双方的利益均衡。主要研究内容包括：分别从虚拟化容器技术下多数据中心计算资源和网络资源两个层面研究满足多租户服务质量需求的自适应调度机制，包括数据中心、容器簇和服务器的选择，以及容器云平台计算资源和网络资源的自适应调度机制；研究多租户多数据中心环境下容器云平台计算资源与网络资源协同自适应调度机制，包括高性能协同自适应调度算法、知识迁移机制等研究；构建原型验证系统，验证本项目所提的理论、技术和方法。
E级计算系统中基于SSD的高性能高可靠IO节点子系统关键技术研究	IO节点;输入输出子系统;高性能计算;固态盘;并行文件系统	在未来的E级高性能计算系统中，IO子系统将是重要的瓶颈之一，而引入IO节点则是提高IO性能的有效手段。本研究拟在E级高性能计算系统中引入基于固态盘的IO节点系统。在此子系统中存在三方面的问题：（1）固态盘寿命有限，基于SSD的IO节点系统可靠性问题尤其突出；（2）固态盘能够提供空前的性能，但现有软件系统不能充分发挥基于SSD的IO节点系统的优势；（3）简单引入IO节点系统并不能完全消除后端存储系统的性能瓶颈。相应地，本项目从三个方面展开研究：基于固态盘的IO节点系统高可靠体系结构、面向高性能IO节点系统的数据并发访问技术、面向后端存储系统的IO节点系统优化技术。研究的成果将确保基于固态盘的IO节点系统稳定可靠和性能充分发挥，相关技术将应用到未来的E级高性能计算系统中。
实时分布式流处理系统资源动态调度问题的研究	实时;云平台;软件定义网络;分布式流数据处理;资源调度	大数据时代的到来带来了各种新的挑战，而越来越多的大数据处理应用对数据分析、统计和计算结果的产生提出了实时性的需求。因此，具有良好的可扩展性和灵活性的分布式流处理系统架构，正在不断获得更多实时大数据应用开发者的青睐。对分布式流处理系统而言，资源的动态调度问题是影响工作性能的关键因素，由于其难度大、复杂度高，至今未能有效解决。本研究项目针对两种重要资源的调度问题分别提出了新的解决思路和方法。针对计算资源，通过建立性能分析模型并结合凸优化理论的思路，考虑允许有多个应用同时部署的情况下，设计出能使计算资源利用率最大化的动态调度算法。针对网络资源，将软件定义网络（SDN）这一新的技术和工具运用在以"端到端连接"为单位的网络带宽资源调度问题上，从而使由网络传输造成的数据处理延时降到最低。
可重构硬实时嵌入式系统中能量有效的任务放置方法研究	可重构硬实时嵌入式系统;静态节能放置;动态节能放置	本项目以能量有限的可重构硬实时嵌入式系统为研究对象，在系统使用固定优先级调度方式时，研究能量有效的任务放置方法。在满足硬实时要求的前提下，静态节能放置和动态节能放置相结合，从而有效地减少了系统的能量消耗。该研究包括任务模型建模方法；并行执行和混合抢占模式下的任务响应时间分析方法；任务生成时能量有效的静态放置方法；和运行时在线决定任务执行位置，以获得能量有效的动态放置的方法。该研究为设计能量有效的可重构硬实时嵌入式系统提供了理论成果、算法和实现技术。
多权限组播通信中群组密钥管理问题的研究	基于身份的加密;多权限组播通信;多线性映射;密钥管理;多叉树	密钥管理技术是实现安全组播的核心技术。传统密钥管理策略不支持多权限组播。本项目研究多权限群组密钥管理问题，以满足多权限组播的安全需求。为了给多权限群组密钥管理提供安全的组播通信模型，本项目将扩展我们现已提出的RingNet模型，使其支持异构网络环境中的多权限组播。基于多权限组播通信模型，研究集中式多权限群组密钥管理方案，采用多叉树支持用户和数据流的动态变化，以适应异构网络环境；利用素数标识密钥节点，使用户能够快速确定需要更新的密钥，并利用中国剩余定理和单向函数更新密钥，以提高方案的可扩展性。为解决集中式方案中的"单点失效"问题，本项目研究分布式多权限群组密钥管理方案，采用多线性Diffie-Hellman假设实现高效的多方密钥协商，利用基于身份的加密技术提高密钥更新的效率。通过研究与开发原型系统，验证所提出方案的有效性。本项目的研究，将推动多权限组播在多个领域的应用，从而促进信息产业发展。
通信受限多个体网络的一致性和分布式优化研究	网络编码;群体智能;分布式计算;分布式系统	多个体网络中个体间的信息传递，通常会受到有限带宽、网络拓扑等因素影响。通信受限多个体网络的一致性与分布式优化主要关注如何设计个体的局部量化一致性协议，使所有个体形成共识并协同地解决关于整个网络的优化计算问题。本项目重点研究网络拓扑、量化信息与个体一致性协议间的关系，以及如何利用无线通信的广播特性来发展更符合实际的量化平均一致性算法，并应用于多个体分布式优化问题。具体地，项目考虑有向非平衡网络中个体如何与其邻居个体交换量化信息来调整状态，并分析量化一致性算法的收敛性；研究利用广播特性对有向网络设计有效的鲁棒量化平均一致性算法。还将考虑基于量化一致性的多个体分布式优化算法的研究，分析如何在量化通信受限下，所有个体达成一致性的同时并使关于整个网络的优化问题目标函数最优。本项目研究将有助于更好地理解多个体系统中个体状态在量化信息通信下的演化，以及量化对分布式优化算法性能的影响。
基于特性分析的大规模机器学习性能优化关键技术研究	性能优化技术;大规模分布式系统;并行编程模型;机器学习;密集迭代收敛性	大规模机器学习系统以其强大的学习能力在诸多应用领域取得了惊人成果，但机器学习算法的计算密集型和I/O密集型特点一直是机器学习向更高性能发展面临的挑战。本项目将针对并行机体系结构、计算资源扩展模式和机器学习应用特征的变化特点，研究密集迭代收敛型机器学习应用的并行计算模型，并构造性能模型，对模型的普适性、扩展性与容错性进行理论分析，进而针对模型的数据层、计算层和通信层，研究性能优化指导理论；在此基础上，围绕密集迭代收敛型机器学习算法特性，研究松散型并行调度策略，建立多迭代同步管理模型，通过连续通信策略、优先级设定策略、参数分时广播策略研究通信管理模型；突破传统的优化方法选择和优化时机限制，构建机器学习算法的运行时开销分析模型，研究资源动态管理方法，通过模型选择和资源组合策略，有效提高资源利用率以及系统并行性能。本项目研究成果可以很好地提高大规模机器学习的性能，对于大数据智能分析具有重要意义。
键值存储系统架构设计与性能优化研究	对象存储;数据布局;纠删码;大规模存储系统	随着非结构化数据的快速增长，传统的块存储和文件存储在可扩展性与性能方面均无法满足应用需求，因此键值存储受到广泛应用。当前主流的键值存储系统主要采用LSM-Tree结构，但该类系统也面临严重的读写放大问题，也即每写入/查询一个键值数据可能引发多次I/O，严重降低键值系统的读写性能。此外，当引入存储编码以提升系统可靠性时，校验数据的更新将进一步加剧写放大问题。鉴于此，该项目计划建立键值存储系统新型架构，以优化系统的读写性能与可靠性。特别地，我们计划采取key和value分离的思想，建立基于层组结构的LSM-Tree和基于日志结构的容器管理技术对key和value分别进行管理，以优化系统写性能；同时结合数据访问特征，建立双向LSM-Tree和异构布隆过滤器优化系统读性能；此外，我们引入存储编码并优化冗余数据管理，以同时提升系统性能与可靠性。最后，通过开发原型系统，测试并验证系统的性能。
应用行为感知的多核处理器存储资源管理关键技术研究	多核处理器;应用行为感知;资源划分;访存带宽;末级Cache	单核处理器中已有的"存储墙"问题在多核处理器中依然存在，甚至更加严重。存储系统仍将是影响计算机系统性能的瓶颈。除了"存储墙"问题外，多核处理器中，存储系统的设计还面临访存带宽有限、线程间访存冲突、应用行为多样等挑战，针对这些挑战研究多核处理器存储资源的管理技术对提升系统性能具有重要意义。本项目提出一种应用行为感知的多核处理器存储资源软硬件协同管理框架，依据应用的行为特征，由硬件和软件协同对多核处理器片上末级Cache(LLC)和到片外存储器的访存带宽进行高效、灵活的管理，重点突破应用行为的表征和在线提取、资源冲突预测模型构建、性能预测模型构建、访问冲突感知的LLC划分和预取请求感知的访存带宽划分等关键技术，使LLC和访存带宽得到有效的利用，以切实开发多核处理器的运算潜能。我们希望依托本项目取得的研究成果能够指导国产高性能微处理器的研发，并最终应用到国防科学技术大学自主研制的多核处理器中。
行为经济学在分布式系统合作性中的研究	行为经济学;前景理论;分布式系统;合作性	当前采用传统经济学理论的分布式合作保障机制，其经济学理论基础存在缺陷，使其不能很好地适应分布式应用需求。本课题引入行为经济学基本理论作为研究基础，归纳提炼行为经济学与分布式合作保障需求的共性知识，(1)建立行为经济学基本理论与分布式领域相关理论的映射规范：根据前景理论以新规则确定成本函数和价值函数，将公平性考量贯穿节点行为选择全过程；(2)充分肯定节点的非理性，并在此基础上构建基于行为经济学的分布式系统合作保障机制和相关数学模型；(3)借鉴行为经济学实验的参数设定，实验过程设计和实验数据分析方法，提出符合行为经济学田野调查规律和原理的分布式仿真算法，最终在实验平台编程实现并实际运行验证。本课题研究可解决传统经济学理论设定没有涵盖心理因素，并错误假设偏好稳定一致，而造成的理论模型与现实结论不匹配的问题，可实现行为经济学的计算机仿真和实现，为促进分布式系统中合作性的研究开辟了新的探索方法。
面向百核处理器Cache一致性协议的高效片上网络研究	Cache一致性协议;片上网络;百核处理器	半导体技术的发展不断增加芯片集成的核数，业界已进入百核处理器时代。片上网络较好地克服了传统总线互联结构的诸多不足，它已成为百核处理器事实上片内互联标准。另一方面，并行编程的高难度和兼容遗留代码的需求使得百核处理器依然采用cache 一致性协议，但百核处理器上的cache一致性协议面临着事务延迟上升、层次化结构、消息量剧增、多播和归约通信瓶颈诸多挑战。为缓解这些挑战，需要在分析一致性协议结构和通信特征的基础上优化设计片上网络。本课题主要研究高效支持百核处理器cache一致性协议的片上网络关键技术，包括低延迟的动态可重构拓扑结构、维持区域隔离的高性能路由算法、高效传输短报文的流控机制、多播和归约通信的硬件支持等。本课题的研究可以为百核处理器片内互联架构的设计与实现奠定坚实的理论和技术基础，具有重要的理论意义和应用价值。
面向数据中心负载的本地存储系统能效优化技术研究	任务;存储系统;数据组织方式;能效;负载	随着数据量爆炸式增长，数据中心存储系统的能效问题日益突出。当前数据中心主要运行数据密集型应用，在任务处理、数据组织和I/O负载等方面呈现出新的特征。然而，已有存储系统能效优化技术不能有效地适应新的特征。本项目拟研究（1）结合数据分布和存储设备特征的任务分解与调度策略，通过延缓部分非关键子任务的执行，延长磁盘处于休眠状态的时间；（2）感知负载特征的数据组织变换策略，依据不同阶段数据访问特征与服务质量需求的差异，按需调整数据组织，减少不必要的数据组织变换带来的I/O开销；（3）优化存储空间利用率和存储带宽利用率的数据分布策略，通过数据复制和写重定向聚合读写I/O，减少磁盘寻道开销并提升数据分布效率。上述三个层面的研究分别从任务处理、数据组织和I/O负载的角度，优化对存储系统性能和能耗有关键影响的I/O行为，在保证满足服务质量需求的前提下，达到提升数据中心存储系统性能和能效的目的。
基于片上网络的多核处理器片上存储系统优化技术	多核处理器;高速缓存;片上网络(NoC);存储墙	多核处理器是计算机系统结构发展的必然趋势，受到了学术界和工业界的热捧。但是多核处理器的处理速度和存储器的访问速度存在着很大的差距，导致了严重的存储墙问题，直接影响了多核处理器的性能。本项目拟利用多核处理器片上网络的全局管理优势，提出基于片上网络的多核处理器片上存储系统的优化策略和方法，提高片上Cache的利用率，最大限度地利用片上存储层次来减少片外存储器的访问次数，降低远程数据的访问延迟，旨在解决多核处理器的存储墙问题，进而提高多核处理器系统的整体性能。本项目拟采用软硬件协同的设计思想，具体研究Cache的网络迁移技术、Cache的网络复制技术、地址空间的网络映射技术、生命周期敏感的Cache管理技术、一致性协议的协同优化等关键技术。本项目的研究对于解决多核处理器的存储墙问题、提高多核处理器的整体性能具有重要理论和实践意义。
基于GPU性能模型的异构系统优化技术研究	异构系统调度方法;GPU性能模型;性能优化;循环优化;异构系统	基于GPU的异构系统已成为构建大规模并行计算机系统的有效途径之一。这种体系结构对程序性能优化研究提出新的挑战，需要深入研究与异构系统相适应的性能优化技术。.本项目首先基于GPU的体系结构，研究多级存储层次和并发流程序的GPU性能模型，然后基于GPU的性能模型，研究提高GPU异构系统计算效率的性能优化技术。从科学计算程序中最常见的循环出发，研究DOACROSS和DOALL两种循环在异构系统上的性能优化方法，解决循环分块算法中块大小的最优化问题；针对不同负载的循环迭代，研究其在异构系统上的调度方法，使循环负载可以均衡地在CPU和GPU间进行分配，发挥两者的协同计算能力，提高系统的整体计算效率。
面向基于图的数据挖掘的FPGA加速方法研究	现场可编程逻辑阵列;可容错计算;数据挖掘;高性能计算;体系结构	高效的"数据挖掘"方法在信息爆炸的"云计算"时代显得尤为重要。现场可编程门阵列（FPGA）是一种灵活的半定制电路，可以在流片以后用软件进行功能定制。尽管很多基于FPGA的"数据挖掘"加速算法在过去的五年中已经被提出，"基于图的数据挖掘"（一种在"知识发现"（KDD）领域的重要技术手段）的FPGA加速算法还没有被深入研究。本项目的核心研究内容是利用FPGA对"基于图的数据挖掘"问题进行硬件加速以及相关的基础科学技术问题。本项目的目标包括：针对"基于图的数据挖掘"问题建立一套高性能、高能效的FPGA的加速IP库、优化和设计FPGA体系结构、设计一套考虑"软错误"的容错FPGA体系结构和CAD算法。本项目所建立的IP库将简化编程模型、提高计算效率，很大程度上扩大"基于图的数据挖掘"的实践应用；同时本项目所提出的新的FPGA体系结构和CAD算法将为FPGA厂商设计下一代FPGA提供依据。
在块存储设备上快速大量高效地写入小数据的研究	存储虚拟化;操作系统内核;I/O调度;文件与存储系统	大数据向存储系统发起全方位挑战的同时，小数据则威胁到存储系统的访问效率。典型的小数据包括文件系统的元数据，虚拟存储设备中的查找表项等。如今，绝大多数存储设备使用块接口，基于I/O栈上的虚拟块设备、I/O调度器和文件系统都需与块接口交互。由于小数据远小于一个块单位，实际访问成本远大于数据本身，导致设备带宽的浪费，显著降低I/O效率。尤其在即时持久化的要求下，小数据写入成为块设备的致命弱点。针对该问题，本项目提出一种高效、彻底的解决方案。不同于某些依赖特殊硬件或需要更改接口的方法，本方案运用了数据压缩技术，小数据通常跟随数据块，当该数据块被压缩时，小数据便可被嵌入数据块中，从而大幅减少小数据写入。不仅降低磁盘磨损，还省去多余的硬盘查找以及为强制顺序执行而使用的昂贵的flush命令。不同于传统数据压缩技术中追求节省存储空间的目标，该方案首次提出利用压缩技术来突破块设备接口的限制以大大提高性能。
云计算中基于内容相似性的资源动态融合技术研究	虚拟化;云计算;资源分配;分布式系统;资源调度	资源动态融合是云计算系统维护、负载均衡、节能减排的一项关键技术，目前其在应用中亟需高效的方法与实现研究。本项目将针对资源动态融合技术中的多虚拟机迁移问题，研究云计算环境网络及存储层次结构、负载执行及数据传输模式对虚拟机在线迁移过程的影响，通过定量的测试和分析影响虚拟机迁移效率的各种因素，形成虚拟机迁移性能和开销预测模型。在此基础上，围绕虚拟机镜像的"内容相似性"特征，研究虚拟机镜像全生命周期管理，通过虚拟机迁移时机、迁移对象、迁移目的地选择以及主机能耗管理多角度研究镜像存储、传输、实例化不同阶段的管理及优化方法，产出具有自主知识产权的云计算资源动态管理系统及迁移算法库。该研究依托具体应用驱动,多领域渗透,瞄准了云计算理论前沿,切中了具体应用瓶颈,在虚拟机迁移性能建模、分布式镜像网络管理方法、迁移优化方法三方面将有理论、方法和技术创新。预期成果将对云计算系统资源有效利用研究起到积极推动作用。
时空大数据环境下移动云计算的数据与服务迁移研究	移动云计算;数据迁移;数据中心技术;服务迁移;数据服务	数据与服务迁移是减少延迟和网络负载，提高移动云计算服务质量的一种有效方法。然而面对大数据环境下用户访问云服务模式的多样性、实时性、巨量性等特点，目前的迁移策略和方法面临严重挑战。本项目基于大数据对移动访问的时空轨迹的挖掘信息，从基本理论和实践的两个角度研究数据与服务迁移的四个基础科学问题: 1) 完善一般形式下数据迁移问题难解性的证明，奠定算法设计的理论基础；2) 通过优化对数据存储和访问的支持，提出更加有效实用的迁移算法，拓展数据迁移模型的实用性；3) 针对服务迁移，提出合作路由的概念，研究其对网络负载和总服务成本的影响，藉此设计流量感知的迁移算法，使迁移更加有效；4) 研究用户访问依时间变化的统计分布，在此基础上，结合离线和在线迁移各自优势，提出以期望最优的方式满足以一定统计分布的用户访问的迁移算法。本项目对促进大数据时代移动云计算应用的发展和高性价比基础平台的构建具有重要的理论价值。
基于软硬件协同优化的线程间指令重用研究	计算机体系结构;指令重用;程序分析	随着现代高性能处理器普遍采用多核/多线程结构，多道或多线程程序成为片上应用的主流负载。在一些重要应用领域，片上同时运行的程序之间存在大量指令重复，通过合理的软硬件设计，消除对重复指令的冗余取指和执行，不仅可能大幅提高片上并行执行效率，还可能显著改善处理器的能效。.本课题以数据并行的多线程程序、数据并行消息传递应用、多道程序、网络服务和安全应用等五大类负载为背景，从编译优化以及体系结构两个方面系统研究如何利用线程间指令和数据的相关性特征优化现代处理器的指令流水线。具体研究内容包括：(1) 利用程序切片的思想，研究静态程序分析或动态即时编译技术分析线程间计算和数据的相关性；(2) 线程间指令重用的体系结构支持，研究能在多个线程运行时自动消除重复的取指和执行操作的指令流水线。研究成果不仅在高性能计算、网络、安全和桌面计算等重要领域有宽广的应用前景，还将大大拓深结构研究领域对线程间指令重用的认识。
物联网中非确定性数据建模与分析	物联网;非确定性数据;可回溯性;分布式数据库;数据流	物联网对社会经济发展和人民生活进步各个方面都有举足轻重的推动作用。它不仅能够帮助简化制造，物流，控制和销售等生产流程以节约成本和提高效率，并且可以对整个流通过程进行记录和查询，以保证其可追溯性(Traceability)。物联网技术的普及将会是大势所趋，因此完善物联网的理论和技术基础，将会对其发展有重大的意义。目前物联网发展中存在的重要问题之一是其数据的准确性和以此为基础的回溯推理的正确性和完备性。     本项目旨在针对物联网这一大型分布式系统，探索一种通用的非确定性数据建模方法，对物联网中的不确定性数据进行全面的分析，并提出有针对性的数据挖掘方案. 本项目将着眼于数据库和数据挖掘的基础理论研究，并将其用于解决实际应用中的问题。从而提高数据质量，并最终为物联网上层应用的发展和建设建立良好的基础。
多维可重构嵌入式平台动态资源管理及存储安全研究	嵌入式系统;存储安全;资源管理	存储墙、功耗墙、算法墙等阻碍着系统计算效能提升，实现不同层次上、粒度可变的动态自适应任务划分和资源重构计算是提升计算效能的新途径；同时，嵌入式系统的安全性研究远远落后于通用计算机系统和互联网的安全性研究。项目拟在搭建嵌入式安全计算环境基础上，进行存储机密性与完整性研究。采用多级秘钥嵌套方法，解决机密性保护中因计数器溢出、秘钥与数据块更新带来的系统开销和延时问题；采用非平衡验证树与节点数据块大小动态分配方法，缩短平均校验路径长度，提高验证效率，在尽量不影响系统性能的前提下为其提供信息存储安全防护。研究不同维度、可变粒度任务资源的抽象模型，全面准确的反映出任务和系统的真实属性；采用三维动态积木策略与多维重构函数相结合的方法，为系统提供高效的任务资源自适应调度管理策略，提升系统整体计算效能；设计开发系统原理样机，为嵌入式系统安全防护机制的设计实现及任务的高效能计算执行奠定理论和实践基础。
连续数据存储系统节能关键技术研究	节能;存储系统;性能优化;数据布局;连续数据访问	视频监控、持续数据保护、备份、归档等应用产生海量数据，需要冗余磁盘阵列(RAID)作为存储系统来实现数据的存储和容错保护。这类系统的负载以连续数据访问为主，对随机I/O性能要求较低，通过调节其数据分布，将部分磁盘调度到待机状态，能够降低存储系统的能耗。本项目针对连续数据存储系统进行节能研究，提出一种动态映射数据布局方式及负载自适应的节能调度算法，将I/O负载集中在部分磁盘上，将其它磁盘调度到待机状态，降低磁盘的能耗；研究适于连续数据访问的数据预取算法、磁盘调度算法、缓存管理算法和RAID-6阵列码，优化"小写"数据写入性能，减少磁头移动次数，发挥磁盘连续读写吞吐率高的特点，使得较少数量的磁盘处于运行状态即能满足负载要求。本项目在满足对存储系统的性能及数据可靠性要求的同时，能显著降低连续数据存储系统的能耗，促进国家节能减排战略目标的实施。
基于随机Petri网的大数据计算模式的模型与评价	计算模式;性能评价;随机Petri网;大数据	随着云计算、物联网、社交网络等技术的迅猛发展，数据的快速增长成为许多行业共同面临的严峻挑战，大数据得到越来越广泛的关注。海量数据的分析与处理，是大数据的一个重要方面。大数据计算模式，即大数据分析与处理的抽象模型及方法论，是大数据计算方法和系统的设计与优化中的核心问题。本项目从大数据计算的特性出发，结合大数据计算系统等应用实例，从操作模式、服务模式和管控模式三个方面，研究大数据计算模式。基于随机Petri网等模型工具，给出普适的大数据计算模式的抽象模型。结合大数据计算中用户的需求，在抽象模型基础上，建立大数据计算的性能和可信赖性不同维度指标的评价模型，研究相关的模型分析求解技术，给出形式化的量化分析。本项目的研究，将为大数据计算的流程设计、机制支撑与系统实现提供理论基础，同时也将丰富和发展基于随机Petri网的性能评价理论。
基于车辆身份的VANET安全机制及其系统评估方法研究	安全评估;隐私保护;安全协议	车辆自组网以其可延展的多跳网络模式成为一种典型的车联网结构，而基于身份的管理模式是当前车辆管理模式在车联网系统中的自然承继。项目研究车辆自组网中基于身份的安全机制，针对当前VANET基于身份管理的研究中存在的RSU认证负荷过大、车辆通信关系泄露隐私信息以及车辆合法身份滥用等三个方面的问题，研究基于身份的车辆系统安全机制，包括：车辆的代理批认证技术，以车辆代理其它车辆认证缓解RSU的认证负荷；车辆轨迹无关的隐私保护技术，以分布式秘密共享的群组认证和RSU认证结合的方式，消除RSU认证带来的位置隐私泄露问题；多个车辆合法身份合谋攻击检测方法，将嫌疑节点聚类为团组结构以团组整体行为来判定合谋攻击。此外，项目还研究车辆中嫌疑节点及嫌疑群组发展态势预测及该态势对整个网络系统安全影响的评估方法，并基于该影响建立系统的激励与惩罚机制以实现对态势发展的有效遏制。
基于无线传感器的可穿戴计算情感交互理论与方法	无线传感网;情感交互;人机交互;可穿戴计算	泛在可穿戴计算与情感交互计算的融合是未来实现"以人为本"信息服务的核心支撑技术。如何通过基于传感器的可穿戴设备，实时采集个人感知物理信息，同时无缝地结合个性化的情感上下文，从而实现智能化的人机交互服务，是亟待解决的问题。本课题着眼于可穿戴模式无线传感器及相关网络设备技术、融合物理上下文的情感交互技术，从五个方面展开研究。在构建基于传感器可穿戴计算情感交互体系结构基础上，分别在面向上下文情感交互的、可穿戴可移动的无线传感节点及网关设备、基于用户参与情感交互的激励机制的上下文信息收集、结合情感状态的信息上下文分析处理三方面展开深入研究，最后以面向空巢老人的健康生活应用系统作为本课题各技术的实现验证。
倾斜任务调度模型及其在医疗大数据处理中的应用研究	数据倾斜;任务调度;数据抽样;MapReduce;大数据	倾斜任务是指MapRedcue框架中由于数据内部倾斜和系统不可靠性所引起的执行时间不确定的任务，医疗信息系统中对病历、医嘱等文本型非结构化数据的处理会产生大量倾斜任务。本项目拟首先通过分析医疗数据的实际特征，提出一种考虑系统性能与样本准确率的内部数据键值对分布抽样算法；同时提出一种旨在提高任务本地性，实现负载均衡的Reduce任务放置及中间数据切割与组合模型，并针对Shuffle过程提出基于写减少技术的数据读写优化策略。其次提出综合考虑数据倾斜程度与系统负载率的任务剩余时间评估模型，实现比现有研究更为准确的启发式倾斜任务推测执行机制，并由此提出DAG自动生成、实时调度与资源分配模型，解决因倾斜任务导致医疗信息系统性能低下的难题。最后对项目原型与现有医疗大数据分析系统进行集成与验证。研究成果还将有助于解决数据中心在倾斜任务的本地化、负载均衡以及容错方面的应用难题，具有较重要的理论与实际价值。
移动机会网络中的低能耗节点移动模型及消息转发机制研究	移动机会网络;消息转发机制;数据恢复机制;低能耗;节点移动模型	随着物联网技术的快速发展和移动数据通信需求的不断上升，移动机会网络不要求网络全连通的特点，更加符合实际环境下的自主组网需求，近年来成为学术界关注的热点。由于机会网络的上下文环境、链路状态、节点通信时间等具有诸多不确定因素，使得如何有效评估转发节点，提高消息转发成功率，降低节点能耗变得尤为关键。.   本项目拟通过分析节点的异构性及移动特性，准确描述节点相遇特征，保持节点能量与传输延迟处于折衷状态，构造一种具有较低能耗的节点移动模型；利用节点的独立性，结合机会网络的随机、动态特性，设计基于相遇预测的数据转发机制；通过实现充分利用相遇持续时间，合理设置转发消息大小并且控制冗余数据量的转发机制，确保消息投递成功；以节点丢包率和节点性能的评价为基础，研究基于接收节点丢包率的前向式数据重传机制，使其对丢包率高的节点进行高效的数据恢复重传，从而构建一个低能耗高投递率和恢复性强的移动机会网络。
数据驱动的自演进未来移动接入资源分配与优化机制研究	数据驱动;数据感知;自演进;机器学习;无线网络资源分配	本项目重点研究数据驱动的网络资源服务模型及以此为基础的自演进闭环资源优化分配策略，为构建高效的智能化的资源优化分配方案开辟新的研究途径。拟深入分析与用户和网络相关数据的特征及其运用方法，研究动态性模型策略，设计数据驱动的智能化网络资源服务模型；基于此模型，设计网络资源服务感知的资源优化分配策略，并运用强化学习理论，提出具备自演进闭环特性的网络资源感知的资源优化分配策略。基于以上理论研究结果，构建实际测试系统平台，通过实测数据验证与评估理论方案的性能表现，并提出基于实测数据的改进方案以得到最优化可行方案。.     通过本项目的研究，可望在理论上揭示用户数据和网络数据与网络资源服务模型之间的联系机制，提出基于实际数据构建与生成的网络资源服务模型，在此基础上形成能适应动态业务需求的自演进闭环资源优化分配策略，并在实践上构建高可用性的网络资源服务感知的自演进闭环资源分配优化方案。
基于CCN的新互联网架构体系对比分析及其路由缓冲策略研究	Centric;Content;Networking;路由缓存策略;网络协议;网络构架体系	伴随着互联网用户和数据的爆炸性增长，以内容为中心的数据交换逐渐主导了互联网应用的发展。TCP/IP结构已经暴露了节点拥塞、数据冗余、路由策略受制于物理连接、数据安全性差等缺陷。而CCN代表了新一代互联网体系结构的一种革命性创新，得到了国际顶尖学者的关注。目前相关研究大多关注其协议完善，罕见真实的成规模验证。在本项目中我们先实现一个基于CCN的P2P网络文件系统（CCN-Maze），原Maze的流行度将提供一定用户规模；同时在真实的应用环境中对比资源下载失效率、速率、镜像分布等指标，比较CCN和TCP/IP在构架和性能上的优缺点；此外，着重研究P2P环境下改进CCN的路由缓存策略这一关键点，以P2P用户网络的全局数据请求失效率为衡量指标评测相关缓存策略的优劣，增加考虑数据热度的不均匀性，提出改进方案。本项目立足实践，搭建可持续研究的新平台，致力于具有战略性研究意义的新一代互联网构架体系研究。
单体三维碳纳米晶体管存储器的容错技术研究与实现	故障建模;单体三维集成电路;存储器测试与修复;碳纳米晶体管	单体三维存储器效能极高，适用于大数据和机器学习等应用；但其制造过程有严格的温度限制，很难用传统CMOS晶体管制造。碳纳米晶体管的能效要比CMOS晶体管高一个数量级且功耗极低，是实现单体三维存储器的理想器件；但碳纳米管生长时有特殊的工艺波动，碳纳米晶体管故障率高。针对这一问题，本项目用图形图像处理技术和统计学方法，研究基于碳纳米管形态特征的故障建模方法；通过对"不完整"存储单元的可测试性设计和对"三维跳跃式测试"方法的研究，揭示"生长中测试"有效降低制造成本的规律；根据独特的故障分布，探索单层存储器的冗余共享机制，进一步利用单体三维芯片的高带宽优势，探索"多层冗余共享"机制和修复算法；建立开源成本评估工具，明确器件层的工艺选择、电路层的存储单元设计，架构层的测试与修复机制之间的互相关系。项目预期能提高该新型存储器的良品率，为其产业化提供方案，为其它新工艺存储器的容错技术研究提供参考。
CPS例化——高铁接触网智能巡检机器人研究	可重构;CPS建模;迁移学习;智能机器人	项目研究提升复杂时变环境下高铁接触网巡检机器人巡检效率与效果所需解决的若干问题。针对巡检任务需求，完成具有时空关键及安全关键的智能巡检机器人CPS建模。对可重构FPGA检测模块展开研究，包含基于FPGA的检测算法研究与模块设计，通过可重构特性实现对不同巡检工作环境与巡检任务所需检测算法的组合与参数快速配置。对巡检任务进行理论抽象，完成满足任务需求的基于层次化模型的智能巡检机器人研究与设计，以ROS平台实现机器人自主导航、人机交互、智能巡检、检测数据分析处理功能。研究迁移学习算法以加快机器人在新环境和新任务中工作模式的适应与调整，包括机器人行动模式、检测模块运行模式与数据分析模式。实现具有安全性、高效性、准确性、便利性的高铁接触网智能巡检机器人原型，提高接触网运维保障效率与效果，为高铁安全运行提供有力支撑，推动我国高铁事业稳步发展。
众核处理器片内资源利用率和能效优化研究	资源利用率;众核处理器;高通量应用需求特征提取;资源时变需求精确预测;能耗感知软硬件协同资源管理	功耗墙是众核处理器结构扩展和性能发挥的极大挑战。新型网络应用是现代计算系统的最重要应用之一，其三个动态时变特征，即高通量需求、大地址空间的离散访问、交互式请求驱动的执行模式，导致众核芯片内共享资源竞争严重，资源有效利用率的降低使能耗问题更为突出。针对此问题，本课题将通过捕捉特征的动态时变规律，研究感知能耗需求的片内资源管理方法，在功耗约束下实现片内资源的高效利用。主要研究内容包括：（1）需求特征提取：通过迭代优化，研究特征降维的聚类方法，分析新型网络应用对片内资源和能耗的时变需求特征；（2）时变需求精确预测：利用程序的需求特征，通过博弈论方法，研究精确预测片内资源和能耗动态时变需求的方法，捕捉需求特征的动态时变规律；（3）软硬件协同管理：利用能耗和片内资源的时变需求预测，通过片内资源动态分配和线程调度技术，研究片内资源有效利用率的动态优化方法，减少资源竞争访问，实现功耗约束下的性能提升。
多维数据布鲁姆过滤器的理论与技术	布鲁姆过滤器;硬件加速;距离敏感哈希函数;并行计算;多维数据	数据过滤技术能够从静态或动态的海量数据中快速提纯出有价值的数据做进一步处理，它是当前数据爆炸时代非常有效的工具。尽管单维数据过滤器已经研究和应用多年，但针对多维数据过滤器的研究还不多，且主要集中在低维数据的集合判断问题。基于数据管理技术的前瞻性考虑，项目以多维数据为处理对象，探索高性能数据过滤器的理论和实现技术，为大数据处理领域提供先进、实用的解决方案。研究内容包括：(1)提出低维数据布鲁姆过滤器关联删除概念和方法，该方法也能用于半连接、窗口更新等其它操作；(2)针对高维数据的不同过滤粒度，提出多粒度距离敏感布鲁姆过滤器方案；(3)结合硬件和并行计算的高效性，设计实现可用于数据处理前端流水线加速的硬过滤器；(4)设计实现后端数据处理服务器的基于MapReduce的批处理和流水线加速方法。该项研究具有原创性，对提高数据处理速度，拓展数据管理技术的理论和方法有重要的理论和现实意义。
多核微处理器并行调试技术研究	并行调试;并行程序;非侵入式调试;模拟器;多核微处理器	微处理器发展已经进入了多核时代。多核微处理器丰富的计算资源和不断增强的线程级并行能力与并行程序开发和调试困难间的矛盾日益突出。研究多核微处理器的并行调试技术，对促进并行程序发展、提高多核微处理器和超级计算机系统的实用性具有重要意义。本项目面向多核微处理器体系结构，针对多核系统级软件和应用级软件两类并行程序，深入研究调试支持部件设计方法、"记录/回放"的辅助调试手段、多核调试系统结构等多方面问题。从非侵入式混合态调试方法、软硬件协同控制的并行程序执行路径遍历方法和独立多维环路结构的多核调试系统三个关键技术切入，对多核微处理器并行调试提出了有效的解决方案。同时设计支持以上调试环境验证的模拟器，对我们提出的解决方案进行验证和评测。我们希望依托本项目取得的研究成果能够指导国产高性能微处理器的研发，并最终应用到国防科学技术大学自主研制的多核处理器中
异构众核处理器非对称片上互连网络研究	异构多核处理器;众核处理器;片上网络;众核体系结构	在异构众核处理器的设计过程中，片上互连网络的设计是最主要的难点之一。目前，国内外学者的研究工作主要集中在传统同构众核处理器的对称均匀互连网络上，而对异构众核处理器上的互连网络研究还处于起步探索阶段。与同构众核核处理器片上互连网络不同，异构众核处理器的片上互连网络需要解决由于芯片结构不对称性带来的诸多问题。举例来说，由于芯片结构不对称，每个核上可以运行的应用程序会有较大差异，因此，不同处理器核或者加速器的访存通讯模式会大不相同，这对片上互连结构的设计在异构化，可扩展，以及低功耗三方面提出了新的挑战。本项目拟面向异构众核处理器片上互连结构设计的，以演化优化问题求解为依托，研究异构众核处理器片上互连结构设计的关键技术，力争形成异构众核处理器片上互连结构以及路由器结构设计的开源工具，为国产异构众核处理器设计提供支持。
宽向量架构DSP高效并行存储技术研究	并行存储;微体系结构;宽向量架构;指令融合;映射函数	近年来，传统的通用DSP厂商不断受到新兴企业的冲击。以手机芯片为例，德州仪器、意法半导体已经退出该市场，高通、MTK大行其道，中国企业也开始在这一领域打破坚冰。这一现象是和4G无线通信协议的普及和频段复杂性紧密相关的。可以预测，在未来的5G无线通讯、新一代视频编码、物联网等需求的牵引下，DSP体系结构也将面临着深刻的变革，我国DSP研究机构和企业面临着一个非常好的"弯道超车"机会。"宽向量架构"是未来DSP体系结构发展的重要趋势。并行存储部件是宽向量架构DSP的重要组成部分，其数据供给能力对系统性能的发挥起着非常关键的作用，并随着向量宽度的增加而愈发重要。本项目针对宽向量架构DSP高效并行存储技术展开研究，拟在低面积开销的新型并行存储方案、面向类FFT算法的向量访存混洗融合技术和面向多路向量访存请求源的冲突缩减技术三个方面上寻求突破和创新，从而缓解向量访存问题对宽向量架构DSP性能的限制。
虚拟计算系统中的自适应资源控制策略冲突与协调机制研究	策略相容性;协同控制策略;计算虚拟化;自适应控制	在虚拟计算系统中，自适应资源控制策略实现了虚拟机资源的动态自适应的按需控制分配，提高了计算系统资源的利用率。但是，系统资源的自适应控制分配是有代价的，它增加了资源控制策略的设计和实现复杂性，并且虚拟机中多种资源控制策略组合和相互影响可能引发资源控制策略冲突问题，影响到资源自适应控制机制的稳定性和系统性能。.针对上述问题，本项目拟从虚拟计算系统中的自适应资源控制策略组合冲突以及虚拟机对共享资源控制的相互影响的角度出发，研究虚拟计算系统中自适应资源控制策略组合的策略相容性和协调机制问题，提出控制策略相容性度量模型和评价方法，以及消解策略冲突的相容策略设计和协调机制设计方法，以解决虚拟计算系统中存在的资源控制策略冲突问题，实现虚拟计算系统资源的自适应动态管理和高效利用目标。
基于网格的分布式雷达仿真系统关键技术研究	仿真系统;雷达;分布式;网格	目前，分布式雷达仿真系统大多是基于HLA进行开发。HLA对仿真应用的开发、管理提出了比较完善的解决方案。但基于HLA的分布式仿真系统缺乏对仿真资源的组织、调度使得系统缺乏弹性和灵活性。网格技术作为一种致力于管理众多地理上、组织上分布的异构资源并提供一种分布的高性能计算与数据处理的底层支持框架，为分布式仿真系统发展中遇到的上述局限提供了一种理想的解决方案。本项目拟研究基于网格的分布式雷达仿真系统的关键技术。通过深入研究雷达仿真系统框架的构建、仿真系统动态调度策略、网格环境安全等关键问题，突破分布式雷达仿真系统跨区域，安全高效仿真的技术瓶颈，提升仿真效率。项目的预期成果将丰富分布式仿真系统理论，建立基于网格的分布式仿真系统框架，提高仿真系统的性能，对我国分布式系统建设有重要的理论意义和应用价值。
基于确定性重演的多核程序并发错误消除方法研究	多核程序;错误消除;并发错误;确定性重演;机器学习	共享内存的多核程序因为线程间内存交互顺序的不确定，极易发生并发错误，但现有错误处理方法难以确保程序的运行质量。为解决这一问题，本项目拟基于确定性重演原理，研究硬件实现的并发错误消除方法。.确定性重演能够实现程序重现以前的执行轨迹，为程序生产运行时的并发错误消除提供了新途径。本项目基于确定性重演，围绕交互特征提取、交互踪迹记录、交互踪迹过滤和并发错误消除四个方面展开研究。在交互特征提取方面，研究硬件支持的程序特征动态提取方法；在交互踪迹记录方面，考虑到应用前景，面向采用监听一致性协议的现代多核处理器，研究高效的并发式记录策略；在交互踪迹过滤方面，研究基于机器学习的交互顺序分类方法，过滤掉不影响程序执行结果的良性交互；在并发错误消除方面，研究硬件实现的低开销并发错误预测和避免方法；最后，构建原型系统并进行性能评测。本项目的研究对提高多核程序的运行质量和多核处理器错误处理结构设计具有重要的意义。
基于形式领域融合的计算模式	系统建模;计算模式	计算模式决定着系统构造的方式，从底层的处理器体系结构到高层的软件体系结构，都以计算模式为基础。目前流行的计算模式之所以导致系统构造效率低质量差，根本原因是缺乏形式语义支持，如SOA一类的结构型计算模式及数据流一类的过程性计算模式，只规定了语法和语用。为此，我们研究一种新的系统构造方法- - 格件模式，它同时引入了构件形式语义与构件连接的形式语义，将系统看做由具有确定形式语义的系统构造成分"格、场、格核、场核"通过自相似滚动式的扩展和纵横交错的融合而成.支持多级高度并行、高可信性保障、高阶操作和多重复合控制集中(类IoC)机制以及结构可扩展及框架跨网络等特性,同时也建立了一种基于形式语义逐步求精的系统模块复用机制，可做为新一代软件的开发方法和云计算系统开发方法，也可用于构造一种基于多核融合与扩展的处理器体系和多处理机体系。该项目萌芽于我们多年的相关实践，本次重点是建立相关理论与规范体系
基于容错代价的云计算可生存性理论与关键技术研究	故障检测;云计算;故障恢复;可生存性	云计算在为租户提供方便易用和低成本服务的同时，其多租户、资源高度集中特性使得系统一旦出现故障所造成的损失远比传统计算模式严重。云计算的可生存性体现在两个方面：1）对租户而言，需要提供一种满足其服务质量需求，且付出代价小的容错方案，并体现租户分等级特征，这也是以"服务质量"为本的云计算追求的目标，是云计算得以生存和发展的一大关键；2）对云平台而言，需要消除动态复杂环境下故障的关联性以隔离故障和缩小其危害范围，并尽可能减小容错开销，实现故障检测和恢复的自动化，体现出平台的自修复能力。拟从三个层面开展研究：1）故障复杂性分析，以组件为粒度开展关联性分析，并以此研究故障传播问题，2）可生存性理论研究，分别从平台和租户的角度研究平台的可生存性建模以及面向租户等级的容错代价理论；3）可生存性关键技术研究，包括多层次、低代价容错架构以及故障自动识别和恢复方法。项目成果将用于指导高可靠云计算平台的设计。
支持QoS的多核异步处理器体系结构研究	多核处理器;服务质量;片上网络;性能模型;异步电路	随着嵌入式应用日益复杂化，多核体系结构得到广泛应用，但也带来功耗增加、时钟偏移增大等问题。异步电路具有天然的低功耗、无时钟偏移与高可靠性等优势，采用异步电路实现多核处理器能够有效解决功耗、时钟偏移等问题。然而，异步电路执行方式与执行时间具有一定的不确定性，使得其对服务质量（Quality of Service，QoS）支持不足，无法满足多媒体、实时等高QoS要求的应用，阻碍了多核异步处理器在上述领域的应用。本项目基于嵌入式多核异步处理器研究如何提高异步电路的确定性，以解决传统异步电路在QoS支持方面的不足。主要研究内容有：高QoS的多核异步处理器执行模型，从系统级研究QoS保障技术；支持执行模型的异步处理器内核与异步片上网络结构，从体系结构与电路级研究如何提高异步处理器的QoS；异步电路的性能模型与验证模拟系统，解决异步电路与异步处理器难以建模的问题，并对本项目的研究内容进行验证分析。
面向GPGPU系统中存储访问优化关键技术的研究	GPGPU存储系统;闪存;固态盘;固态存储;闪存存储系统	GPGPU提高了数据密集型应用处理的性能。分析发现，存储受限型应用中，当GPGPU访存失效且需访问CPU端存储空间时，其访问路径较长，软件栈内数据传输和管理开销较大，造成访问延时较高，导致流水线阻塞，加重了GPGPU内存墙问题并降低其性能。本项目拟提出一种面向GPGPU系统的存储访问优化方案。1）拟构建一种基于固态盘的GPGPU内存-SSD分层存储系统模型（含架构、软件栈及性能模型等）；2）拟提出一组关键技术（含基于编程接口和数据访问通道的编程模型、基于空间检测和数据传输管理的运行时系统、基于数据分布和存储访问一致性的数据存储管理等）；3）拟设计一种系统原型及测试方案。达到缓解GPGPU内存墙问题并提高其性能的目的。本项目旨在研究一种高性能、大容量的面向GPGPU系统的存储访问优化方案，其预期成果将丰富GPGPU系统性能优化的研究，为面向大数据处理的异构系统性能优化提供方案借鉴和技术参考。
超多核处理器片上网络性能模型研究	片上网络;模拟器;性能模型;超多核处理器	随着超多核处理器的发展，片上网络及其通信效率日益成为影响处理器性能的重要因素。然而迄今为止，完善的片上网络性能模型在学界仍付阙如，故而研究该模型具有重要意义。本研究课题涉及：①超多核处理器的片上网络性能评测技术；②片上网络拓扑结构、路由算法及性能模型；③片上网络与超多核处理器缓存一致性协议性能模型等三部分。主要解决：①片上网络的通信模型；②片上网络性能评测技术与模拟器；③片上网络拓扑结构、路由算法及性能模型；④片上网络与超多核处理器缓存一致性等四个关键问题。本课题拟通过先分析量化性能指标，后建立片上网络通信模式的方式架构性能模型，并使用该模型研究网络拓扑、路由算法与缓存一致性。本项目提出的片上网络性能模型对未来的超多核处理器设计有重要的指导意义，而针对特定应用程序的超多核处理器片上网络拓扑结构与路由算法，适应超多核处理器片上网络环境的缓存一致性协议，对提升系统整体性能具有较高的实际价值。
智能移动嵌入式系统的协同节能关键技术研究	嵌入式系统能耗;能耗优化;协同节能;能耗评价;能耗分析	在目前全球倡导"低碳节能"的背景下，面对移动互联网快速发展的需要，智能移动嵌入式系统（简称智能嵌入式系统，如智能手机）的能耗问题始终是困扰智能嵌入式系统高性能应用与多样化服务的重要瓶颈。为提高智能嵌入式系统的续航能力与可用性，本研究从硬件单元功耗、软件任务运行效率及系统资源配置策略三个方面探讨智能嵌入式系统的协同节能技术，首先研究新型高能效的可定制异构多核体系结构，包括可定制核和可编程模块的高能效异构多核结构和能量感知的编程模式等软/硬件两个层面的静态低功耗设计。然后，基于任务的动态性和差异性，设计一种基于电池电量反馈的自适应闭环控制任务调度模型，实现能量可感知的任务运行模式，并根据任务运行的"局部化"原理，形成一种软/硬件资源的自适应节能配置策略，三个方面节能技术的交互与协同，以获得能量资源全局最优的应用效率。最后，进一步探讨能耗与速度等指标关系，并集成研究成果开展实例应用与验证工作。
空间天气学全球MHD数值模拟的大规模并行计算研究	空间天气;MHD;性能优化;区域分解;并行计算	空间天气学的核心是空间灾害性天气过程的研究，并科学地评估其对人类活动的影响。全球MHD模型是描述地球空间的磁层、电离层和热层系统中磁物理现象的重要模型，可以模拟诸如地球磁层受到扰动出现的亚暴现象。然而，由于该问题的数据规模大、时空尺度跨度大、方程非线性、多物理量耦合、各向异性等特点使得应采用大规模的高性能并行计算作为其高效预报的必要手段。.为解决空间天气学全球MHD模型大规模并行计算问题，针对该模型研究区域分解高效并行算法，并结合当代超级计算机体系结构的特点，对程序在处理器核间、节点内处理器间、超节点间实现多层互连、应用加速部件等方面研究多结构混合并行计算模式，最终实现面向空间天气学全球MHD模型的高并行效率、成百上千核可扩展的大规模数值模拟计算。
面向众核处理器的非易失性缓存低功耗技术的研究	高速缓冲存储器;低功耗设计;片上多核处理器;非易失性存储器;片上网络	片上多核处理器需要大容量的缓存来满足所需的数据访问带宽。随着技术向深亚微米发展，传统SRAM缓存的泄露功耗会急剧增加，严重限制了芯片的性能并对芯片的稳定性构成威胁。STT-RAM是一种新型的非易失性存储器，具有访问速度快、存储密度大和泄露功耗可忽略不计等优点，是替换SRAM作为片上缓存的最有前景的存储器技术。针对STT-RAM的写操作功耗大与延迟长等问题，本项目将开展STT-RAM缓存的低功耗技术研究，主要内容包括：基于缓存一致性协议的写机制、基于缓存块重用预测的旁路算法、适用于低数据保持时间STT-RAM缓存的无数据块机制及基于写密集度预测的高性能混合缓存等，并将研究成果集成到本项目所开发的众核处理器性能仿真验证系统进行分析、对比和验证。本项目的研究将有望提升基于STT-RAM的缓存系统的性能，并显著降低其功耗，将进一步推动STT-RAM在有高性能低功耗存储需求场合的应用。
基于软硬件协同的数据中心SSD缓存系统关键技术研究	固态硬盘;缓存管理;垃圾回收;软硬件协同;数据布局	基于Flash的固态硬盘（SSD）凭借其容量、价格和性能优势，正受到广泛关注，并逐渐作为缓存部署在数据中心。数据中心SSD缓存系统面临三方面挑战：第一，Flash写放大特性会加速SSD磨损；第二，SSD FTL封闭性导致上层软件对SSD性能表现可预测性降低，并且无法利用软件层更加丰富的信息进行优化；第三，数据中心使用虚拟机运行不同应用，导致SSD缓存需要处理多样化IO负载。本课题围绕这三方面问题开展高效SSD缓存系统的研究工作。本课题以软硬件协同作为基本研究方法，重点在软硬件协同的缓存数据布局，软硬件协同的垃圾回收和基于缓存收益与成本模型的缓存策略优化三方面进行研究突破。本课题实现的SSD缓存原型系统将FTL功能部分开放给缓存管理软件，实现软件决策，固件执行，充分利用缓存软件层信息，有效避免写放大影响，提升缓存命中率，提高SSD利用率，有助于推动SSD在数据中心高效使用和广泛部署。
片上网络互联的多核系统中的区域化任务映射算法	多核;区域化;片上网络;映射算法	随着多核芯片的普及，片上网络逐渐取代总线结构成为了芯片内部的通讯方式。在多核系统中如何将任务分发到具体的硬件设备上执行直接影响这系统和具体任务的执行效果。本课题主要研究多核片上网络中的映射算法，均衡系统中各部分的通讯负载，提高系统的整体性能和资源利用率。具体的研究内容包括：多核系统片上网络通讯分析；任务通讯模型建立；多核系统区域化映射算法的设计；映射算法的评估和优化。研究的过程首先通过对系统各部件进行性能分析以及实际运行测试来提取可以反映系统运行状况的参数指标，在此基础上建立多核系统中片上网络模型和系统任务通讯模型。设计区域化映射算法，并对映射算法对系统的影响进行分析评估。最后再通过系统状态分析和通讯预测对区域化的映射算法进行调整，使映射算法更加符合当前运行环境的需求。
基于容迟与容断网络的安全路由协议研究	容断网络;路由协议;安全;容迟网络	容迟与容断网络具有间歇连接、频繁割裂和时延极高等特点，现有的适用于移动自组织网络和无线传感器网络的路由算法与协议很难应用在容迟与容断网络中，其路由面临新的挑战。容迟与容断网络路由协议作为一项关键技术成为当前无线通信网络研究领域的前沿热点课题。针对容迟与容断网络路由安全技术、路由技术及路由协议的分析和设计、安全防御机制的建立等难点问题以及国内外现有研究工作在路由的安全性、多目标优化和服务质量等方面存在的问题，以节点能量消耗与存储空间占用、路由安全性、可扩展性与复杂性以及网络传输率、时延为路由设计评估指标，围绕单播路由、组播路由、安全路由和路由安全技术4方面深入研究，建立基于容迟与容断网络路由技术的密钥管理方案、认证机制、安全防御机制和安全路由协议，使具有安全性的高效路由协议设计得以实现，为容迟与容断网络技术的发展提供一系列有参考借鉴意义和实际价值的研究结果和结论。
基于溯源的高效智能的入侵检测与数据重建方法研究	溯源;入侵检测	如何防止系统遭受入侵以及快速的自动数据重建一直是存储系统安全领域的一个重要话题。传统的方法大多通过监控入侵进程的系统调用信息来进行入侵检测，然而这种方法存在检测率不高、不能追踪入侵来源以及不支持快速的自动数据重建等重要不足。本项目拟研究基于溯源的入侵检测与数据重建方法，来解决这些问题。该方法包括基于溯源的入侵检测方法、基于溯源图的数据跟踪方法以及基于溯源的快速自动重建方法，旨在利用溯源数据的特点，使计算机系统能够更高效的应付各种入侵攻击。本项目的研究可为云存储环境下的安全机制设计提供前期理论研究支持和部分实现技术探索。
基于结构感知的大规模动态图划分算法研究	分布式存储系统;分布式计算;大图处理	随着分布式图数据处理时代的到来，大规模动态图的划分问题引起研究者的广泛关注。然而，现有划分算法大多基于局部信息进行迭代式优化，时间复杂度高且划分效果不理想。划分产生的大量交叉边直接制约着分布式图数据处理的性能。图的结构对划分具有至关重要的影响，本项目提出基于结构感知进行大规模动态图划分的研究思路，首先研究以划分支配因子为中心的大规模动态图结构的描述方法与动态感知方法；其次，基于结构感知信息，研究划分参数给定条件下的大规模动态图划分算法；最后研究结构特征与划分参数选择之间的关系，以根据结构特征选择最佳划分参数，实现划分效果的综合优化。本项目拟在大规模动态图结构的描述与感知、结构特征对划分的影响规律方面形成创新性理论成果，并基于上述理论成果，形成基于结构感知的大规模动态图划分算法，在保证负载均衡的同时，有效减少交叉边数量。本项目的研究将为大规模动态图的高效分布式处理奠定重要的理论和技术基础。
云计算环境中面向虚拟资源的能效模型及算法研究	随机服务模型;虚拟资源;云计算;能效模型	能耗管理与控制属于云计算领域的基础性研究课题之一，其难点与挑战在于如何在资源虚拟化环境中有效地实现细粒度和自适应的能耗/能效优化管理。本课题以研究虚拟资源的能耗模型为起点，提出采用随机服务理论来建立虚拟资源的抽象服务模型，从而提出一套量化分析虚拟资源"服务能力-能耗"之间关系的方法，最后基于该方法构建一个面向"能耗-能效"双目标优化的虚拟资源供给与配置模型。本课题的研究有望解决虚拟资源在动态负载环境中的能耗评估问题，从而有效推进云计算环境中细粒度和自适应的能耗/能效优化管理研究。此外，课题研究的"能耗-能效"双目标优化模型有望直接应用于实际的云计算系统，从而降低数据/计算中心的能耗成本，并提高云计算系统的总体能效指标。
基于多值自旋磁存储器的高速缓存结构及调度技术研究	自旋转移矩磁存储器;非对称写通道模型;寿命感知;高速缓存;可靠性	片上处理器核数目的不断增加需要更深的Cache层次和更大的Cache容量来满足日益增长的带宽需求。传统的SRAM单元尺寸大、静态功耗高，而eDRAM的加工工艺复杂，因此在未来的大容量Cache结构中均不具有吸引力。自旋磁存储器具有数据非易失、静态功耗极小、单元面积小、读取速度快且与流行的CMOS加工工艺兼容等特点，已经成为未来构造大容量Cache的候选器件之一。多值单元技术促使自旋磁存储器的单元密度进一步增大。本项目拟研究使用多值自旋磁存储器构造Cache结构的方法，提出相应的Cache调度策略；研究多值自旋磁存储器状态翻转错误率的精确表征方法，提出针对自旋磁存储器单元错误率不平衡特性的内容相关的纠错码技术；研究应用程序对Cache写操作的特性，提出使写操作尽可能均匀分布以延长多值自旋磁存储器Cache寿命的调度。本项目的开展有助于缓解多核/众核处理器中存在的存储墙、功耗墙和带宽墙等问题。
适用于高性能计算平台的金融风险度量模型及计算方法研究	Monte;金融风险度量;高性能计算;集群计算结构;大规模并行计算机结构;Carlo模拟	金融风险管理的核心基础是风险度量，随着金融市场不稳定性逐渐增加，风险度量需要处理海量数据，难以满足实时需求，本课题将开展创新的可计算风险建模与计算方法研究解决这一技术问题：（1）金融风险度量模型及新型并行化计算方法研究；（2）基于动态扩展与自适应网络环境的四层并行架构研究；（3）大规模并行计算环境下节点间任务动态分配与协调调度机制研究；（4）基于众核的高性能随机数产生器研究。通过本课题的研究，利用高性能计算平台实现金融风险实时度量，促进了金融领域风险度量技术的发展，为其在高性能计算应用提供理论依据，并拓展金融领域大规模并行计算软件应用。
面向TSV的时延故障检测及容错方法研究	可测试性设计;时延故障;硅通孔;时延测试	TSV在制造和绑定过程中存在非常高的时延缺陷率，从而严重影响三维集成电路产品的品质和良率。针对这个问题，本项目拟计划研究面向TSV的时延故障检测以及容错设计方法，主要研究内容包括：(1)研究基于环形通路振荡的TSV传输时延测量方法，以此达到检测TSV时延故障的目的；(2)研究基于链式控制的缺陷TSV容错结构，以此实现缺陷TSV的容错并提升三维集成电路产品的良率；(3) 研究面向容错结构的TSV分组及选择方法，以此保障缺陷TSV的容错修复成功率；(4)研究基于TSV 扩展的关键通路选择和时延测试方法，为TSV分组和选择提供关键通路依据，并确保三维集成电路在绑定和容错修复后的时序正确性。通过研究面向TSV的时延故障检测和容错设计方法，可以显著提升三维集成电路产品的品质和良率，并以此降低三维集成电路产品的成本。
适合并行系统结构的存储系统多容错编码设计及其算法优化	多容错编码;组合构造;多核/众核架构;并行编码/解码	海量信息存储一直是计算机科学最重要的领域之一。近年来，由于突发灾难事件时有发生，人们对数据保护技术越来越重视。容错编码技术是基础的数据保护技术之一，当前应用需求的推动和存储技术自身发展的趋势，对多容错编码提出了迫切的需求。但现有多容错编码中还没有公认的完美的方案，而性能优化的研究就更为滞后。因此，本项目拟利用组合数学方法改进现有多容错阵列码的不足、设计新的高容错阵列码和多级混合码方案；设计非MDS阵列码和解码性能最优的阵列码方案；针对现代多核/众核体系结构，优化编码方案，深度优化、并行化编码/解码算法；设计编码方案按需选择算法，实现编码/解码算法库，验证编码方案和算法在实际存储系统中的表现。这些研究内容符合国内外学术界最新研究趋势，有望取得在国内外学术界有一定影响力的成果。并与国内兄弟院校、科研机构及IT企业的工作形成互补，为我国存储技术研究和产业化起到一定促进作用。
面向E级计算可靠性墙问题的关键技术研究	系统结构;可靠性墙;容错;E级计算;性能	当前，高性能计算系统都采用并行处理方式显著地提高系统性能。随着系统规模的增长，尤其是增长到E级（百万万亿次）计算规模时，可靠性墙是其面临的巨大挑战性问题。因此，为了缓解或消除可靠性墙问题，本项目计划基于课题组在计算机系统结构、容错技术等方面的研究成果和技术积累，面向未来E级计算的高效能需求，研究可靠性墙瓶颈模型和理论，以及轻量级检查点/恢复、基于硬件冗余的可扩展容错和基于应用特征的容错技术，并运用软硬件验证平台对上述模型、理论和技术进行验证，以实现可扩展的系统容错，保证未来E级计算系统的高效持续运行。本项目计划发表高水平学术论文8篇以上，参加国际学术会议2人次以上，培养研究生4－6人。
无线传感器网络安全数据处理研究	隐私;完整性;传感器网络;数据处理;安全	数据处理是传感器网络中至关重要的一项操作。本项目以部署于不可信环境的传感器网络为研究背景，提出一种新的基于数据隐藏和数字水印技术的轻量的安全数据处理机制。依据传感器网络分簇特性，重点研究了数据处理中传感数据的隐私保护问题、数据的完整性认证问题及容忍节点失效问题，并提出了相关的技术方案和路线：提出可擦除数据隐藏技术，使得节点间无需额外信息交互即可分布式解决线性处理中数据的隐私保护问题；提出可擦除数据隐藏和匿名技术，解决非线性处理中数据的隐私保护问题；设计链式水印技术解决非线性处理中实现了隐私保护的数据的完整性鉴别问题。此外，针对节点俘获问题、节点动态增加和撤离问题，进一步研究这些方法的鲁棒性，最终形成数据处理中开销轻量、灵活性高、抗节点俘获能力强的安全方案。项目将采用仿真和实验相结合的方式对提出的算法和机制进行评价。该课题的研究，将有助于推动传感器网络的应用，促进传感器网络安全技术的发展。
物联网环境下视频大数据处理平台设计理论与关键技术研究	云计算;视频数据管理;视频大数据;分布式计算引擎;视频分析	随着网络视频监测系统的广泛部署和应用，对监测视频大数据的高效处理和挖掘的需求日益迫切。虽然云计算能够为解决大数据的高效处理问题提供有效的手段，但由于监测视频的数据量大、内容相关性强、数据流信息量分布不均衡等特点，造成了视频大数据处理的实时性差、视频处理平台的资源利用率低、视频分析数据的吞吐量低等挑战。本项目联合考虑视频信息处理和大数据技术的特点，研究物联网环境下视频大数据处理平台设计理论和关键技术，具体内容包括：（1）基于内容的视频大数据分级管理；（2）关联性视频大数据批处理任务调度；（3）基于工作流重构的视频大数据流式计算；（4）物联网环境下视频大数据处理验证平台。通过本项目的研究，一方面在视频大数据处理平台基础理论和关键技术方面取得原创性的科研成果；另一方面，本项目的成果可应用于城市管理、公共安全、智能交通等多个领域，社会效益和经济效益显著。
高性能低功耗合成孔径雷达成像片上系统体系结构	合成孔径雷达成像;快速傅立叶变换FFT;FFT处理器;片上网络;低功耗体系结构设计	合成孔径雷达（SAR）具有全天时和全天候以及远距离高分辨率成像的优点，在军事和民用领域有着广泛的应用。本项目研究合成孔径雷达成像片上系统高性能低功耗体系结构设计，对推动国防建设和社会经济与社会发展有重要意义。..本项目主要研究（1）基于片上系统的FFT并行计算方法。通过算法设计降低计算复杂度，减少数据交换。通过算法映射充分利用片上系统资源和片上系统的特点，降低传输延迟。（2）面向合成孔径雷达成像的新型块浮点数据表示和计算方法，提高成像处理精度。（3）FFT处理器核存储器组织结构优化设计，降低存储器的动态功耗和静态漏电功耗。（4）自适应快速虚拟通道微体系结构，通过自适应性降低网络拥堵，通过虚拟通道的设置与优化配置降低信元缓冲器的静态漏电功耗。..本项目拟在相关国际会议和期刊上发表论文4-6篇，拟申请国家发明专利4项。
片上众核集群体系结构关键技术研究	体系结构;单指令流多数据流;众核;编程模型;片上集群	随着半导体技术按照摩尔定律进一步延续，如何有效利用丰富的芯片资源成为研究的热点。片上多核处理器结构以其低功耗、高性能、低成本优势逐渐成为高性能处理器设计的主要途径。但是随着片上处理器核数量不断扩展、异构核的种类日益丰富，片上众核并行结构的复杂性使得其众多的计算资源难以得到有效的利用。本课题针对多核微处理器日益强大的计算能力和相对落后的多核组织与计算模式之间的矛盾，将日趋成熟的集群技术引入片上众核处理器体系结构的设计，为有效组织和利用众多处理器核提供有力支持。将从芯片底层设计出发，在片上众核集群并行硬件结构、硬件高效支持的片上众核集群并行编程模型以及片上众核集群资源管理等不同层面，深入研究延长摩尔定律的高性能众核微处理器的片上集群体系结构关键技术，为我国未来高性能处理器的发展探索新的道路。
基于协作通信的能量有效无线网络多信息流路由问题研究	无线网络;能量有效性。;协作通信;多信息流路由	影响无线网络应用的技术瓶颈是无线传输的效率和性能，而信号衰落是降低无线传输效能的主要因素。因此，近年来人们提出了一种新的协作通信技术来克服信号衰落问题，即多个单天线节点通过某种约定的协作方式来共享彼此的天线，以获得类似于MIMO系统的空间分集增益与空间复用增益，来提高无线系统的传输效能。由于协作通信改变了传统的通信模式，而路由选择又是无线网络中最核心的问题之一，因此设计新的协作路由机制来提高无线网络中多信息流的传输性能至关重要。本课题将以无线网络特性和性能需求（如带宽等）为出发点，以降低无线网络能耗为优化目标，紧紧围绕多信息流协作传输建模与协作路由实现机理这两个科学问题，设计基于协作通信的多信息流路由协议和算法，使其适用于资源（如能量、带宽等）受限的无线多跳网络环境，满足无线网络不同的应用需求，并将在相关测试床上进行协议和算法的验证，推动协作通信技术在下一代无线网络和移动通信中的应用。
异构多核可重构计算平台上面向服务的操作系统关键技术	操作系统;可重构计算;面向服务;服务抽象;异构多核	异构多核可重构系统是嵌入式系统的重要研究方向之一。可重构技术的快速发展，使在异构可重构计算平台上集成操作系统成为可能和必要。然而如何对异构资源进行有效管理，同时充分发挥可重构平台的优势已经给目前的操作系统研究带来了严峻的挑战。本课题拟将面向服务的概念引入异构多核可重构计算平台的操作系统设计中，建立面向服务的操作系统构造模型。为了提高通信和同步的效率，本课题拟基于软硬件计算资源的执行特征差异设计具有针对性的通信和同步机制。为了降低任务调度开销，本课题拟设计一种分级的调度算法，对异构处理单元上的任务采用不同的调度策略。为了缓解编程墙问题，本课题拟提出一种面向服务的编程模型，为程序员提供透明的编程接口。最后，本课题将在实际可重构平台上对提出的操作系统构造模型进行验证和分析。本课题的研究内容是一次将可重构计算技术与传统操作系统相结合的崭新尝试，具有重要的理论意义和应用价值。
基于热力学属性的计算编码序列的研究与应用	编码序列;物理化学属性;DNA;热力学属性;计算	随着DNA计算研究的逐步深入，编码问题的重要性愈加凸显，它在一定程度上决定着DNA计算模式的未来走向。早期通用的随机定长编码方案本身所固有的不可移植性和偶然性等特点严重地制约了DNA计算的发展。从分子生物学角度看，DNA序列自身的物理化学属性决定了编码序列的存在形式；而编码序列的热力学属性则是后续生物化学反应的动力源泉。基于此，本项目拟较为系统地研究适用于DNA计算的较短的(15-100bp)DNA序列的热力学属性和物理化学属性；探求各约束条件之间的内在联系；并对其进行整体优化组合。然后在此基础上，构建计算编码序列的通用设计平台，该平台包括：设计标准、设计策略、编码序列生成软件以及编码序列评价体系等；并以此为指导在生化实验室中用于解决图与优化组合中的某些实际问题，通过解决这些问题，对原编码方案寻求进一步的优化与完善；最后争取就某种NP困难类问题在计算规模上寻求突破。
暗硅时代新型热可靠众核系统的性能与能耗优化关键技术研究	热可靠性;暗硅;软硬件协同设计;片上网络;异构众核片上系统	随着纳米工艺和芯片集成技术愈趋成熟，片上多核系统的发展进入异构众核时代。单个芯片上集成十余个乃至数百个具有不同计算能力和能耗表现的计算核心，将为日益复杂的应用场景提供强大的可扩展能力及更高的性能能耗比。然而，受到芯片散热、封装和供电能力的制约，部分计算核心必须动态地关闭或运行在低功耗状态下以保证系统可靠性，即暗硅现象。为解决暗硅给异构众核系统的热可靠性、性能与能耗间的协同优化带来的全新挑战，本课题创新地提出一套软硬件协同设计方法和技术：硬件上提出三种不同的片上互连网络的体系结构设计，用于均衡芯片的热可靠性与通信性能间的矛盾；软件上分别提出相匹配的任务映射算法，以充分利用新的硬件设计，结合新提出的集群式异构众核管理机制，实现应用执行性能与系统能耗的优化；软硬件结合实现了系统热可靠性、性能与能耗三者间的协同优化与最大化。本课题的研究成果将为高性能低功耗异构众核片上系统的设计提供有力的技术支撑。
基于虚拟机的微体系结构级仿真原型快速构造方法研究	虚拟机;微体系结构;仿真原型;处理器	在现代处理器的设计过程中，仿真原型广泛应用于体系结构的性能评价、硬件设计的功能验证以及软件设计的仿真调试。然而，尽管目前有大量公开源代码的仿真原型可用，但其中绝大多数都是针对特定ISA处理器的特定体系结构编写，研究人员在构建自己所需的仿真原型时需要修改大量代码，花费大量时间调试验证。特别是微体系结构级仿真原型，由于要描述周期精确的数据通路和控制通路，需要修改的代码将会更多。本项目的目标是研究一种基于虚拟机的微体系结构级仿真原型快速构建方法，可以利用时序仿真与功能仿真耦合分离的原理，使用现有各种虚拟机作为功能模型，快速且准确地构建研究人员所需的特定微体系结构仿真原型，包括多线程、多核、异构多核、PIM、流处理体系结构等。除此以外，为加快仿真速度，该仿真原型还可将时序模型部分使用FPGA进行硬件加速，相对传统纯软件仿真原型的加速比预期达到100以上。
共享存储多核系统中的确定性重放机制研究	访存记录;并行调试;多核系统;确定性重放;访存冲突	在共享存储多核系统中，并行程序往往存在对共享内存的冲突访问，从而导致程序每次执行的路径不尽相同，这种不确定性是并行程序调试中的一个主要障碍。通过硬件支持使得并行程序每次执行路径一致，即实现确定性重放是解决该问题的有效方法。本课题研究如何在共享存储多核系统中高效地支持确定性重放，主要研究内容包括：（1）如何使得访存记录数量少，研究通过建立精确的访存依赖关系来抑制访存记录信息的产生，以减少记录条数；（2）如何使得重放执行速度快，研究通过记录偏移逻辑时钟的方法来减少重放时的并行度损失，以提高重放执行速度；（3）如何使得硬件结构简单，研究利用Bloom-Filter机制来高效记录访存信息，以降低硬件逻辑开销。通过上述三方面的研究，本课题将提出一种访存记录数量少、重放执行速度快、硬件逻辑结构简单的确定性重放机制，以高效地支持并行程序调试。
云计算安全关键技术理论、仿真和实现方式研究	DDoS攻击;虚拟资源安全;隐半马尔可夫模型;云计算安全	从提高服务质量和保证虚拟资源安全性出发，对云计算安全的关键技术进行研究：（1）利用HSMM（Hidden Semi-Markov model）研究云数据中心系统各种流的统计特性和用户行为特征，以及研究在时间和空间上相关的、动态变化的网络行为，为实现对流的分类控制和总体调度技术提供基础；（2）研究云数据中心系统内部虚拟资源的安全模型、监控方式、异常处理技术，为虚拟资源安全提供保证，为云计算系统的网络安全决策提供依据；（3）研究透明的在线排队与流量控制技术以及动态的负载均衡方法，化解针对云计算系统的DDoS攻击，保证异常和正常时的云服务质量；（4）I/O流的异常处理技术和虚拟资源异常检测算法在实际云计算系统中的应用研究。 本项目的研究方法与成果，为下一代网络存储系统的网络行为与安全研究提供一种有价值的参考方案，为云计算数据中心的安全系统设计提供一种新的方法和思路，具有重要理论意义和重大经济价。
支持高效流处理的通用CPU体系结构关键技术	通用CPU;编译优化;编程模型;数据并行;流处理	近年来，伴随着视频压缩与互联网技术的广泛普及，多媒体计算蓬勃兴起，逐渐成为通用CPU的主流计算之一。它们对计算能力的急剧需求给通用CPU体系结构的设计带来了新的挑战：计算能力的提升严重滞后流处理计算要求，并且现有存储结构不能满足其数据访问行为特征。虽然以流处理器为代表的专用处理器针对此类应用展现出了巨大的计算性能潜力，但是其不能满足通用性需求。本课题针对多媒体应用日益提升的计算需求与通用CPU相对滞后的流处理能力之间的矛盾，引入流体系结构特征，提出了支持高效流处理的通用CPU体系结构，为有效组织和利用丰富的芯片资源提供有力支持。将在高效能通用CPU流存储结构、超高位宽数据并行加速器设计、支持流处理的高效能编程模型及其编译优化技术、通用CPU流模型的多核扩展方法等不同的层面，深入研究支持高效流处理的通用CPU体系结构关键技术，为未来通用CPU的发展奠定坚实的基础。
基于非易失性存储器的低能耗数据存储方法研究	绿色存储;高性能;非易失性存储器;低能耗;分布式系统	近年来，新型非易失性存储器（NVM）因其具有非易失、高性能、低功耗、高可扩展性等特点，成为近期分布式存储的热点研究问题。然而，NVM具有读写不对称性性，即写操作代价要比读操作代价高很多，从而导致较高的写能耗。因此，如何基于NVM设计高效的数据分布与存储优化算法来降低能耗同时保证存储性能、改善存储寿命是一个非常重要的问题。本课题着眼于"降低能耗"这一绿色存储的关键问题，首先分析现有低能耗数据分布与存储优化算法中存在的问题，基于NVM与应用数据个体信息，建立数据存储访问模型，并设计能耗有效的数据分布与存储优化准则；其次根据应用实时性等性能需求，为平衡性能改善与能耗降低，设计性能保证的低能耗数据分布与存储优化算法；最后，在保证存储寿命的前提下实现低能耗数据分布与存储，设计基于写感知的数据分布算法与数据更新策略。课题的研究为未来存储管理技术提供重要的借鉴。
传感器网络中不依赖MAC认证的虚假数据过滤机制研究	分布式协同水印;传感器网络;虚假数据过滤	传感器网络的主要任务是收集传感数据，如何有效过滤虚假数据，是一个具有挑战性的问题。已有虚假数据过滤算法一般是在数据包后附加t个认证码，这类算法不具备"鲁棒但脆弱"的特性、难以支持网内有损数据处理、抗干扰能力差且通信开销大。本项目结合无线传感器网络节点协同工作、分簇采集数据和网内进行数据处理等特点，利用数字水印技术在数据认证方面的优势，研究不依赖MAC认证的虚假数据过滤机制。项目依据传感器网络的特点和虚假数据过滤的需求，建立分布式协同水印的理论模型；然后研究分布式协同水印的鲁棒性和脆弱性问题；在此基础上，提出基于分布式协同水印的虚假数据过滤算法，针对传感器网络中的重放攻击和网络节点的动态增加与撤离等特点，进一步研究分布式协同水印所需的密钥预置机制、水印实时更新机制和抗重放攻击策略等，最终形成不依赖MAC认证的虚假数据过滤机制。项目采用仿真和实验相结合的方法对提出的模型、算法和机制进行评价。
车载自组网实时协作定位系统及多数据源融合算法研究	实时;协作定位;车载自组网;数据融合	满足应用环境复杂性要求的车辆实时协作定位技术是车载自组网(VANET: Vehicular ad hoc networks)应用的关键环节。本研究针对车辆运动速度快、定位时延问题突出等实际情况，提出满足实时、可靠要求的车载定位解决方案。首先，针对VANET的拓扑动态特性和定位的实时性要求，研究网络动态性、异构性和连通性等要素之间的制约关系，提出多尺度实时评价策略，研究车载定位的实时性分析方法，建立实时性评估模型。通过挖掘城市环境车辆运动特点，提出具有实时保证的车辆协作定位模型及其实时定位算法。通过分析网络层行为对协作定位的影响，研究多信道动态频谱分配问题，为车载协作定位的实时性提供网络保障。最后，对支持多信息源定位的数据融合机制进行研究，给出实时数据融合模型，据此提出支持容错和实时的融合体系结构，进而构建、优化实时定位系统仿真和物理验证平台，支撑车载定位面向实时、可靠的应用部署。
面向基于FPGA的细粒度可重构混合系统的编译技术研究	循环的映射;细粒度可重构计算;软硬件划分;自适应存储结构;可重构编译	细粒度可重构混合结构逐步成为高性能计算机体系结构发展的重要趋势之一，可重构编译技术是其应用与推广的关键。针对目前可重构编译技术中，软硬件划分准确性差、生成的硬件结构并行度低的问题，本课题基于粗粒度软硬件划分技术、并行存储技术及并行编译技术，结合面向FPGA的细粒度可重构混合计算特点，主要研究：基于簇和Profiling信息反馈的软硬件划分模型；面向FPGA硬件结构的编译优化技术及效能评价模型；基于"超标量"并行流水调度和脉动阵列结构的多层循环映射；面向FPGA的并行存储映射算法及自适应调度算法；对各种编译技术的有效性和正确性进行评价的验证平台。此课题的研究将为设计实现自动化程度高的细粒度可重构异构多核处理器的编译器奠定坚实的理论和技术基础，具有重大的理论和现实意义。
移动互联网的用户隐私保护研究	移动互联网;分级保密;匿名认证;隐私保护;位置隐私	移动互联网已进入广泛应用阶段，而由移动服务的开放性导致的隐私泄露等安全风险也日益突出。本项目将对移动互联网的用户隐私保护问题展开研究，主要内容包括：（1）用户信息隐私保护研究。通过对移动互联网隐私泄露攻击路径的回溯分析提取信息敏感性要素，合理描述各要素之间的关联关系，建立信息敏感性评价标准及度量模型；定义映射函数，按敏感度对信息进行分级；设计具有多级安全强度的保密机制及算法。（2）用户身份隐私保护研究。针对匿名性、可追踪性、高效性和移动性的需求，基于伪名、盲签名、聚合签名等技术，利用椭圆曲线密码设计具有可追踪性的轻量级匿名漫游认证协议。（3）用户位置隐私保护研究。基于匿名和位置泛化技术设计移动互联网的用户位置隐私保护方案，达到安全强度、位置精度和系统性能之间的良好平衡。本项目的研究为移动互联网用户提供多方位的隐私保护方案，促进移动互联网应用的健康发展。
基于新型高密度赛道存储器的缓存设计与优化研究	缓存纠错;高能效缓存体系结构;缓存建模仿真;赛道存储器;新型非易失存储	赛道存储器是一种高密度的新型非易失存储器，它同时具备能够媲美SRAM的读/写访问速度。因此，在缓存结构中使用赛道存储器，能够有效提高缓存的容量、性能并降低功耗。然而，赛道存储器的设计复杂，并具有一些传统缓存结构不适应的新特性。尤其是它独特的"移动操作"，是影响其效率的一个主要潜在障碍。本课题研究如何利用赛道存储器设计缓存结构，主要内容包括：（1）赛道存储器电路级建模及设计优化研究，（2）面向赛道存储器缓存的体系结构优化研究，（3）针对移动开销的编译优化研究。通过这三个层次的研究，可以针对不同系统需求选择合适的设计，同时利用结构级和编辑优化技术进一步提高缓存的性能和可靠性并降低功耗。
网络病毒的追踪与寻源技术研究	信息安全;网络病毒;病毒追踪;网络安全	比之单机病毒，网络病毒具有专门的网络传播机制，这使得它们的寄生平台扩充到整个网络，而现有的杀毒工具却没有像网络病毒那样充分利用网络的优势，杀毒是被动、孤立的，所以无力应对网络病毒的泛滥。课题将网络看成一个整体，以发现感染病毒的节点做为寻找病毒传播路径的突破口，依据病毒的"来龙去脉"将病毒的查询分为病毒的寻源和追踪两部分，充分利用网络的连通和信息共享的优势，分别建立病毒的追踪和寻源方程，通过对方程的计算求解，就可以由一个感染点揪出病毒的一个感染链，同时也可以找到病毒的源头。课题的主要内容和关键技术包括:研究、分析网络病毒的网络传播机制和主要网络应用连接平台的通讯协议；建立、完善病毒的追踪方程和追踪算法；建立、完善病毒的寻源方程和寻源算法；收集相应的网络连接信息验证算法的有效性。课题的难点在于制定寻源方程解存在的唯一性条件以及条件不能满足时的解决办法；分析时间跨度对方程求解的影响。
基于大规模异构众核系统的代数多重网格解法器算法研究	稀疏矩阵;扩展性;异构众核;代数多重网格	异构多核体系结构已经成为构建大规模并行计算机系统的一个重要的技术路线，国内外已经有多台基于异构加速部件的千万亿次超级计算机，如何设计基于大规模异构系统的高扩展并行算法是迫切需要研究的课题。考虑到代数多重网格（AMG）方法在高性能计算领域的重要性，本项目拟研究基于大规模异构众核系统的AMG 解法器算法：1）针对AMG中的稀疏矩阵向量乘核心，研究矩阵模式和体系结构特征在执行性能上存在的规律，以充分发挥加速部件的计算能力；2）考虑到异构系统中体系结构级并行的多样性，研究AMG 算法中多级并行的最优映射和粒度选择的方法，以有效利用大规模异构系统中的并行性；3）针对异构系统中通用和加速部件在处理能力上的差异，研究通用和加速部件协同调优的高性能AMG 算法，以提高并行程序在全系统规模的扩展性。通过本项目的研究，将在大规模异构多核系统上实现高扩展、高性能的AMG 并行算法和应用程序，促进高性能计算的发展
嵌入式处理器细粒度安全运行机制研究	行为监控;完整性检测;嵌入式系统;安全运行机制;污迹追踪	以病毒、蠕虫以及木马为代表的恶意软件攻击具有成本低廉、危害广泛的特点，是嵌入式系统在实际应用中所面临的最具威胁性与破坏性的攻击方式，传统的软件防护策略已无法满足嵌入式系统应用环境中的新型安全需求，因而迫切需要寻求一种新的防御方法。 课题拟从嵌入式处理器的体系结构入手，采用计算机防侵入技术中的完整性检测以及行为阻断的思想，对嵌入式处理器中所运行程序进行指令级监控，建立细粒度的安全运行机制，解决恶意程序所带来的运行安全性问题。与以往单纯的安全性研究所不同的是：本项目拟在虚拟执行部件的基础上，从程序的完整性、指令的运行行为以及数据运行轨迹等角度出发，在嵌入式处理器内构建多层次的防御机制并对其底层硬件进行相应的延伸性研究。其研究结果还可以应用于其他安全敏感的电子设备中，为保密通信、网上银行、电子商务、电子政务等系统的安全运行提供基础支持。
多核帮助线程应用中高性能的预取质量确保机制研究	多核数据密集应用;帮助线程预取控制;预取质量确保机制	帮助线程技术是多核数据密集应用中访存性能优化的重要研究方向之一，预取控制策略是它的核心，预取过程中的负获益是这类应用的重要性能瓶颈。本研究拟在我们提出的支持跳跃式预取的数据块同步方法基础上，以高性能的预取质量确保机制为目标，利用流计算方法对获取的预取质量数据进行高效的筛选与关联、分类与转换等动态分析操作，开展预取正获益和负获益的状态判定、度量与改进、支持可变参数和阈值的全方位预取控制调优等预取质量确保机制研究，使预取过程中正获益最大化和负获益最小化，满足高性能访存的需求。我们拟从控制参数与阈值的快速优选、预取过程中缓存管理的分析入手，提出帮助线程应用中预取质量确保机制和关键算法，采用多核环境、多核模拟器、软件算法和测试程序等方式进行实验分析与评价，并结合建模分析，探讨其理论界限。
嵌入式多媒体流计算的质量驱动机制与共生调优	嵌入式系统;流计算;多媒体系统;共生调优;质量驱动	目前常用的嵌入式多媒体流计算系统缺乏质量驱动的动态优化机制，不能主动增强应用资源需求的相容性，导致系统资源利用效率不高。本项目根据多媒体流计算的质量可伸缩性，提出结合主动调节计算服务质量的双向跨层适应来替代传统的单向被动适应，实现服务质量和计算资源的联合共生优化。研究内容包括动态流计算的系统分析模型，有限系统资源的协同优化管理方法，分析跨层交互作用的理论框架和支持多层联合优化的跨层适应机制。创新性地提出场景感知数据流图描述流计算的动态行为，以模型驱动的灰盒式合作资源分配替代原有的黑盒式竞争资源分配，实现对共享资源的合理分配与高效利用。建立非线性回归模型来定量分析跨层交互的复杂行为，从而获得精确的跨层优化模型；并基于马尔柯夫决策过程进行跨层适应，以优化系统的长期效用。这种计算模式具有随应用需求和系统资源变化自行向最优工作点迁移的能力，可节省能耗。研究结果具有重要的理论意义和广泛的应用前景。
基于人类动力学的城市交通拥堵建模技术研究	系统建模;交通拥堵;人类动力学;复杂网络	城市交通拥堵已经并将在未来很长一段时间内成为大中城市的一大痼疾。同时，原本被认为具有随机性的人类行为（如出行距离等）已被证明具有规律性，并成为人类动力学研究的热点。因此，我们猜测人类行为与城市交通拥堵之间具有内在关系。为此，本课题创新地提出了利用了人类动力学理论来研究城市交通拥堵问题，以期建立人类行为与城市交通拥堵之间的定量分析模型。首先，根据车辆行为是由人类行为所驱动的事实，研究在城市交通中人类行为模式与交通拥堵的关系。其次，利用复杂网络的拓扑理论，建立城市交通网络的数学模型；结合人类行为中的出行时间规律、出行距离规律和城市人口密度分布模型，以网络拥塞理论为切入点，建立城市交通拥堵模型，并进一步研究其动力学过程。最后，根据建立的拥堵模型，建立基于人类行为的城市交通仿真平台，研究城市交通拥堵发生的机理和防治手段，从而为解决城市交通拥堵、合理规划城市建设和优化城市交通设计提供决支持。
基于表面等离子激元理论和方法的DNA计算生物芯片杂交信号研究	计算模型;DNA计算;DNA检测;表面等离子体共振	目前实用化DNA计算机研制过程中，其关键难点是解的检测问题。大多数DNA计算的检测技术主要是利用扫描电镜，原子力显微镜，PCR扩增技术等，但是由于检测精度、检测范围和仪器适用条件的限制，亟需发展实时、高精度的DNA-纳米探针杂交检测技术。本项目利用局域表面等离子体激元信号（LSPR）检测金属纳米颗粒经由DNA分子连接后的光学性质，它的信号强度以及光谱表征了DNA分子的杂交情况。目标为探索DNA-纳米颗粒聚合体系中，检测以纳米颗粒组装形状的不同形状带来的光学信号，实时获取核酸连接过程的信息，研究核酸连接动力学过程。利用该方法可以检测出DNA杂交反应前后表面等离子体激元信号的变化，即可输出DNA杂交信号。为研究自动化的DNA计算机中解的检测问题提供了新思路。
云计算环境下移动Agent系统信任安全关键技术研究	移动计算;可信计算;云计算;安全评估;系统安全	目前，"云计算"成为学术界、产业界和政府部门等各界关注的焦点，预示着全球信息化进程进入资源集成化、应用规模化、服务专业化发展阶段。移动Agent计算范型为"云计算"环境下应用系统的设计与开发提供了普适参考模型。但由于云环境的虚拟性、动态性、开放性以及公用性，给移动Agent范型的应用带来了极大的安全性挑战。本项目主要研究与探索在云计算环境下，移动Agent系统设计中的信任和安全问题。具体研究移动Agent系统中实体信任关系的形成、传播与进化规律，建立一个动态信任量化计算模型；构建一种增强的移动Agent系统安全保护方法；基于"CC信息技术安全评价共同标准"，提出一种移动Agent系统安全功能确信度量评价方法。本项目研究将有望解决云计算环境下移动Agent信任、安全与保护难题，丰富移动Agent的理论与算法，拓宽云计算在互联网产业中的应用范围，为提高信息网络安全保障能力提供理论和技术支持。
异构GPU集群混合粒度任务协同调度与动态均衡机制研究	动态均衡;混合粒度;GPU集群;协同调度	GPU集群计算技术是目前国内外高性能计算研究的热点，对生物、金融、气象等需要进行大规模数据处理的领域具有重要意义。虽然通用并行计算架构如CUDA能够有效地发挥GPU的计算能力，但这些加速（协）处理器的使用带来了新的通信和存储等问题，使GPU集群整体的计算能力难以得到高效利用。本课题从计算、通信和存储三方面考虑与性能相关的体系结构因素，建立适用于异构GPU集群的性能模型；通过分析GPU集群上典型应用的计算模式，提出面向异构GPU集群的混合粒度任务模型，在此基础上实现任务协同调度与动态均衡机制；基于分布式数据管理机制和高效通信机制实现支持多种调度策略的高性能计算框架，并采用大规模计算问题进行效能测试与调优。课题针对GPU带来的新结构特征和编程模型，从提高异构GPU集群计算效能角度提出混合粒度任务调度与动态均衡机制，为异构GPU集群大规模计算研究和应用提供新思路和方法。
基于深度学习的遥感图像分类及其CPU/GPU异构并行系统结构研究	深度学习;GPU;并行计算;遥感图像处理	如何对大规模多源遥感数据进行高效分类是遥感领域近年来的研究热点。相比传统方法，深度学习可以更有效的学习遥感地物特征从而提高分类水平。利用深度学习对大规模遥感图像进行高精分类的任务量大，计算密集且复杂，传统的串行处理模式已无法满足应用实时性需求。CPU/GPU异构并行体系结构可以满足应用对计算资源的不同需求，是未来高性能体系结构的发展方向之一。本项目拟面向CPU/GPU开展基于深度学习的大规模遥感图像分类研究，以提高分类效果和处理速度。主要研究内容：1）设计无监督深度卷积网络对大规模遥感数据的时空不变性特征进行有效学习；2）设计应用先验知识的深度学习遥感图像分类方法以提高分类精度；3）研究面向CPU/GPU平台的深度学习遥感图像分类多级混合并行算法，以及适于此类结合邻域分析和复杂机器学习的细粒度并行技术。本项目研究将有效提高遥感图像分类处理的业务水平，推动深度学习,遥感应用与并行计算的发展。
面向基因组重测序分析的混合异构体系结构关键技术研究	生物信息学;异构体系结构;高性能计算;可重构计算;基因组重测序分析	基因组重测序分析是生物信息学乃至现代生命科学领域重要的基础性研究工作。随着高通量测序技术的发展，序列数据量急剧膨胀，海量数据处理已成为该领域面临的严峻挑战。此外，该领域的应用具有程序特征多样化、数据相关多维度、访存行为不规则等特点。通用结构计算机虽然能够提供很强的峰值计算能力，但不能完全适应该领域复杂计算特性的特殊要求，计算效率不高。.课题以基因组重测序分析典型处理流程的高时效性需求为背景，从并行算法设计和体系结构设计两方面出发，在深度研究基于CPU-GPU和CPU-MIC两类通用异构计算平台和基于FPGA的可重构计算平台的并行算法设计与优化技术的基础上，构建包含GPU、MIC和FPGA三种不同特性的算法加速器的混合异构计算平台，实现不同计算单元的体系结构特征与数据分析流程不同环节计算特征的适配，从而有效提高数据分析流程的整体计算性能，实现对基因组重测序分析全流程、系统化的加速解决方案。
资源全局共享的存储阵列关键技术研究	磁盘阵列;磁盘存储系统;虚拟存储设备;数据布局;大规模存储系统	RAID技术使得逻辑卷的能力不再受单个物理设备的限制，自从提出以来一直作为标准存储架构而被广泛采用。然而，磁盘工艺技术的发展将传统RAID 保护推到了能力极限。磁盘容量带宽比的提高导致传统RAID的重构时间明显加长，降低了存储服务质量并增加了数据丢失的风险。传统RAID数据保护能力不足背后的原因是卷组之间的IO资源隔离。RAID 重构所需的读操作局限于单个卷组内部，导致该卷组内的所有逻辑卷性能都受到很大影响；恢复数据的写操作局限于单个热备盘，使得该热备盘容易成为RAID 重构的性能瓶颈。因此，申请者提出资源全局共享的存储阵列关键技术研究，将紧密结合构建存储阵列的目标本质，深度挖掘新时期磁盘的介质特征，主要研究：全局均衡的存储阵列数据布局理论；全局并行的存储阵列快速重构方法；资源全局共享的存储阵列快速扩展技术。最终目标是实现高性能、高可靠、易扩展的新型存储阵列，全面满足新时期的存储需求。
面向汽车的CPS中混合关键级调度模型及算法研究	信息物理融合系统;混合关键级;汽车电子系统;调度算法;实时性	混合关键级CPS的同一硬件平台上将同时集成具有不同关键级的多个功能，它们在实时性、安全性和可靠性等方面具有不同要求，但是现有嵌入式系统设计假设系统中所有功能的关键级相等，因此混合关键级CPS对已有嵌入式计算理论和方法提出了严峻挑战。为此，本项目研究功能级的混合关键级调度模型，对关键级与实时性、系统资源和成本之间的关系进行形式化的建模和描述；研究融合时间触发和事件触发的混合关键级调度算法，实现系统资源的静态配置和动态管理之间的有效融合和多个非功能属性的联合优化；研究安全、准确的端到端实时性分析算法，解决网络化和集成化带来的实时性不确定性问题。最后，通过搭建物理原型平台，对提出的模型和算法的有效性进行分析和验证。通过本项目的研究，可在保障混合关键级CPS安全、可靠运行的同时，促进其高效实现和提高其服务质量。研究成果可为混合关键级CPS在汽车、航空等关键工业领域的应用和其下一步发展提供理论依据。
云计算环境中内存的弹性分配和调度	内存;换页;调度;弹性分配;云计算环境	虚拟化云计算平台通过弹性调度来实现各种资源的过量使用，从而进一步提高资源利用率。其中高效的虚拟机内存资源弹性分配和调度是提高整个云平台效率的关键问题之一。虽然已经有许多有效的虚拟化内存管理技术来提高云平台中内存利用率，但都无法直接有效解决客户机换页带来的性能损耗和跨物理主机的内存调度。本课题从客户机换页问题出发，采用对客户机操作系统完全透明的方法，综合使用本地内存、网络内存和本地磁盘构建多级的客户机换页机制。一方面基于所构建的性能参数模型通过气球技术动态回收各个虚拟机内的闲置内存，另一方面通过内存换页的灵活方式打破单个物理主机的限制对内存紧缺的虚拟机进行资源调度，并通过优化策略促进内存资源按需地在各个虚拟机之间高效流动，从而综合平衡整个云平台的内存分配。本课题研究将为内存的弹性共享与分配研究增加新的思路和方向，进一步提高内存资源整体利用率，降低服务成本，具有良好的实用价值和显著的应用前景。
众核处理器片上网络的功耗优化方法研究	异构NOC;可重构NoC;片上网络;片上网络功耗优化;片上网络低功耗设计	片上网络，由于能提供较大数据传输带宽和良好的可扩展性，是众核处理器片上互连的主要发展方向。然而，随着集成电路工艺的细化和处理器规模的不断增大，片上网络的功耗开销也在大幅增加。本项目主要面向片上网络的功耗优化方法，并计划从节点级、子网级和系统级三个角度提出跨层的功耗优化解决方案：（1）在节点级，通过链路重构，解决门控功耗技术带来的连通性破坏问题，降低门控功耗技术的性能开销；（2）在子网级，通过对应用程序运行阶段的预测分析，动态改变子网的带宽和供电电压、频率，在保证应用程序网络性能需求的同时降低功耗；（3）在系统级，通过分析处理器核与片上网络性能指标的相关性，对两者的功耗进行协同优化，达到系统级的能效最优。通过节点级，子网级和系统级的跨层功耗优化解决方案，提出了包括穿梭片上网络体系结构，数据流形态预测技术等多项创新方法，用于探索片上网络功耗降低的下限，并提升众核处理器系统的整体能效。
两层结构传感器网络中的安全查询机制研究	两层结构传感器网络;二元对称多项式;安全查询;链式水印	本项目以两层结构传感器网络中的安全查询机制为研究背景，提出一种新的基于二元对称多项式模型和链式水印的安全查询机制。依据两层传感器网络的结构特点，重点研究网络中的隐私保护、查询结果完整性与正确性认证和容忍节点失效等安全查询问题，并提出相关的技术方案和路线。针对妥协存储节点泄露敏感数据的问题，提出基于二元对称多项式模型的隐私保护方法，由此保证存储节点上的数据安全且支持精确查询；针对妥协存储节点返回不完整查询结果的问题，提出一种新颖的链式水印算法，解决查询结果的验证问题。针对传感器失效或未感知到数据而产生的虚警现象，建立相关度分类和节点信誉度评价模型，提出低虚警的安全查询算法。最后形成能支持隐私保护与查询结果验证的安全查询机制。项目将采用仿真和实验相结合的方法对提出的算法和机制进行评价。该课题的研究，将有助于推动两层结构传感器网络的应用，促进传感器网络安全技术的发展。
精确快速的结构级软错误量化关键技术研究	软错误;概率图模型;体系架构敏感因子;多位翻转	随着集成电路工艺尺寸的不断缩小，更高的软错误率和复杂的多位翻转模式对芯片设计带来的挑战日益严峻。目前针对处理器结构的软错误量化方法存在评估过程耗时过长和量化结果精度过低的问题，难以为高可靠微处理器提供精确快速的容错设计指导。针对这些问题，本课题提出了一套面向微处理器架构的软错误量化评估方案，通过程序指令分析获得错误逻辑屏蔽关系，以此建立关键存储结构的敏感状态概率转换图，并利用概率图模型实现特定部件的软错误量化指标的精确快速计算。与现有类似方法相比，本课题研究的优势体现在：1）全面分析了软错误在存储部件中的传播效应和屏蔽效应，结合涵盖"一位翻转"和"多位翻转"错误模型的统一形式化描述方式，使得评估结果的精度大幅提高；2）将概率图建模方法引入软错误量化分析中，结合应用场景分析制定了合理的模型化简机制，有效加速了大规模变量情况下的边缘概率求解，从而保证了关键存储部件的精确软错误量化值的快速获取。
智能终端虚拟化环境中新型存储系统管理与优化关键技术研究	新型非易失性存储器;嵌入式系统;闪存;智能终端虚拟化;相变存储器	智能终端虚拟化技术可以为个人隐私、企业管理及公共安全等领域应用构建安全可靠的系统环境，是未来智能终端技术发展的新方向。但传统虚拟机对存储系统的共享模式和访存需求给智能终端内存和外存管理与设计带来了新的挑战。在资源受限的智能终端虚拟化环境中，如何高效地满足各虚拟机的访存需求是迫切需要解决的关键问题。本项目基于新型非易失性存储技术在存储密度、能耗、速度和非易失性等方面的优势，研究面向智能终端虚拟化环境的存储系统管理与软硬件协同优化技术。从系统性能、能耗和存储空间等多方面，分别在内存管理、文件系统以及外存管理等不同层次研究虚拟机感知的内存管理技术、虚拟机检查点快照存储优化技术以及虚拟机镜像文件存储优化技术，并构建原型验证平台，实现关键技术验证。本项目有望为解决终端虚拟化环境中存储系统的性能瓶颈问题提供新思路，为终端虚拟化研究和应用提供有用的理论和技术成果。
面向死锁检测与控制的无界Petri网复可达树技术研究	死锁预防;Petri网展开;可达树;Petri网;死锁检测	死锁检测与控制在分布式并发系统设计与运行中具有重要作用，Petri网可达树是进行死锁分析的主要手段之一。然而，现有可达树仅适用于特定的Petri网子类，本项目拟提出一种应用范围更广、可用于无界Petri网死锁检测与控制的新型可达树—复可达树，并以此为基础研究无界Petri网的死锁检测与活性监督控制器设计技术。研究内容包括：（1）提出面向死锁检测的无界Petri网复可达树构造规则，设计并实现相应的构造算法；（2）以复可达树为基础，给出无界Petri网死锁检测的具体算法，并设计相应的活性监督控制器；（3）将Petri网的有限完全前缀展开技术推广到无界Petri网，以此降低状态爆炸问题给无界Petri网死锁分析带来的困难。通过本项目，将Petri网的死锁检测与控制技术从有界Petri网或特定无界Petri网子类向更为一般的无界Petri网进行推广，这无疑具有重要的理论价值。
大规模服务传感器网格中基于QoS的能效资源调度技术研究	网格计算;能效资源调度;资源可靠性;能耗与QoS平衡;面向服务的传感器网格	随着无线传感器网络和网格计算的发展及融合,传感器网格已成为国内外研究热点。传感器网格具有广泛的应用领域，如国防，医疗，工农业，环境观测，灾害预报等。如何高效调度传感器资源进而提高传感器网格资源管理性能，是亟待解决的重要研究课题。本项目旨在为服务传感器网格中基于QoS的能效资源管理与调度提供新的方法。定义传感器网格能量效益函数，以反映网格用户QoS满意度及能耗之间的关系。运用能量效益函数研究QoS满意度和能耗平衡的传感器网格资源调度算法。研究基于负载均衡及QoS感知的传感器网格资源节点选择策略，设计基于性能预测模型的资源节点选择算法。研究传感器网格中基于价格机制和效用公平性的QoS约束能效资源调度优化，建议基于资源可靠性及任务分类的能效传感器资源调度策略。设计基于效用优化的QoS能效传感器网格资源调度算法，最大化系统效用，实现传感器网格用户公平性。研制适合传感器网格的模拟器平台及实验床环境。
并行光学逻辑运算器与并行光互连的融合研究	高性能计算机;融合一体化;并行光互连;并行光计算;光学运算器	发展并行光计算技术被认为是突破电子计算机性能提高主要瓶颈的最佳途径，将推动高性能计算机的可持续快速发展。光学运算器和并行光互连是并行光计算技术的基础性核心构件和关键技术，两者的融合符合运算的互连网络化和互连网络的多功能化发展趋势，有利于促进并行光计算关键技术在高性能计算机中的应用，加速并行光计算机研究进程。.  结合多交叉学科分析原理和方法，对基于VandLugt光学相关器的并行光学逻辑运算器和自由空间全混洗交换并行光互连之间的光学共通性进行分析，找出两者在结构与功能上的互补与互用方面，实现融合一体化复合单元结构设计，并建立适合的分析与测试方法模型，对融合一体化实验系统进行分析测试。.  并行光学逻辑运算器和并行光互连的融合研究结果，为集运算与互连于一体的多功能互连集成模块的研制提供依据，支持高性能计算机的可持续发展，并为未来并行光计算系统体系结构的建立奠定基础。
DNA分子纳米组装体和计算机模型研究	自组装;NP-完全问题;检测;纳米孔;DNA纳米机器	构建DNA纳米计算机对认识纳米尺度下的能量转换规律、分子相互作用与调控机制、发展高性能计算有重要意义. 本项目以DNA分子作为纳米计算机的材料, 重点研究: (1) 基于界面吸附能力和DNA计算的序列设计与优化计算体系. 该体系包括编码的类型与功能、编码计算模型、编码算法相应的软件以及各种编码的应用等; (2) 结合分子计算基本原理设计多种特定功能基修饰的DNA分子作构筑单元, 发展定向、维数可控、大面积、高有序复杂结构自组装技术, 研究DNA分子结构与纳米结构的变化规律和功能单元结构与性能的关系; (3) 采用现代测试手段, 研究纳米计算的基础科学问题, 尤其是反应介质及温度、pH值、模板等组装条件对纳米组装体形成的影响和解的检测问题. 结合纳米孔和DNA纳米机器,  建立基于DNA碱基超灵敏检测的多模式检测方法; (4) 建立实用化的大规模型求解NP-完全问题的DNA纳米计算机模型.
面向物联网的混合式无线传感网容侵结构及关键技术研究	无线传感器网络;物联网;安全协议;容忍入侵	本项目以物联网应用框架为背景，研究该背景下混合式无线传感网的容忍入侵结构和容侵关键技术。本项目重点研究三个方面，首先是一种隐层支持的面向物联网的混合式传感网的容侵结构，其关键难点在于面向物联网的多基站、多类型节点的跨区域混合式网络结构，该结构区别于现存的无线传感网、无线局域网和无线ad hoc网络,因而该网络结构下的容侵的安全结构研究是新颖而且富有挑战的；其次是在该结构下挖掘移动节点在入侵检测、网络容侵能力修复方面的作用，其中移动节点跨基站、跨区域的漫游巡查，其安全接入、节点认证安全和组合安全问题是研究难点；最后，研究在恶劣环境下，节点损坏/变节对网络容侵能力的影响，其难点在于多个节点变节或者损坏对网络的影响不是单个节点影响的简单相加，网络安全及容侵能力的下降应是一类新型相继故障模型。本项目的可行性在前期研究工作中得到了验证，而相关领域的研究进展也为下一步研究的技术可行性提供了支持。
面向在线数据密集型应用的高效数据存储与复杂查询关键技术研究	分布式内存存储;在线数据密集型应用;应用层网络虚拟化;复杂查询;全局本地化	以网络搜索、电子商务、社交网络等为代表的在线数据密集型（OLDI）应用逐渐成为工业界和学术界的研究热点。除了传统数据密集型应用的"大数据"特点外，OLDI应用还具有延迟受限、查询条件多样等特性，对数据访问的延迟性能和数据查询的多样性（如多属性区间查询、Skyline查询等）提出迫切需求。本项目将针对OLDI应用的存储、网络和查询等几方面开展研究。1．面向数据访问的低延迟需求，研究基于大规模网络内存的键值（KV）存储技术，通过全局化存储/备份以及本地化失效检测/恢复，实现高效的KV数据访问。2．针对数据传输的延迟受限特性，研究最后期限感知的应用层网络虚拟化技术，实现多个OLDI应用之间的网络隔离和共享，以及同一OLDI应用内部基于最后期限的数据流调度和数据传输。3．面向复杂查询需求，设计负载均衡的多维数据到存储资源的维序映射机制，研究基于KV的分布式索引结构，进而实现延迟受限的复杂查询。
认知无线传感器网络中的信道感知与分配算法	认知无线传感器网络;信道分配;能量有效;信道感知	认知无线传感器网络借助认知无线电技术有效地减少信道干扰，提高了数据传输速率与可靠性，但由此引入的信道感知与分配功能带来严重的资源开销，这于资源受限的无线传感器网络是一巨大挑战。因此项目研究适合认知无线传感器网络的信道感知与分配算法。具体研究内容和目标是：研究认知无线传感器网络的信道感知技术，拟提出一种基于审查机制与频谱检测时间的单信道感知算法；考虑无线传感器网络数据突发特性，拟提出一种结合信道特性与数据传输的多信道感知算法；为降低合作信道感知能耗，拟提出一种合作信道感知策略中的最优感知节点选择算法。研究认知无线传感器网络的信道分配技术，建立数据传输的能量模型及信道状态模型，拟提出一种基于节点剩余能量的信道分配算法；以上述算法为基础，考虑信道切换及公平性，拟提出一种结合信道切换与信道公平性的信道分配算法；最后考虑数据的突发特性，拟提出一种基于传输数据与信道质量的信道分配算法。
针对故障注入攻击下密码芯片的故障产生机理及安全验证方法研究	设计阶段安全性验证;密码安全芯片;硬件安全;故障注入攻击;单粒子翻转	密码安全芯片大量地存在于电子产品中，尽管安全芯片中有复杂的加解密算法和密钥保护机制，然而安全芯片易受到故障注入攻击，尤其是当半导体制程工艺从深亚微米进展到纳米级别，高精度的故障注入攻击已对安全芯片产生严重威胁。深入理解芯片在高精度故障注入攻击产生和传输有效故障的机理对于安全芯片的设计与防御至关重要。本课题旨在研究深亚微米集成电路工艺条件下有效故障产生与传输的机理，建立故障成功率模型。并针对器件级单粒子翻转模型无法快速在超大规模集成电路中仿真的问题，建立模块级、门级、晶体管级相结合的设计阶段的仿真系统，运用通用的IC 设计EDA 工具在安全芯片设计阶段的后期，对算法弱点进行系统全面的评估。本课题需要整合半导体物理离子化机理、密码算法与IC设计验证流程。在设计阶段对故障攻击进行安全评估，并对防御性设计进行验证，可以避免对实际芯片测试才发现设计缺陷情况下所导致的费时费力、价格昂贵的再次流片。
互连网络构造与算法及其在覆盖网络中的应用研究	代数图论;互连网络;覆盖网络;并行分布式算法;复杂网络	互连网络是当代计算机科学技术的主要研究领域之一，网络设计者和图论学者利用各种技巧提出并研究了一系列互连网络模型，但是研究者们一般侧重于针对某种具体的网络结构进行研究，并且大多数是采用直观的方法。由于互连网络表示符号的不同，经常会出现相同的网络结构被重复地提出的问题，因此就有必要采用一种研究方法来统一处理互连网络拓扑结构问题。本项目的研究重点在于首先利用代数图论的方法分析一些现行网络拓扑结构的构造共性及本质，总结出代数图论方法对于互连网络模型研究的优势；然后使用该方法中的Cayley图和群半直积构造方法，提出了两类互连网络模型，并进一步研究新型网络拓扑性质、通信算法以及一些典型的并行算法等；最后把这种研究方法应用于复杂网络和P2P（Peer to Peer）网络，构出了一种具有小世界特性的P2P覆盖网络模型，并进一步研究新型互连拓扑在网络虚拟化中应用问题。
面向共享Cache多核处理器的低功耗关键技术研究	多核处理器;共享高速缓存;路预测;低功耗;可重构	低功耗是多核/众核处理器发展中所追求的重要目标之一。随着处理器核心数量的增多，会带来线延迟增加和功耗增大的问题。共享Cache的结构对多核体系结构的处理器功耗有着重要的影响。面向共享Cache的多核处理器低功耗技术的研究对未来不断发展的众核处理器、艾级超级计算机等的低功耗设计有重要意义。.本项目将在已有相关研究的基础上，着重对多核处理器中共享二级Cache的划分方法、路预测算法和Cache可重构方法进行研究来降低功耗。利用程序运行的局部性原理，通过私有和共享两种资源分配方式相结合来实施共享Cache的混合划分，通过关闭Cache列来降低功耗；通过综合考虑Cache的访问频率和LRU替换算法，预测同一个数据组中下一个预先访问的数据路，避免对不命中的路的访问；根据应用需求动态改变Cache的相联度及相应参数，使每一路或多路能在不同的情况下处于工作或休眠状态，从而降低功耗。
移动云计算中数据流应用的动态计算切分技术研究	计算切分;任务调度;移动云计算	移动云计算切分作为一种能有效提高移动应用程序运行性能的技术，近年来被研究者们广泛关注。计算切分技术合理选择将移动应用程序中部分计算任务迁移到云上，达到计算性能最优化。已有计算切分研究工作仅适合于简单工作流应用，并在动态环境下的性能有待提高。针对以上问题，本项目探索并研究复杂数据流应用模型下动态计算切分问题。首先，研究数据流应用建模，解决输入数据时序和复杂性能指标约束下，数据流应用计算切分决策的最优化问题。其次，基于构建的数据流应用模型，进一步研究计算环境动态变化时，计算切分决策自适应性调整问题。最后，将针对实际大规模移动云应用，研究在多用户共享和竞争云资源的情况下，对各用户计算切分进行最优化综合决策问题。本项目将为构建未来新型移动云计算平台提供关键技术，并为云计算服务提供商进入移动市场提供理论基础和应用模型。
异构并行系统全程序算法级低功耗优化方法研究	算法级;优化技术;低功耗;全程序;异构并行系统	降低功耗是异构并行系统设计与实现的首要问题。硬件层功耗优化已经很难满足不断增长的功耗优化需求，软件层功耗优化得到广泛关注。为融合多种处理器计算资源的效能优势，软件层功耗优化面临功耗建模对象更复杂，节能效果与功耗优化算法选择相关联两个新的科学问题。.本课题开展异构并行系统全程序算法级低功耗优化研究，拟重点研究多处理器多计算段划分的程序执行时间与动态功耗关系；研究数据传输与多任务动态分配的通信功耗形式化描述；研究热分析模型下实时芯片温度管理与静态功耗相互影响机理；研究算法级低功耗优化方法；设计并实现异构并行系统功耗优化验证原型系统。.本课题将在全程序功耗模型，处理器最优频率选择与最优下降多计算段时间分配、基于Profiling静态分析的整数线性规划、以及电压预测与温度感知的并行任务调度算法方面形成创新性成果，为功耗优化技术由硬件级、编译级走向算法级，高效应用异构并行系统提供理论依据与技术支撑。
高效能异构处理器的存储层次设计和管理	高效能计算;存储层次;异构处理器;Cache管理	异构处理器根据不同类型的应用定制处理引擎，能够有效提升系统效能，近年来成为了工业界的主流。针对异构处理引擎的研究普遍开展，但是对异构处理器的存储层次的研究还存在很大的空间。首先，不同类型处理引擎的访存模式存在差异，对存储子系统提出了不同的需求。其次，异构系统的任务调度对存储子系统的效率影响很大。再次，异构处理引擎共享存储子系统资源时，由于各自的访存模式存在明显差异，很可能出现相互干扰的情况。本研究针对异构并行处理器的存储层次设计和管理问题，选取移动、桌面和服务器三个主流应用领域的体系结构，根据各自应用的特点和业界的发展趋势从中提取最为关键的具体研究点，从存储层次的角度（以cache管理策略为研究的首要重点）来缓解异构处理器中的存储墙、功耗墙和编程墙。
云存储系统中节能关键技术研究	重复数据删除;节能;分级存储;数据迁移;云存储系统	随着数字信息的爆炸性增长，用户对海量数据的存储提出了新的需求，日益增大的数据存储规模所带来的能耗问题也越来越严重，基于性能保证的节能技术因此成为研究热点。本项目在云存储系统的基础上，对节能过程中的关键技术进行研究。具体包括：为满足云存储系统的实时性在线服务，研究基于多指标的数据分级存储策略；为满足云存储系统的可靠性及性能，研究一种可保证可靠性及性能的基于数据迁移的节能技术；为满足云存储系统的海量数据管理、减少海量数据的存储容量需求，研究一种基于重复数据删除的节能技术。最后，基于Hadoop搭建一个低能耗的云存储平台，对所提出的节能方案进行测试。项目的主要目标是从数据存储与管理等方面着手，为满足云存储系统的新特性，对云存储系统中的节能关键技术进行研究，使得云存储系统在实现节能的同时也能满足用户的需求。
基于GPU的并行排序算法设计与优化	排序算法;性能优化;GPU;加速器增强型体系结构	利用GPU来加速科学问题的求解已成为高性能计算的一个重要研究方向，而排序算法是一个非常基础的算法，设计基于GPU的并行排序算法可以直接支持一大类科学计算应用。基于对GPU内部层次化内存模型和流处理单元的抽象与分析，设计实现层次化确定性采样排序算法，该算法与同类型的并行排序算法相比，能够更加有效的利用GPU的高并行性和层次化内存模型。同时，本研究深入对比GPU和CPU特点，设计并实现基于CPU-GPU协同工作的快速排序算法。不仅对这些算法进行理论分析，本研究还设计开发原型系统进行实验验证，将理论和实验结果进行对比分析，进一步对本研究提出的优化方法、算法以及相应的程序实现进行改进和提高，并结合MapReduce并行计算框架对本研究成果进行实际应用。本研究提供的基础性理论与方法一方面可充分挖掘GPU计算潜力，另一方面对广泛的应用问题和领域在新型加速器增强型体构上提供高效支持。
无线片上网络及其设计方法研究	多核处理器;无线网络;片上网络（NoC）;分布式系统;系统芯片（SoC）	多核处理器与SoC的快速发展，对系统体系结构提出了新的挑战，片上网络（NoC）应运而生。然而传统NoC仍基于金属互连线实现片上路由器之间的连接，终究还是要面对互连线材料的物理极限问题，其通信性能仍然受到限制。为解决这一问题，研究者提出了无线片上网络（WNoC），采用新一代RF（射频）互连技术连接片上处理核心，形成无线数据交换网络，既利用了RF互连的超高速又具有NoC架构的优点。本课题针对WNoC的网络理论和设计方法开展研究工作，主要研究内容包括WNoC媒体接入控制算法、路由算法、任务分配、拓扑结构生成、缓存空间分配及WNoC和传统NoC的混合架构等问题。通过对这些问题的深入研究，解决实现WNoC的关键基础技术，提出一套完整的WNoC架构设计方法。本项目作为国内外NoC设计研究领域的前沿课题，将推动对新一代体系结构的研究，在设计方法学方面的探索，将有力推进NoC的实际应用。
基于GPU的提高三维集成电路良率的测试数据优化方法研究	GPU通用计算;测试数据优化;三维集成电路良率	工艺尺寸细化和片内晶体管数量速增在不断增强芯片并行性能同时，也造成集成电路（IC）缺陷越发复杂，给IC测试带来巨大挑战。现有测试程序在运行工业电路时因耗时过多而被裁剪，导致海量测试数据（测试向量集）和非最优故障覆盖率。3D IC缺陷比传统IC更复杂繁多，测试程序时间过长及测试向量集过大的问题更为严峻。本课题围绕3D IC良率和测试成本，利用GPU并行性能，高效获得给定测试成本下的高质量的测试数据，分为三部分：1）基于GPU面向多种故障类型的故障模拟，其产生的故障检测表可评估测试质量，是测试优化基础；2）面向不含无关位的测试向量集，利用GPU从中选择容量小且较高测试质量的测试向量集，调节不同层晶片的良率，最终优化3D IC整体良率；3）基于GPU的提高3D IC良率的测试集优化，在选择高质量的测试向量同时，结合无关位填充优化该测试向量检测故障能力，最终优化测试成本和良率。
非主从式混合云存储系统伸缩性管理研究	极限伸缩性;伸缩性;云存储;资源配置;伸缩性管理优化	按需服务是云存储系统有别于传统系统的最显著特征之一。它改变现有产业布局和应用模式，对信息技术发展具有深远的影响。云存储系统的特点是高性价比、高伸缩性和高可用性。其中，高伸缩性是实现云存储系统按需服务的最基本保证。随着云存储系统用户数量增加，规模更大，结构更复杂的发展，系统可伸缩性和可用性面临巨大的挑战。.    本项目针对目前云存储系统伸缩性研究中资源配置策略粒度过大，对存储资源重视不足、评估验证困难、对系统性能影响较大等问题，研究基于非主从式异构的云存储服务平台兼顾性能的伸缩性分析和评估模型以及实现和优化等问题。内容包括:1)文件级的伸缩性管理优化方法。2)测试与计算相结合的系统极限伸缩性的评估与验证方法。3)面向多用户基于预测的预反应式存储资源配置策略。4)基于多维目标的动态伸缩性模型。.    研究成果将为提高云存储系统利用率，高效实现、评估和动态管理系统伸缩性提供科学的理论指导和技术支持。
性能驱动可编程自重构图形处理器体系结构研究	性能评价模型;自重构;阵列计算;体系结构;图形处理器	当前面向不同应用的图形处理器正朝着高性能、多种API兼容的方向迅速发展，众多效率不一的图形渲染算法使图形处理具有高度灵活性，如何根据实际需求动态获取最优性能是图形处理器设计面临的瓶颈问题之一。本项目旨在建立一种通过自主快速切换图形渲染算法获取最优性能的可编程自重构体系结构，具体包括：1）建立综合权衡图形渲染要素与渲染效率之间制约关系的性能评价模型，为图形硬件的实时自主重构提供依据；2）建立基于H-Tree的自主重构机制，通过层次化配置网络以调用指令的形式发送重构信息，实现重构过程的高效完成；3）将性能评价模型与重构过程相结合，建立基于同构轻核处理元邻接互连的图形阵列处理器，采用分布式指令存储与指令邻接寻址方式实现计算资源的可编程与自重构，支持数据粗粒度与操作粗粒度的高效阵列计算。本课题研究的性能驱动自重构阵列处理器架构不仅可以高效完成图形渲染，还将为突破传统计算架构的发展瓶颈提供一些思路。
生物医学文本大数据中的疾病关系并行挖掘模型研究	MapReduce模型;大数据处理;生物医学文本挖掘;本体标注;并行处理技术	本项目旨在通过深入分析生物医学文本中非结构化数据的特点，研究面向疾病关系并行数据挖掘中模型训练、模型推断、实体标注以及语义挖掘等关键理论问题，设计并实现基于MapReduce的文本分类、分析和处理模型。首先将研究生物医学文献基于MapReduce的文本分类方法，提出基于MapReduce的并行化生物医学命名实体识别的模型训练及模型推断算法，并在此基础上对生物医学文本中的疾病和相关实体进行规范化标注。其次将提出生物医学文本中与疾病相关的语义关系挖掘和假设生成并行算法，构建基于文本大数据的疾病关系网络，为实现对疾病之间、疾病和基因、药物和基因、疾病和药物之间的假设生成进行预测提供理论基础。最后将实现生物医学文本大数据并行分析与处理原型系统，并基于混合语料测试集对本项目的理论和原型进行全面的性能评估与测试。
四维变分同化CPU/GPU协同并行计算研究	混合异构体系结构;GPU;CPU;并行计算;四维变分同化	四维变分同化是当前数值天气预报领域极有价值的资料同化方法，对于提高中期数值天气预报效果起到了关键作用，但是其巨大的计算量是制约技术发展和业务化的重要因素，开展四维变分同化并行算法研究具有重大学术价值和实际应用需求。当前基于CPU和GPU相结合的新型混合异构体系结构是构建千万亿次高性能计算机的有效途径，同时也对四维变分同化并行算法研究提出了新的挑战。本项目在已有工作的基础上，面向千万亿次异构混合体系结构计算平台研究四维变分同化并行计算关键技术。在四维变分同化核心算法的细粒度并行、CPU与GPU协同并行计算、海量观测资料GPU加速处理算法、GPGPU程序优化和面向千万亿次系统的大规模无约束最优化算法等方面开展研究，实现四维变分同化CPU/GPU异构协同高效并行计算，从而满足业务预报时效性要求，推动数值天气预报水平不断提高。
面向数据密集应用的功耗感知调度研究	功耗感知;调度;能耗管理;数据密集应用	IT行业作为新兴高科技支柱行业，面临着巨大的节能降耗压力，IT能耗问题已经成为影响全球能源和环境的重要因素之一。本课题针对目前异构计算环境下功耗感知调度存在的主要问题，对面向数据密集应用的任务功耗感知调度展开研究。针对异构环境的数据密集应用，建立相应的系统模型、应用模型和功耗模型；以功耗模型为基础提出数据密集应用分别在DVFS-Enabled和DVFS-Unable系统中兼顾静态能耗的功耗感知调度框架；以框架为基准，进一步平衡应用的优先约束性、系统的异构性和不同性能指标的冲突性关系，提出数据密集应用在兼顾计算资源和通信资源异构的环境中不同目标的功耗感知调度算法，并提出相应的评测方法。本课题的研究既可有效降低数据密集应用中的能量消耗，还能满足用户的应用需求，为数据和计算中心的能耗管理提供技术支撑。
预算功率指导的高能效GPU集群任务调度模型与算法	GPU集群;任务调度;能量效率;绿色计算	高能耗已经成为超级计算机研制与应用中必须解决的挑战性问题。本研究针对GPU集群这种典型的超级计算机体系结构，旨在解决在GPU集群上实现高能效任务调度所面临的基础性核心问题，设计可以长期、大幅度降低超级计算机能耗的调度模型与算法。本研究分析并抽象典型GPU集群的系统模型、任务模型、能耗模型以及调度模型；提出一种可以度量与比较超级计算机能量效率水平的指标；设计出瀑布模型用于指导多层次、多粒度的节能策略开发，在充分考虑全局节能效果的基础上给出预算功率的设置原则与方法，据此提出了基于预算功率指导的层次化、高能效任务调度算法的设计方法；基于仿真环境、原型系统以及真实系统，分别设计了对本研究提出的调度算法与相关策略的有效性进行全面验证与进一步优化提高的方法。这项基础性的研究成果，一方面可以用于指导未来节能型超级计算机的研制，另一方面可以应用到已经存在的超级计算系统中，大幅度降低其能耗开销。
云数据管理相关理论与技术研究	虚拟视图;云数据管理;云计算;服务质量感知;冗余可用性	由于云数据具有海量性、异构性、外包性和混杂性等特点，使得传统数据管理技术不再适用。随着云应用的普及，目前围绕云数据的管理在可用性、服务质量保障、隐私保护和访问控制等一系列问题上面临新的挑战。本项目针对云数据管理难题开展相关理论与技术研究，内容包括：研究故障常态化下的云数据冗余可用性理论；研究云数据服务QoS表示与获取方法研究，突破基于QoS感知的副本放置技术；研究基于知识证明的冗余数据存在性与完整性证明方法；研究基于动态信任的虚拟视图访问控制机制。项目力图在理论上有创新，在关键技术上有突破，建立一套全面增强云数据可用性、安全性与服务质量的数据管理技术框架，促进云计算技术的发展和应用的推广。
多接口车联网可变带宽信道分配算法研究	路由;可变带宽;信道分配;车联网;链路调度	满足交通环境应用要求的可变带宽信道分配技术是车联网(VANET: Vehicular ad hoc networks)成功部署的关键一环。本研究针对车辆运动速度快、拓扑随路网分布的实际情况，提出满足车联网动态拓扑特性的可变带宽信道分配方案。首先，研究多接口车联网逻辑拓扑描述方法和冲突图构建策略。根据动态拓扑和连通性等要素间的制约关系，研究链路无冲突并行传输问题及半定规划链路调度方法。通过挖掘城市交通车辆运动特点，提出车联网地理成簇策略，并在此基础上构建基于车辆位置装箱的离线信道分配机制，以降低拓扑动态特性对信道分配的影响。在确定带宽分配优化目标的基础上研究网络初始数据流分配方法，并针对路由与网络负载之间的耦合关系研究链路负载感知的地理路由策略，尽量维持优化的网络负载分布。最后，构建车联网可变带宽信道分配技术的评价策略和仿真验证平台，对提出的方案进行评估，促进在车联网中的可靠部署。
基于线程级推测的非规则算法并行化研究	性能评估;线程级推测;编程模型;调度方法;并行非规则算法	非规则算法采用基于指针的数据结构解决问题，导致具有模糊关系的数据和控制依赖在运行时才能确定，现有的并行策略通过静态分析技术解决依赖问题，使得并行化效果不佳，非规则算法在多核平台上面临难以并行以及并行度不高这一科学问题。本课题拟借助线程级推测技术消解模糊依赖，增加算法运行时的并行度，解决上述问题。研究非规则算法并行规律，建立线程级推测并行编程模型，用于显式构造并行非规则算法；运用理论分析和实验验证方法，构建基于概率图的性能评估模型，实现对执行模型的性能评估及动态选择；研究非规则算法特征、调度方法和加速比之间的关系，提出基于机器学习的线程级推测调度方法，为算法预测出最优调度方法。研究目标是提出基于线程级推测的非规则算法并行化理论和方法，缩短非规则算法所能达到的性能和多核平台具有的潜在性能之间的"软差距"。预期成果可应用于多核平台加速非规则应用，为实现高并行度的非规则算法探索新途径，提供新思路。
多人共享云计算服务环境中的安全问题研究	加密;云存储;云计算;访问控制;网络安全	本项目将云计算服务提供商（CSP）视为潜在攻击者，首次考虑了企业用户租赁CSP的存储空间之后所带来的多人共享云计算服务环境中的安全问题。首先提出了混合式身份与属性的层次加密模型。该模型采用层次结构的密钥生成过程，使用精确和模糊两重身份标识用户。基于该模型提出了高效、可扩展、灵活的安全云计算方案。该方案中，用户密钥环与用户精确身份相关，体现了用户的唯一特征，以抵抗合谋攻击；在不泄露关键词和消息内容的前提下，CSP参与到解密过程中，为用户分担部分解密开销，从而更好地满足随时随地享受服务的需求；用户独立完成由用户新增/撤销引起的用户密钥环生成/更新操作，因此可扩展性很好；加密者能够为密文指定三种形式的访问控制策略，以灵活地实现细粒度的访问控制。本项目还将设计与开发安全云计算原型软件系统，拟在某典型云计算环境中进行示范应用。本项目的研究符合国家重大需求，具有重要的研究价值和应用前景。
多虚拟机系统HDD-SSD混合磁盘I/O调度及其性能优化研究	HDD-SSD;虚拟化;磁盘I/O;性能优化;资源调度	由于资源复用与服务聚合，磁盘I/O系统已成为多虚拟机系统的性能瓶颈。为提高虚拟化系统的磁盘I/O性能，同时兼顾系统成本，目前采用传统机械式硬盘(HDD)和固态硬盘(SSD)混合系统，其中SSD充当高速缓存或作为部分应用的专属存储，大容量HDD作为主要存储设备。由于HDD和SSD在性能、操作方式上的异构性，传统的面向单一的非虚拟化环境下的SSD或HDD的磁盘调度策略，无法适用于多虚拟机间磁盘I/O高度竞争的HDD-SSD混合磁盘。本课题根据虚拟化环境下HDD-SSD混合磁盘系统调度机理、负载模式及性能约束上的重大变化，研究虚拟机环境下HDD-SSD混合磁盘系统性能模型、性能、服务质量、公平性等指标联合约束的磁盘I/O调度策略、I/O性能优化与能耗优化方法。通过进行细粒度的HDD-SSD联合磁盘I/O调度和性能优化，提高多虚拟机系统的磁盘I/O性能和服务质量，同时进一步降低系统能耗。
基于排队论的三值光学计算机性能分析与评价	排队论;三值光学计算机;任务调度;处理器分配;性能分析与评价	如何提高计算速度是世界各国面临的重大难题，光计算是解决该问题的有效途径。三值光学计算机作为一种新型计算资源，可为用户提供具有高性能和安全性的服务。然而，本课题组前期研究发现其任务调度策略和光学处理器分配策略都没有从系统性能角度开展深入研究，亟需新的方法以分析与评价三值光学计算机系统性能问题。本项目以三值光学计算机性能为研究对象，采用排队论技术、数值仿真技术和实验室性能测试试验技术相结合，对策略等因素影响系统性能的规律进行基础科学研究。具体内容包括：理论分析任务调度策略和光学处理器分配策略，并阐明其形式化定义；研究三值光学计算机服务性能分析与评价的排队论建模与数值求解；通过实验室性能测试试验，建立三值光学计算机服务性能指标的定量表达与预测模型。旨在揭示诸因素对系统性能的影响规律，为三值光学计算机系统性能分析与评价、有效提高系统性能以及其他并行计算系统的性能评价奠定理论和技术基础。
基于关联性的分布式元数据存取优化研究	存储;元数据;文件系统;关联性	在文件系统中，对元数据的I/O操作占全部I/O操作的一半以上。元数据存取是大型分布式小文件系统性能的关键因素之一。本项目研究大型分布式文件系统中元数据的布局、负载管理及I/O优化，实现大规模元数据的高效存取，提高系统访问性能。主要研究内容与目标有：（1）研究元数据几类关联性的数学模型及轻量级高效关联性识别算法，并利用元数据之间的关联性，在元数据服务器集群中合理布局元数据，结合预取技术提高元数据的存取效率；（2）针对元数据的特点，结合元数据的关联性，设计轻量级高效内存重删与压缩算法，提高内存复用率，减少元数据存取的内外存交换；（3）针对海量元数据粒度小、数据量大的特点，结合元数据的关联性，设计高效的纠删码并加以优化实现，降低元数据容错带来的内存开销，加速元数据的存取；（4）实现一个分布式文件原型系统，采用元数据优化技术，提高文件的读写性能。特别对内存受限的分布式系统，读写性能将有很大提升。
动态时变约束下的赛百平台资源优化理论与算法研究	受限序优化;赛百平台;性能评价;动态时变约束;资源优化	随着先进计算和信息技术的发展，回答以前人类无法认识和解决的问题，如黑洞合并、暗物质结构、气候变化因素、蛋白质折叠以及基因测序等，开始逐渐成为可能。为了迎接大规模、分布式、跨组织的资源管理、共享与优化的挑战，美国NSF提出打造国家统一的赛百平台（Cyberinfrastructure，以下简称CI）战略。CI资源优化的挑战一方面源于资源的多元相关性；另一方面源于优化约束条件的动态时变性。本研究课题以赛百平台资源共享为应用背景，以序优化方法作为理论基础，在动态时变约束条件下对资源优化理论与算法做出前瞻性、基础性和创新性研究。本课题的理论上的创新主要在于采用传统受限序优化方法排除的不可行约束很可能具有优良的特性，于是当我们能够分析得到这一些较为优良的不可行约束变为可行约束的概率分布规律，便能够更好的为我们的资源选择、决策与优化作出贡献，使得更满意的解能够获得，相应的优化方法更具有实际应用价值。
集成在处理器中的计算型可重构逻辑体系结构探索研究	高性能;低功耗;可重构逻辑;算法计算;处理器	将可重构逻辑集成到处理器中组成Core-RL系统，既可以提高系统整体性能/功耗指标，又可以对创新应用提供更好的特性/市场响应时间，处于当前的研究前沿。然而现有Core-RL系统中的可重构逻辑体系结构（包括FPGA）还存在各种不足。本项目将针对集成在处理器中的计算型可重构逻辑体系结构进行探索研究，充分考虑其应用领域及特有性质，全面考察设计参数，得到优化的体系结构设计空间，能够：1）使性能/功耗指标达到或超过传统FPGA系统的3倍；2）使Core-RL系统中的可重构逻辑可以与处理器高效通信、有效访问数据，并易于被使用；3）提供快速的重构功能及高效的部分/动态可重构和虚拟化特性，以支持大型计算；4）支持从通用算法设计语言（C语言）程序生成到可重构逻辑配置信息的整个工具链，使人们易于在可重构逻辑上进行设计。本研究将对设计高效的、易于使用的Core-RL系统提供依据，推动计算系统整体能力的进一步发展
面向普适应用的上下文质量管理技术研究	上下文质量;中间件;上下文感知;普适计算	上下文感知是系统自动感知上下文信息变化并调整自身的行为，是提高计算智能性的重要途径，上下文感知中间件是普适计算中间件体系结构的重要组成部分，对实现普适计算理想有重要意义。但现有的上下文感知中间件往往更多的基于理想条件下的上下文信息，没有考虑上下文来源的多样性、异构性、时效性、准确性、确定性、位置相关性等多种质量因素的影响,从而会影响上下文感知应用的确定性、自适应性和效率等多个方面。因此，本课题旨在针对现有的普适应用在上下文质量管理方面所面对的新的挑战，通过研究关注服务质量的上下文感知模型、上下文度量因子评估、基于质量的上下文管理与适配、上下文质量管理框架等问题，为解决分布、异构、智能的普适环境中上下文质量的管理问题提供理论基础和有效的技术手段。
针对FPGA协处理器的高速布局布线算法研究	FPGA协处理器;布线;并行化;布局	FPGA协处理器能支持可重构计算，在特定应用中实现高性能低能耗的计算。然而，FPGA程序的编译时间（高层次综合、逻辑综合、布局布线）远远大于同等功能的CPU程序的编译时间。漫长的编译时间降低开发效率，阻碍了软件工程师应用FPGA协处理器与可重构计算。在FPGA编译过程里，布局布线占了大概四分之三的时间；为了缩短编译时间，我们打算开发高速的布局布线器，实现比现有布局布线器快10倍至100倍的效果。首先，我们会对现有布局器做最优化研究，并开发一个高质量的支持现代异构FPGA体系结构的布局器。之后，我们将采用算法加速（采用解析式算法和高层次方法）和并行加速的手段，来使我们的布局器以及一个基于协商的布线器达到最大的加速效果。另外，我们将开发一套开放源代码的FPGA物理综合流程，以推动可重构计算的研究。
多时空视频数据解析的动态任务引导机制研究	任务并发;前后端交互;视频解析;需求预测	视频图像数据的使用是目前公安机关侦查办案、社会安全防控等社会安全领域工作中最为重要的一个手段。同时，视频数据的智能化解析也已经成为视频系统发展的大势所趋。利用各类视频图像分析处理算法，视频数据解析可以自动高效地检测识别视频里的各种关注情况并按照预定规则进行后续处理。然而，视频资源的迅猛增长也给海量视频数据的内容分析、数据管理和数据挖掘等视频解析任务带来了新的挑战。其系统规模大、工作负载高、时空动态性强，因此任务的生成、协调、并发等都具有很大的难度。为此，本课题拟研究可有效管理视频解析系统全局能力的任务引导机制，探索基于时空特性的任务管理和调度策略，构建流畅高效运行的大规模视频解析系统。任务引导机制将以自演化视频任务的动态资源需求出发，构建系统全局资源拓扑结构图，并在此基础上完成任务的引导以及在视频解析系统中的高效并发。
分数阶微分方程并行算法研究	分数阶微分;并行算法;数值方法;并行计算	由于分数阶微分算子空间上的非局部性和时间上的长尾效应，与传统整数阶微分算子相比，其数值近似的时间复杂度有阶级的提高。这对分数阶微分方程的数值近似计算提出了新的挑战，需要研究针对分数阶算子的并行算法。本项目结合并行计算机体系结构特征，展开分数阶微分方程粗细粒度并行算法研究。具体研究内容包括以下三个方面：分数阶常微分方程并行算法及优化、非均衡空间分数阶并行算法和分数阶微分方程高效隐式并行迭代算法。从算法层和实现层对相应并行算法的任务分配、负载平衡、通信和访存方法进行设计和优化，实现分数阶微分方程数值近似的可扩展粗粒度任务级和细粒度数据级并行计算。本项目的目的是通过对上述并行算法的研究，能有效提高分数阶微分问题的数值模拟速度和求解规模，推动分数阶微分算子在各相关领域的理论和应用研究的发展。
基于多维矩阵的智能计算相关研究	多维矩阵;随机数据;演化计算;演化硬件;智能计算	本项目研究基于多维矩阵的智能计算相关问题。首先适应数据仓库多维数据技术的广泛应用，将多维矩阵理论引入智能计算，对一些经典模型以及我们自己提出或改进的一些模型，实现数学模型的多维矩阵表达，并进一步实现程序表达。这也是一次数学理论与计算机科学的对接，具有重要的科学意义。同时将我们已经取得的一些多维随机研究成果引入到神经网络和演化计算，包括利用我们提出的一种全空间连续且可微的多维正交多项式来改进贝叶斯神经网络、利用我们提出的多层结构方程模型及其新算法来建立自组织路径约束的神经网络、利用我们提出的因变量也未知的凸约束评估模型建立无监督学习的交互投影神经网络等。在演化计算方面，我们将利用多维矩阵理论探讨演化计算及其收敛性证明过程，探讨基于泛函空间的数学函数表达式的随机优化，顺势探讨相关的演化硬件实现。这些研究工作将促进智能计算与相关领域的协调发展，具有重要的科学意义，也具有广阔的应用前景。
基于机器学习的线程级推测模型和编译优化方法研究	多核处理器;线程划分;多核编译器;推测多线程;机器学习	线程级推测（TLS）是多核体系结构上加速串行程序的一种线程级自动并行化技术，TLS加速机制的性能受到软硬件多种复杂因素制约，已有的基于软硬件协同设计和启发式规则的编译优化方法带有经验性弱点。本项目首次提出TLS样本集以及从样本集中学习线程划分知识的思想，克服了经验性方法的弱点。研究基于机器学习的TLS模型，包括：TLS特征设计、特征值获取及表示，构建TLS样本集的方法和关键技术，学习算法，以及线程划分知识的表示及评价。提出基于机器学习的TLS线程划分方法，并基于课题组研制的Prophet编译系统实现新方法，预期经Olden基准程序测试比现有加速机制提高性能20%以上。研究目标是揭示TLS线程划分影响程序加速比的内在规律，证实机器学习能够全面提高TLS性能，为支持TLS的软硬件协同设计提供新途径。预期成果可应用于多核处理器体系结构设计、多核编译器、多核并行计算等领域以及加速现有串行应用程序。
移动环境下应用层组播网络模型构建及稳定性问题的研究	稳定性;应用层组播;移动环境;模型构建;负载均衡	近年来，随着智能终端的迅速发展，用户对流媒体在移动环境下的需求已不断增长。同时，应用层组播技术的兴起，既给流媒体在智能终端上的应用带来了新的发展方向，也使得在移动环境下构建应用层组播的研究成为一个热门话题。但是，由于应用层组播相对于IP组播在稳定性方面有着先天的不足，而移动环境更加大了组播系统中组播节点的高度动态性，所以研究移动环境下的组播系统的稳定性问题既是一个关键性问题也是一个难题。 本项目旨在通过构建和优化移动环境下基于地理位置聚类分簇的节点分类机制，建立移动环境下基于实时覆盖网络结构检测的自适应组播覆盖网络构造模型，研究移动环境下反向抑制主动告警故障检测与恢复机制，构建合理的主动反馈负载均衡机制，为移动环境下的应用层组播提供良好的稳定性，达到为移动用户提供流畅快速应用层组播服务的目的。
基于固态盘阵列的数据布局和缓存管理策略研究	缓存管理;校验更新;垃圾回收;固态盘阵列;数据布局	随着固态盘技术的快速发展和广泛应用，基于固态盘阵列的存储系统成为解决磁盘存储系统性能瓶颈和能耗问题的主要途径之一。不同于传统的磁盘阵列技术，固态盘固有的垃圾回收机制和介质损耗问题会影响固态盘阵列的性能和可靠性，因此不能简单地将磁盘阵列技术应用于固态盘阵列。本项目基于固态盘阵列存储系统，主要研究内容包括：（1）研究和分析固态盘阵列的分块大小和垃圾回收操作对固态盘阵列性能和可靠性的影响；（2）研究基于多分块大小的固态盘阵列数据布局策略，减少固态盘阵列中校验更新对固态盘阵列性能和可靠性的影响；（3）研究垃圾回收感知的固态盘阵列缓存管理策略，减轻固态盘阵列的性能波动性；（4）研究并实现一种集成式固态盘阵列存储系统。通过上述数据布局方法和缓存管理等技术的研究和原型系统实现，提高固态盘阵列存储系统的性能和可靠性，进一步推动固态盘阵列在企业级存储系统领域的广泛应用。
面向众核负载聚集模式的动态区域化层次一致性协议研究	Cache一致性协议;片上网络;众核处理器;负载聚集	负载聚集模式给众核处理器结构，尤其是cache一致性协议的设计，带来了诸多挑战。高效一致性协议必须充分利用负载聚集模式的数据共享特性，在虚拟机内部提供低延迟的数据共享，在虚拟机之间提供低开销的数据共享，同时还应支持虚拟机动态迁移和区域隔离。区域化层次一致性协议具备满足这些需求的潜力，但现有设计只关注上层协议层，忽略了底层片上网络，导致它们在延迟、开销和支持虚拟机动态迁移及区域隔离等方面存在较大缺陷。本项目从协同设计一致性协议和片上网络的角度出发，充分利用片上网络的特性设计动态区域化层次一致性协议，在区域内研究监听协议以降低事务延迟，在区域间研究目录协议以优化带宽开销，将在面向负载聚集模式的一致性协议框架、基于无序网络的带宽优化监听协议、低开销低延迟的目录协议、片上网络对聚合通信和区域隔离的硬件支持等方面取得创新研究成果。这些成果可直接用于众核处理器设计，具有重要的理论意义和实用价值。
超高密度二维磁记录读磁头阵列及其记录系统关键技术研究	磁记录系统;读磁头阵列;二维磁记录;磁盘技术	在大数据时代，数字信息量的爆炸式增长对大容量数据存储设备产生了巨大的市场需求，磁(硬)盘因其具有显著的容量价格比优势而在大规模数据存储系统中占有非常重要的地位。磁记录密度的不断提升导致沿磁道方向的码间干扰和跨磁道方向的道间干扰（即二维干扰），以及磁介质不规则边界形成的噪声问题异常突出，在此条件下从磁盘介质中准确高效地恢复磁记录信息成为巨大的技术挑战。二维磁记录技术通过新的读磁头阵列化设计，同时获取目标磁道及其相邻磁道的信号，采用高效的二维信号处理和差错控制编码技术，消除二维干扰和介质噪声对回读信号的影响，实现高密度环境下记录信号的获取和纠错，达到超高密度磁记录的目标。本项目拟对二维磁记录读写通道进行分析和建模，对读磁头阵列、二维信号处理算法和编解码技术等进行研究，以建立二维磁记录模型和实现方法，为实现10Tb/in2超高磁记录密度提供理论和技术支持。
千核级处理器的高效模拟关键技术研究	动态负载平衡;分布式并行模拟;体系结构模拟器;模拟加速;模拟时间同步	近年来，片上多核及众核体系结构成为国内外的研究热点。而根据摩尔定律，单芯片上集成的处理器核数目将呈指数级增加，千核级处理器已不再遥远。随着处理器核数目的增加，传统串行模拟器在模拟此类结构时的性能会急剧恶化。与此同时，千核级处理器的体系结构设计空间却扩大了数倍。因此，体系结构设计空间探索的效率将会非常低，模拟器这一重要研究手段将面临巨大挑战。.本课题提出一个面向千核级体系结构的分布式并行模拟加速框架，利用 Host 平台的并行计算能力来开发目标机模型中天然存在的粗粒度并行性，以提高模拟器的速度。重点突破目标机模型动态自适应划分技术、Host并行层次与拓扑结构感知的模型映射技术、保守与乐观相结合的高效模拟时间同步技术，以及模拟负载动态平衡技术。研究成果将对国产多核众核微处理器设计起到重要的基础支撑作用。
虚拟肝脏手术中高精度软组织建模与交互式仿真技术研究	有限元分析;水平集;软组织形变模型;虚拟手术;图形处理器	肝癌是我国第二位癌症杀手，严重威胁着我国公民健康，而外科手术治疗目前仍然是治愈中晚期肝癌最有效的方式也是首选方式。目前虚拟肝脏手术仿真系统已开始逐步应用于临床诊断，辅助医生进行手术前期规划和手术结果预测，以减少手术失误及减轻病人痛苦。然而，现有虚拟肝脏手术系统仍存在肝脏软组织几何形态与形变模型精度不高的问题，极大限制了其在临床诊断中发挥应有的作用。为此，本项目拟在已搭建的3D医学影像工作站基础上，研究基于混合多尺度几何分析的图像模糊区域增强算法及基于非线性自适应水平集的分割算法，以获得肝脏软组织的几何形态模型，在此基础上，研究性能优良的有限元物理建模方法，高精度地仿真肝脏软组织的形变；为了进一步提高虚拟手术系统的性能，研究基于GPU的有限元求解算法，实现交互式仿真，以充分显示肝脏器官的几何形态模型及形变过程细节，辅助医生获得更多的信息。该研究成果将为虚拟肝脏手术仿真提供坚实的理论基础和技术
云存储系统中重复数据删除技术研究	重复数据删除;缓存管理;云存储;数据布局	随着移动互联网等信息技术的快速发展，数据呈爆炸式增长。"大数据"的兴起对云存储系统的发展和应用起着积极的推动作用，而云存储系统的性能和可用性是影响云存储服务应用和推广的重要因素。重复数据删除技术可以有效地提高存储效率和网络带宽利用率，并已逐渐被应用于云存储系统中。本项目拟通过研究云存储系统中的重复数据删除技术来提高云存储系统的性能和可用性，主要研究内容包括：（1）测试和分析缓存空间分配对重复数据删除存储系统读写性能的影响；（2）在基于重复数据删除技术的云存储系统节点内部研究基于应用负载变化的动态缓存管理策略，提高缓存利用率和存储系统性能；（3）研究在多个云存储系统之间根据重复数据删除技术中的数据引用特征进行数据的布局，提高云存储系统的可用性。通过本申请项目的研究，可以有效地提高云存储系统的性能和可用性，并推动重复数据删除技术在云存储系统中的更广泛应用。
闪存存储系统关键技术研究	磨损均衡;闪存存储系统;垃圾回收;地址映射	闪存作为一种新型的存储设备，可以很好地解决传统磁盘存在I/O性能低下的问题，已经成为工业界和学术界的研究热点。现有的存储系统是针对磁盘的物理特性进行设计和优化，可以发挥存储的最佳性能，然而闪存与磁盘的物理特性有显著的差异，因此，现有的存储系统无法直接应用于闪存。本项目针对闪存的物理特性，展开了闪存存储系统关键技术的研究。研究内容主要包括：1）基于多级映射表的页级映射方法的研究；2）自适应的垃圾回收策略的研究；3）优化的混合磨损均衡算法的研究；并采用成熟的闪存仿真平台，结合多种真实I/O数据集验证各项研究的实际性能。本研究可促进研究者对闪存存储系统做深入研究，为闪存存储系统中关键技术的解决开拓新颖研究思路，有望获得创新性和实用性并举的成果，将对基于新型存储的计算机体系结构发展起着重要的推动作用。
基于高速缓存动态锁定的多核实时嵌入式系统任务调度算法研究	多核处理器;实时嵌入式系统;任务调度;高速缓存锁定	任务调度是实时嵌入式系统领域一个重要的研究热点。高效的任务调度算法能够确保满足实时嵌入式系统时间约束，并提高其的执行效率。然而高速缓存（cache）的应用使得任务的最差运行时间（WCET）无法精确预估，通常会造成任务WCET的过量估计，降低调度算法的有效性。高速缓存动态锁定（DCL）可以有效的解决这个问题。然而将其整合于任务调度算法之中会带来cache空间分配、被锁定内容的选取、被锁定内容的存储等问题，目前尚未有完善的相关算法。本项目拟研究基于DCL的多核实时嵌入式系统任务调度算法，最大程度提高系统执行效率。将分别针对指令缓存和数据缓存提出高效的锁定算法，减少单个任务的WCET；针对多核处理器，整合DCL，建立最优化的实时任务调度算法，满足任务时间约束并减少整体运行时间；对两级cache分别提出高效的空间分配和地址确定算法，提高cache的利用率；实现算法并用aiT软件进行仿真分析。
空间信息网络安全关键技术研究	空间信息网络;安路由;网络安全;密钥管理	针对空间信息网络呈现的特点，采用协同融合理论，从多层次的全局角度来考虑空间信息网络中安全威胁和安全需求，构建涉及多个层次的高柔性空间信息网络安全体系结构。在保证路由安全性的同时，设计基于目标优化的安全路由协议，并尽可能减少计算开销、存储容量等。针对空间信息网络加密机制存在的难以添加有效密钥管理机制的问题，提出分级多属性的空间信息网络密钥管理机制，将分级和多属性机制相搭配，降低整个网络运行加密与密钥管理机制的负担。同时，提出基于角色的空间信息网络安全访问控制机制，从资源使用层面来研究空间信息网络的访问控制策略，利用角色作为媒介构造访问控制模型及其约束条件、角色、授权等机制，将主体和角色、权限和角色相关联，实现主体和访问权限的逻辑分离，使得可操作性及可管理性更灵活，安全性更强。
基于张量积的向量化代码自动生成和调优技术研究	张量积;自动代码生成;乘加融合;向量处理器;向量化	计算平台的快速发展、日益复杂和变化多端对科学计算提出了一个核心问题：如何用合适的代价实现可移植的最优性能？为了实现乘加融合结构向量处理器的向量化代码自动生成、性能最优化和平台自适应，本项目拟通过张量积理论设计数字信号变换算法的特定数学结构和代数规则，研究FMA变换算法、并行和向量化算法的一般方法，以及向量化代码的自动生成、自动调优和评估算法。设计和综合出一套基于张量积的自适应、自动调优和高效能的向量化代码生成机制和实现算法，并通过实验测试和理论分析的手段评价和比较所设计机制与算法的性能。
访存模式感知的自适应智能存储体系结构及关键技术研究	片上多处理器;存储系统;性能优化;数据布局;访存模式	多核处理器的核数越多，对存储系统的竞争就愈发激烈，存储墙和功耗墙问题就愈发严重。目前存储控制器普遍采用固定逻辑实现，无法根据访存模式和系统目标进行静态和动态调整，实现复杂存储管理优化算法的代价高，阻碍了系统性能和能效的提升。本项目将研究的访存模式感知的自适应智能存储系统是对上述问题的有效解决思路。通过在存储控制器中集成处理器，用软件实现存储管理优化算法，用硬件实现存储器的接口控制，可有效简化存储系统设计。智能存储控制器可根据访存模式和系统目标定制和在线重构核心算法，降低复杂访存优化算法的实现代价。但存储控制器设计本身难度较大，且引入处理器可能会带来额外耗费。为应对挑战，本项目以智能存储系统结构研究为核心，根据对海量访存轨迹的离线分析和在线分析，研究新体系结构下核心算法的定制和动态重构方法以及软硬件协同的存储体系结构感知的数据布局优化技术，充分挖掘智能存储系统灵活性带来的性能与能效优势。
任意网络中的可分数据处理研究	最大完成时间最小化问题;任意网络;可分数据;并行与分布式处理;线性规划	在传统用于分析可分数据处理的等效节点法中，为了建立求解数据分配方式所需的线性方程，网络中的节点最多只能从一个邻居节点接收数据。这极大的降低了网络链路资源的利用率，从而导致了数据传送时间长，数据处理速度的提升受到了限制。为了提高网络链路资源的利用率以进一步提升数据处理速度，我们拟在三个方面展开研究工作：1）在允许节点从多个邻居节点接收数据的情况下，为任意给定网络中的可分数据处理优化建立模型；2）运用新的分析方法找到所建模型的最优解；3）在最优算法的基础上设计低时间复杂度且性能优异的经验算法。项目解除了等效节点法中对节点接收数据的限制，进而为可分数据处理速度的进一步提升奠定了基础。为了对新建模型进行求解，我们提出了一整套的分析方法，不仅找到了模型的最优解，还为设计有效的经验算法提供了思路。本项目的研究具有重要的理论创新和广泛的应用前景。
面向社会影响力的多源异构在线社会网络建模与分析研究	在线社会网络;社会影响力;异构网络;网络分析	在线社会网络（如即时通信、微博、微信、email等）已深刻影响人类社会经济、政治和个人生活，而社会影响力是在线社会网络能真正作用于实际社会网络的核心因素，因此对其建模与分析对于理解、利用社会网络规律，进而优化在线社会网络，服务人类具有重要意义。本项目拟基于复杂网络分析、数据挖掘相关研究成果，利用元信息建模完成异构在线社会网络基础数据以及数据间相互关系的完整一致表示；根据社会影响产生、传播的内在规律，利用元网络分析技术对异构在线社会网络中网络结构、个体社会属性、交互信息以及行为各对象间相互关系构建多个层面的网络，进而对各层面网络独立及组合分析研究，挖掘隐藏信息，实现社会影响力的评估建模；从时间维度研究在合适时间尺度上社会影响力与网络的协同演化规律，分解影响在线社会网络演化的各因素，利用多事件流建模各种变化，再综合各因素构建社会影响力与在线社会网络演化模型，实现科学优化、合理利用在线社会网络。
基于网络编码的无线网状网性能优化技术研究	无线网状网;网络编码;网络编码性能增益;流内网络编码;链路容量	针对无线网状网中基于网络编码的性能优化面临的问题，本课题将结合无线网状网数据传输的广播特征和网络拓扑的多跳特征，从系统角度分析网络编码与性能优化之间关系，并基于跨层设计与协同思想，从MAC层、网络层等不同层次剖析信号干扰、拥塞控制等对网络编码性能增益影响的内在机理，建立相应的性能增益评价模型。在此基础上，以"分析影响机理-建立评估模型-研究优化技术-提供协议支撑"为研究的技术主线，从"MAC层网络编码性能增益评估/优化模型"、"网络编码感知的QoS路由评估/优化模型"、"容量感知的自适应流内编码效能评估/优化模型"等层面将对基于网络编码的无线网状网性能优化所涉及到的关键技术开展研究。
制造物联网协同感知的服务组合优化模型与寻优算法研究	非确定环境;协同感知;制造物联网;服务组合;动态制造过程	制造物联网是实现传统制造过程信息化、智能化的重要手段，其中人、物料、在制品、产品等制造资源的快速流动、生产过程多种工艺与环境参数动态变化等过程，需要多传感器协同感知来保障其过程监测的数据准确性与完备性。.本项目开展协同感知的服务组合优化模型与寻优算法研究，主要包括：制造环境传感器节点实际感知区域快速高效获取方法与描述模型；协同感知的覆盖质量评价方法，多目标关注的协同感知服务组合优化数学规划模型；传感器节点服务的功能域、空间域关联图模型，多优化目标间的博弈均衡方法以及高效低计算复杂度的功能域、空间域联合搜索寻优算法。.本项目将在启发式事件移动的不规则感知区域快速高效获取方法、确定覆盖与概率覆盖结合的覆盖质量评价方法、基于合作博弈策略的多目标粒子群的空间域服务组合寻优搜索算法等方面形成创新性成果，为制造物联网协同感知提供理论依据与技术支撑。
容错处理器网格的高效重构技术	片上网络;可重构网格;容错技术;重构算法	网格连接的多处理器阵列是人们推崇的高性能体系结构之一，它被广泛地应用在大规模并行分布计算、图形图像处理中。随着VLSI技术的高度发展，越来越多的计算单元可集成在单个芯片上，并由此发展而来了当下流行的片上网络系统。这种高密度集成的处理器网络，无论在其制造过程还是在其运行过程中，处理器都不可避免地会出现故障，容错重构机制成为系统不可缺少的可靠性保障技术. 本项目对网格连接的可重构处理器阵列，研究设计高效快速的容错重构算法与选路技术；针对低功耗、高性能等不同应用的要求，生成不同优化目标下的最优或近似最优的高质量逻辑阵列，而不仅仅是现有重构技术所能产生的普通逻辑阵列；充分利用硬件具有并行加速的内在优势，率先引入并行算法设计技术，用于快速生成特别是实时系统所需的逻辑阵列；借鉴相应的阵列重构技术为多核片上网络系统的实时作业提供低温高性能子阵列，在完成作业分派的同时，兼顾提高片上系统的可靠性。
嵌入式软件低功耗设计关键技术研究	功耗度量;功耗优化;嵌入式软件功耗;功耗分析;功耗评价	在目前全球倡导"低碳经济"的背景下，嵌入式系统的功耗是一个日益引起人们关注的热点问题，其中嵌入式软件低功耗设计是众多嵌入式应用的重要支撑技术。为了降低嵌入式软件的功耗，本研究首先在完成嵌入式软件功耗度量的基础上，分析嵌入式软件功耗与多个软件层次因素的关联关系，设计一种嵌入式软件功耗融合建模方法，并通过BP神经网络拟合功耗函数，在保持功耗估计速度的前提下，以提高模型精度。然后，在源程序级、算法级和软件体系结构级三个层次上，采取相应措施改善影响嵌入式软件功耗的关联因素，并形成一种基于软件体系结构的嵌入式软件功耗优化方法，以获得功耗的最佳优化效果。最后，从仿真试验与理论分析两方面入手，进一步探讨功耗与速度、实时性、代码空间、可靠性等多指标间内在联系，提供一种综合的评价与验证方案,对提出方法的功耗优化性能进行评价与比较。
面向下一代移动应用的移动云服务关键技术研究	任务卸载;软件定义网络;移动云服务网络;网络功能虚拟化;云无线接入网络	软件定义网络（SDN）与网络功能虚拟化（NFV）是一种软件集中控制、硬件虚拟化管理、资源可动态分配和拓扑可灵活重构的网络优化技术框架，是实现下一代移动云服务的关键技术。本项目针对大规模移动云服务网络体系架构，将SDN/NFV与传统移动网络有机融合，研究网络控制与数据转发的分离机制，实现移动网络拓扑的快速响应与自适应扩展；将云计算引入接入网络，研究移动服务请求与云接入资源的映射关系，提出在线滚动优化的分布式资源共享策略，分析大规模移动接入的分布特性，提出基于联盟形成博弈的协作传输机制，建立频谱接入的动态优化模型；针对异构场景下移动设备的计算密集型任务的随机特性，建立云辅助的移动任务卸载模型，设计卸载可接受的服务质量约束，提出基于在线学习机制的动态任务卸决策算法。本项目尝试为现有移动云服务网络设计一种新的可扩展架构和一系列服务优化框架，提高大规模接入移动云服务网络的可扩展性与服务质量。
非精确加速器结构与编程的若干关键技术的研究	误差容忍;非精确计算;微体系结构;加速器;编程模型	由于受到封装功耗和散热方面的限制，未来芯片上只有一部分晶体管能够同时全速工作。因此，未来芯片上将会集成多种面向不同应用、不同时开启的高能效加速器。我们注意到，大量新兴应用（包括云端的大数据处理和终端的智能计算）都能在一定程度上容忍计算过程中的误差和非精确性。针对上述应用特征，我们可以在可容忍的范围内牺牲计算的精确性，换取加速器性能功耗比的大幅提高。因此，研究这类新型加速器的结构设计和编程方法有重要的现实意义。 本项目拟通过对非精确计算的误差分析和控制，解决非精确高能效加速器的结构设计以及编程方法中的一系列关键难题，包括非精确高能效加速器的结构设计原理与方法、误差和能效之间最佳权衡的非精确计算加速器的结构、非精确计算加速器的验证和测试的方法学、适用于多种非精确加速器的编程环境、多个非精确加速器间任务调度方法等。最终本项目将形成一套非精确加速器的参考设计、样片和编程工具，为国内外同行提供借鉴。
大规模云中基于用户体验和收益优化的能效资源提供技术研究	云收益矩阵;大规模云;能量效益函数;QoS满意度;用户体验	随着云计算规模的扩大、云服务竞争日益激烈和IT能耗持续上升，围绕用户体验、节能减排和收益模型等问题的云资源管理策略研究具有重要的理论及实际意义。本项目研究大规模云中基于用户体验和收益优化的能效资源提供技术。针对大规模云中应用的多样性，提出组合策略实现差异化资源提供。研究基于用户体验矩阵和云收益矩阵的云服务选择策略，采用层次分析法和效用函数，设计基于用户占优和云服务商占优的云服务选择算法，以实现协同优化。分析能效和用户体验之间的关系，运用能量效益函数实现用户QoS满意度和能耗平衡的虚拟机提供方法。运用对偶分解技术，提出基于云效用优化的资源提供算法，实现云效用优化及用户公平性。研究基于DBSCAN密度聚类的能效虚拟机提供方法，减少虚拟机创建开销。从碳排放、能耗、收益、QoS等方面研究基于收益优化及能耗感知的虚拟资源提供技术。构建云实验床，对所提出的方法进行验证和测试，并提供云平台示范应用环境。
面向嵌入式系统的TLC NAND闪存存储系统优化技术研究	NAND闪存;TLC;嵌入式系统;存储;闪存转换层;NAND	NAND闪存(NAND Flash Memory)在数据存储领域扮演着越来越重要的角色，已被广泛应用于嵌入式系统中。TLC NAND闪存作为最新闪存技术，可以大幅提高存储容量并降低成本，但是也带来了可靠性及读写性能的下降。着眼于解决这些问题，本项目将基于TLC NAND闪存存储系统的结构特性，研究面向嵌入式系统的TLC NAND闪存存储系统理论框架和软硬件协同设计优化技术。我们将在以下三个方面开展研究：(1)构建影响TLC NAND闪存存储系统整体运行效能的理论关系模型；(2)基于TLC NAND闪存存储系统的分层存储结构模型，设计跨层数据管理和优化技术，以提高存储系统的读写性能；(3)研究提升存储系统整体运行效能的动态模式转换技术，以提高存储系统的可靠性和使用寿命。最后，我们将在嵌入式系统开发平台上设计实现系统原型并进行实际验证。
异构并行环境下的MapReduce资源调度模型与方法研究	任务调度;MapReduce;并行计算;异构系统	如何构建面向CPU/GPU异构环境的Map-Reduce系统将有助于解决超级计算在大数据处理方面的实际应用难题。本课题首先通过分析异构机群系统的特点，提出其面向Map-Reduce的结构划分策略，并由此提出 Map-Reduce在大数据处理中的随机任务图模型，以描述随机任务间的基本关系。其次提出了基于随机概率分布的任务量动态预测算法，并在此基础上实现了基于计算任务类型的动态资源调度模型。最后通过在CPU/GPU节点上建立面向大数据处理的线程级负载均衡及节点容错模型，推进了Map-Reduce资源调度模型在异构机群上的实现。本课题将基于国家超算长沙中心的天河1号异构超级计算机系统对本项目的理论和原型进行全面的性能评估与测试。本项目中的所建立的异构环境下的Map-Reduce资源调度模型将是云服务在超算平台上部署和应用的突破，是提升大数据处理并行计算性能的关键，具有重要的理论与实际价值。
云计算中基于多维空间拓扑映射的资源机会复用技术研究	拓扑映射;云计算;资源配置;机会复用	"按需服务"要求云平台为用户提供数量适当的资源，而资源复用则是云平台整合用户需求的基本途径，两者的本质目标都是通过保持云端合理的资源利用率，满足需求并降低成本。本项目以提高云平台资源利用率为基本出发点，以建立高效资源复用机制为核心，拟以准确刻画用户需求出发，研究包括资源需求拓扑的资源模型和需求映射机制，确立资源的概率使用模式，由此研究冲突约束的资源机会复用机制。在此资源复用框架下，研究该机制的核心资源调度技术：从满足用户请求的资源预分配角度，研究基于多维空间拓扑映射的均衡资源配置技术，实现分时复用的可伸缩资源配置机制，以达到整体最小化资源占用；从运行态资源调整角度，研究支持快速伸缩的自适应资源重配置技术，以适应负载波动并能保持系统高效率运行。上述成果将运用于我们现有云平台原型，并验证其正确性和有效性。该系统化的资源机会复用机制研究将促进资源利用率的提升，为云服务用户与云平台降低成本。
大型数据中心热管理中基于热成像节点的热点成因自动诊断方法研究	状态评估;功耗问题;性能优化	节能减排关系到我国的可持续发展。大型数据中心（DC）的能耗巨大，并且随着大数据时代的到来，其能耗密度、规模和数量都在不断增加。在DC的所有能耗中，用于机房空调的大约40~50%。提高空调制冷效率可以有效节省大量能耗。普遍存在的局部高温，即"热点"，严重影响了制冷效率。因此，项目组以提高空调制冷效率为目的，基于热成像节点，利用图像处理与模式识别技术，研究DC内部热点的成因自动诊断方法，进而为DC的热管理提供辅助判断依据。具体研究内容包括：以热点成因为属性，建立热点的红外图像库；以红外图像中的热点和服务器为对象，研究快速图像分割方法；基于红外图像数据，在空域和时域研究对热点成因敏感的特征集合提取方法，特征集合包括热点特征和受热点影响的服务器的相对特征；以分类器的分类性能为评价标准，研究特征集合的降维方法。最终形成的方法可以利用热成像节点对DC内服务器表面上的热点的常见成因进行自动诊断。
关联性感知的并发图计算系统优化机制研究	图计算;海量数据处理;数据密集型并行计算系统结构	图计算平台上通常有多个图分析任务并发地对图数据进行多方位分析，以获得各种目的性分析结果。然而，由于没有感知各图分析任务数据访问关联性和图顶点之间依赖关联性，在目前图计算系统中，并发图分析任务面临低效的cache、内存和数据总线共享使用以及冗余数据访问和处理，导致数据访问瓶颈和性能干扰问题。因此，本项目拟研究关联性感知的并发图计算系统。其研究内容包括数据访问关联性感知的多图分析任务执行机制、基于依赖关联性的单图分析任务执行机制和以数据为中心的运行时资源分配及调度策略。这样，本项目期望利用各图分析任务之间数据访问空间/时间关联性，充分共享图数据及其访问，减少整体存储需求和数据访问量；然后利用图顶点之间依赖关联性特征，减少单个图分析任务中冗余数据访问和处理；最后通过以数据为中心的资源分配及调度，在上述基础上感知和利用关联性，进一步有效共享cache、内存和数据总线，最终达到提高系统吞吐率的目的。
基于位置服务的社交网络中的数据安全与隐私保护机制研究	基于属性的加密;细粒度访问控制;关键字搜索;位置隐私	随着移动互联网和基于位置服务技术的发展，越来越多的社交网络用户随时随地的与其他用户共享自己的数据信息，但保护用户数据的安全与隐私是基于位置服务的社交网络广泛应用的关键，研究其中的数据安全与隐私保护具有重要的价值与意义。本项目面向"诚实且好奇"的云存储环境，依据多用户环境下应用场景的需求，研究基于位置服务的社交网络(LBSNs)中的数据安全与隐私保护机制，并提出了相关的模型与算法。针对社交网络中用户隐私保护的查询问题，提出基于位置邻近的多维关键字查询模型与算法，保证云端数据的安全且支持多维关键字搜索；针对社交网络中用户数据的访问控制问题，提出了基于层次属性加密的访问控制模型，来防止非授权用户和云服务器的非法访问；针对用户个性化的位置隐私保护问题，提出基于动态策略的隐私保护方案。最后，项目采用真实的和合成的社交网络数据来验证所提出算法的有效性。
新型异构多核系统的渐近拟合优化技术研究	渐近拟合;存储模型;自动映射;异构多核	本项目旨在：1) 面向异构多核为特征的新型高性能计算机系统结构，通过层次化存储模型建立硬件统一抽象，研究以协处理器为中心的性能优化理论与存储访问优化方法；2) 研究面向新型异构多核架构的软件向硬件映射机制，分析协处理器编程特点，通过源到源编译转化技术解决传统编程模型应用向新型异构多核高性能计算机系统的映射，提高应用可移植性，充分发挥海量加速协处理单元的性能；3）利用动态剖分技术，将映射机制和性能优化策略相结合，形成渐近拟合优化方法，实现应用高效移植的同时充分挖掘新型系统架构的性能。.解决面向新型异构多核高性能计算机系统性能优化的基本理论问题，提高新型异构多核系统下应用性能及可移植性，在理论创新和专利申请方面取得成果，使得本项目的研究成果达到国际先进理论水平。研究成果对高性能计算系统结构基础理论，以及《国家中长期科学和技术发展规划》提出的先进计算平台建设有重要意义。
千核级通用微处理器共享存储体系结构研究	页面属性;低功耗;众核处理器;共享存储;TLB一致性	未来千核级通用众核处理器支持共享存储编程模型是一种必然趋势，但面临延迟、带宽以及数据使用方式等方面的重大挑战。传统上通过专门硬件（总线或目录）维护Cache一致性的共享存储结构的代价、性能和功耗可扩展性有限。本项目针对千核，甚至万核级通用众核处理器，研究应用软件、系统软件、微体系结构三个方面协同管理共享存储资源的方法，提出一种新型共享存储结构- - D2SM（Dynamic Distributing Shared Memory，动态分布共享存储结构），课题将深入研究该结构在传统科学计算，以及新型高通量计算领域里的性能、功耗等微观特性，旨在解决千核级共享存储结构中面临的问题，对未来自主高性能众核处理器研制有重要的理论和实际意义。
面向高效透明虚拟化的多核体系结构关键技术研究	多核;多线程;指令扩展;虚拟化;动态二进制翻译存储器	虚拟化计算技术展示了新型的计算机理与模式，具有广阔的应用前景，其发展和应用将会给人类生活和生产的各个领域带来深远影响。课题结合当前计算系统多核化、异构化的发展趋势，深入研究支持高效透明虚拟化的多核体系结构关键技术，通过降低虚拟化软件自身的运行开销以及应用程序的运行开销提高整个系统的效率，通过动态二进制翻译技术屏蔽多核平台的异构性，以实现整个虚拟化系统的高效、透明。本课题的研究内容包括：分析虚拟化软件模块的运行机制和开销，创新地提出并研究动态二进制翻译存储器体系结构，基于指令扩展技术研究面向高效透明虚拟化的处理器核优化设计方法，研究计算加速器的虚拟化方法、动态二进制翻译和优化模块在多核平台上的并行化方法、动态二进制翻译与VMM的融合方法、基于动态二进制翻译的应用程序多线程化方法等，上述研究工作的成果能够为多核平台下的虚拟计算系统提供体系结构和支撑环境的支持，具有重要的理论意义和实用价值。
软件指导的高性能计算机系统功耗和热量管理	高性能计算机;功耗问题;编译器;热量问题;系统软件	功耗问题和热量问题是研制新一代高性能计算机面临的严峻挑战，功耗管理和热量管理技术将是高性能计算领域研究的热点。本课题研究软件指导的高性能计算机系统功耗和热量管理技术，预期研究成果包括（1）高性能计算机系统功耗和热量测试平台及高性能计算机系统部件功耗和热量特性分析技术。（2）系统软件指导的高性能计算机系统功耗管理和热量管理技术。（3）基于程序剖视（Profile）的编译指导的高性能计算机系统功耗管理和热量管理技术。本项目的研究成果包括实际的测试环境和高水平论文，取得的研究成果将直接应用于高性能计算机系统的研制任务中。
基于自适应结构网格的NUFFT并行算法研究	自适应结构网格;NUFFT;并行计算	FFT可在降低数值模拟计算复杂度的同时保证计算高精度，但其只能用于均匀采样生成的网格。在众多需要捕捉局部关键区域信息的应用FFT的数值模拟中，因受限于均匀采样而对网格全局加密，势必会因计算资源的有限性而影响关键区域网格密度，无法有效捕捉局部信息。为解决这一问题，申请者尝试将自适应结构网格加密方法(SAMR)与FFT结合，这面临两个挑战。一是在非均匀采样网格上计算FFT。尽管有学者提出了非一致FFT(NUFFT)来解决，但现有NUFFT大多网格针对性很强，对于由SAMR生成的具有明显局部特性的表层网格不具普适性，必须开发新的NUFFT算法。二是NUFFT的并行算法,这在国内外仍属空白。本项目主要研究：适用于SAMR的NUFFT串行算法；适用于SAMR的NUFFT并行算法；面向上千个处理器的适用于SAMR的NUFFT可扩展并行软件。旨在降低程序研制难度、缩短程序研制周期，为相关需求者提供支撑。
数据中心延迟敏感型应用尾端响应时延服务质量保障方法研究	服务质量保障;尾端时延;数据中心;延迟敏感型应用	数据中心延迟敏感型应用对请求尾端响应时延服务质量有着极其严格的要求，然而请求的大规模并发、多阶段处理和系统状态的动态变化，都对请求处理的响应时延带来不确定性影响，从而对请求尾端响应时延的服务质量保障带来了新的挑战。本课题以延迟敏感型应用请求处理过程为线索，自顶向下依次探索应用层面、运行时层面和体系结构层面的响应时延缓解技术。在应用层面研究延迟感知的请求调配方法，选择满足服务质量要求的节点调度请求，避免节点不匹配严重拖长响应时延；在运行时层面研究面向服务质量的运行时管理机制，在线管控请求对系统资源使用的优先级和速率，缓解负载水平和计算行为变化等因素对响应时延的影响；在体系结构层面研究体系结构自适应的任务调度方法，在满足服务质量要求的基础上发挥不同体系结构特性的优势，改善数据中心的运行效率。同时通过不同层面间响应延迟缓解技术的有机结合，实现尾端响应时延服务质量保障的系统性解决方案。
面向云计算的混合诊断与自修复理论研究	混合诊断;自修复;可靠性;云计算;Multi-agent	云计算提供廉价高效的服务是以数量庞大的计算节点和网络传输为基础的，融合了网格计算、分布式计算、并行计算、效用计算、网络存储、虚拟化、负载均衡等技术。云计算这种模式将会成为各行业在其IT支撑系统中实现弹性计算、资源共享以及节能减排的重要技术手段。.在云计算这样复杂的平台中，需要实时监测大量物理节点、虚拟主机、网络状态和用户信息、以及自动故障应急和处理。然而在如此大规模多节点的平台中，某个（某些）节点的故障和错误在云平台中会经常出现，仅靠人力进行所有的监控和修复是远远不够的。为解决此问题，本项目以云平台的实际需求为基础，提出基于结果的推理方法，采取多值决策树、贝叶斯网络、神经网络、支持向量机混合诊断的方式，并且结合多代理技术联合监测网络和主机状况，实现云计算服务中故障的自我诊断和自修复。因此该研究具有重大的理论和现实意义，可以为云计算在各行业的顺利推广和应用打下良好基础。
基于BDD的大规模静态失效关联系统故障树分析	BDD;静态失效关联;变量排序;故障树分析	本项目针对现代电子信息系统失效行为越来越复杂和相互关联这一发展趋势，以大规模静态失效关联系统（主要包括蕴含关联和互斥关联）故障树BDD分析方法为研究对象，根据已有研究中存在的变量排序和BDD生成算法问题，重点研究：基于特征参数的静态失效关联故障树基准库构造、基于互补性完备性分级评价指标和设计-评价-设计迭代机制的变量排序策略库构造、自适应高效系统BDD生成算法设计。最终获得能够有效涵盖各种故障树结构特征的变量排序策略库，以及允许任意可能变量排序的自适应BDD生成算法。该研究对于在大规模静态失效关联系统故障树分析中成功应用BDD方法有着重要的理论意义和应用价值。
面向国产通用DSP的类OpenMP并行程序设计方法	基于反馈的自动循环展开;通用数字信号处理器;优化空间搜索及自动向量化;HPL;类OpenMP编程方法	通用数字信号处理器GPDSP在芯片限售令的封锁和限制下将加速成为国产自主可控高性能加速器的核心。但是，GPDSP不支持OpenMP并行编程模式，开发高效的GPDSP并行应用程序需要程序员显式管理GPDSP设备、数据通信和任务调度。为此，本项目提出了一种易于编程、易于调优的类OpenMP并行程序设计方法，该方法主要包括：（1）以简化GPDSP并行编程为目标的类OpenMP编程命令指示字扩展；（2）为了充分利用GPDSP向量计算单元，提出了基于反馈的自动循环展开RLU技术和设计了优化空间搜索及自动向量化OSp4Vector方法；最后通过基准测试程序HPL验证有效性。面向国产通用DSP的类OpenMP并行程序设计方法能够很好地屏蔽GPDSP体系结构和向量处理单元细节，显著减少GPDSP程序开发与移植的工作量，降低程序优化难度，促进国产自主可控高性能处理器的发展和推广应用。
阵列编码及其在存储系统中的应用	阵列编码;MDS码;存储容错	阵列纠删码在存储系统的容错技术中发挥着重要作用。较之经典的RS码与LDPC码，阵列码可以在保持最少冗余度的同时兼顾降低编码/解码的计算复杂性，因而受到越来越多的研究者的关注。尽管目前已有多种阵列码被构造出并被应用到实际系统中，但它们大多为2容错码。随着云存储等海量信息应用的不断发展，对多容错阵列码的需求日益紧迫。目前已知的几种多容错阵列码的构造均有较多的限制(比如仅当码长为素数才能达到最优性能)，对于"在给定参数及相关优化指标的条件下，如何构造最佳编码"的问题，仍缺乏理论依据及有效的构造方法。.    本项目以冗余度、更新复杂度以及计算均衡性作为指标，通过扩展现有编码、代数及组合构造辅以计算机搜索等方法，研究各种存储系统需求下的多容错阵列码的存在性及构造、研究实现快速编解码算法、以及探讨各种码的关系和各种性能指标的界。这些研究无论从理论上还是实践中都有十分重要的意义。
面向大数据的计算与存储融合CPU体系结构研究	计算机体系结构;微处理器;三维集成;大数据;非易失存储器	近年来，以基于海量数据的大规模学习、数据挖掘以及新型互联网服务为代表的大数据应用成为学术界与工业界的"明星应用"。相对于传统应用，大数据处理所涉及的数据量大，强调高吞吐、强并行、低延迟等特性。然而，传统通用CPU在计算密度、访存性能、并行性开发和功耗等方面都无法满足大数据处理应用发展的要求。因此，分析传统CPU在面向大数据处理所存在的不足，研究面向大数据的高效CPU体系结构与关键技术，为研制高效大数据系统提供基础计算部件，具有重要的学术价值和应用前景。目前，面向大数据的CPU相关研究才开始起步，将三维集成新型非易失存储介质应用于CPU设计还处于概念形成阶段。本项目抓住这一契机，研究计算与存储融合的片上三维CPU体系结构，拟在重定制处理器资源、低功耗技术、编程模型硬件支持以及原型验证系统等方面取得突破，为解决阻碍通用大数据CPU发展的核心问题探索新的道路，以期在大数据系统研究中抢占先机。
面向大规模并行应用通信优化的多目标细粒度拓扑映射方法研究	互连网络;拓扑映射;高性能计算;通信性能优化;通信模式	拓扑映射优化方法可用于解决应用通信模式与网络结构之间的适配性问题，是提升大规模并行应用通信性能的重要技术手段之一，也是当前高性能计算领域研究的热点和难点。然而，随着高性能计算系统和应用规模增大、复杂通信模式混合、节点结构多核化、众核化，拓扑映射优化正面临诸多新的挑战：大规模复杂通信模式混合中通信特征识别效率问题、多目标约束下拓扑映射效率和准确性问题、多通信特征混合下通信性能评价准确性问题。本项目以提高大规模并行应用通信性能为目标，以拓扑映射优化方法中通信特征识别、拓扑映射、通信性能评价等关键步骤为主线，研究并建立一套拓扑映射优化新方法：基于图相似和局部性匹配的通信特征高效识别方法、面向多目标优化的细粒度高效拓扑映射方法、面向拓扑映射的通信性能多维量化评价方法。项目研究工作对提升大规模并行应用性能、促进高性能计算领域技术创新有重要的理论意义和应用价值。
鲁棒的数据驱动的飞机故障诊断技术	数据驱动;飞行数据;鲁棒性;故障诊断	数据驱动的飞机故障诊断技术可以充分利用飞行数据对飞机进行故障诊断，提高飞机的安全性。飞行数据的复杂性、多维性、随机性、不完整性及不平衡性等特点，造成故障诊断结果对具体飞行数据值以及随机噪声敏感等问题。本项目主要研究如何提高数据驱动的飞机故障诊断技术的鲁棒性，并对设计的方案和算法进行理论分析和仿真验证。研究内容主要包括不完整飞行数据源的完备化处理、特征性能参数提取、鲁棒的多维非线性飞机性能参数分类。具体包括提出了鲁棒竞争聚类技术，并利用该技术对无标记飞行数据进行样本化处理；采用相关特征向量机方法获取飞机的性能参数与故障模式的关联关系；建立了摄动支持向量机模型，并利用该模型进行故障诊断，提高了诊断算法对噪声的免疫能力。力争通过本项目的研究，提高数据驱动的故障诊断系统在各种噪声扰动情形下保持工作性能指标的能力。为我国军机和民机的故障诊断、故障预报、健康管理等技术的发展提供可行的解决途径。
面向多核虚拟集群的并行应用性能优化方法研究	程序优化;云计算;虚拟集群;并行计算模型	随着多核处理器和云计算技术的普及，多核虚拟集群以其低成本、按需灵活配置的优点，成为运行并行应用的重要平台。同步通信和负载均衡是影响并行应用性能的关键问题。然而，虚拟集群容易使其承载的应用陷于对CPU资源的竞争，从而导致现有并行应用的同步通信和负载均衡机制性能低下。本项目以优化并行应用的同步通信机制和动态负载均衡特性为切入点，进行面向多核虚拟集群的并行应用性能优化方法研究。具体内容包括：基于过载虚拟机条件的并行应用性能分析模型，在此基础上，重点针对高性能无等待并发数据结构的构建技术、基于自适应任务创建与窃取的两级负载均衡技术两项关键技术方面开展研究，并依托我们已建立的IaaS实验平台进行系统集成，结合并行应用性能分析模型开展关键技术的评测和验证。本项目有助于同时保障云服务提供商和客户的利益，从而为推动云计算平台及其支撑业务的健康发展具有重要的应用价值。
基于密钥技术的多类型混合无线传感器网络安全机制研究	无线传感器网络;多媒体传感器网络;安全密钥;多类型混合	无线传感器网络中，可能包含了采集如温度、湿度、光强等标量信息的普通传感节点，也包括了采集如图像、视频和音频信息的多媒体节点，这种多类型混合的特征使其面临新的安全课题，例如如何加密数据量庞大的多媒体信息，如何通过安全协议来实现混合传感网络节点之间的通信。本申请课题着眼于多类型混合无线传感器网络的安全密钥技术展开研究，拟分为四个子课题：一是研究安全轻量级的密钥管理协议，使其能同时满足标量数据和媒体数据加解密和认证的需求；二是针对于多媒体传感器节点，结合灰色系统理论，提出基于密钥的信息隐藏算法；三是研究新型的安全路由协议，利用可再生的Hash密钥函数来保障数据传输的可靠性；四是对多类型混合无线传感器网络的信任机制展开研究。最后通过NS-2仿真实验拓展平台和节点实测两种方式来验证所提出的算法和协议。
虚拟环境中处理器高速缓存管理的关键技术研究	高速缓存器;虚拟机;替换策略;预取机制;虚拟机管理器	虚拟化技术是云计算底层基础架构中的关键技术之一，备受多方关注。然而已有研究工作和商用技术不能在虚拟环境中充分利用多核处理器的私有和共享Cache资源，也未能通过虚拟机管理器对Cache资源进行有效管理。因此，本课题提出一套云计算虚拟环境中多核处理器Cache的管理策略，具体包含三方面研究工作：1）挖掘Cache替换和预取策略的潜在关系使二者结合以提高私有Cache性能；2）采用基于虚拟机访存流特征的管理策略提升虚拟机之间共享Cache的性能；3）在虚拟机管理器（VMM）中使用基于指令动态插入的Cache管理机制以实现Cache的软件控制。通过涵盖从宏观用户体验至微观体系结构各层级指标参数的实验评测方法，提高云计算虚拟环境下多核处理器Cache的利用率和命中率，最终获得虚拟机性能和用户体验的共同提升。本项目拟搭建两套实验和模拟平台，发表8-10篇SCI/EI索引论文，并申请10项以上专利。
基于CPU-GPU异构平台的SAR成像算法并行优化和性能度量	SAR成像算法;CPU-GPU异构平台;性能度量;并行优化	实时合成孔径雷达（SAR）成像技术是当前军事和遥感领域的研究热点。SAR成像处理具有庞大的数据量和运算量，对高性能计算的需求巨大。新型的CPU-GPU异构系统具有强大的计算潜力,如何采用CPU-GPU异构系统加速SAR成像算法是值得深入研究的问题。CPU-GPU异构系统具有特殊的执行模式和复杂的编程方法，对基于该平台的SAR成像算法的并行优化和性能度量提出了严峻挑战。为此，本课题以CPU-GPU异构系统为平台，深入研究SAR成像算法的并行优化和性能度量技术,主要研究内容包括三个方面：首先基于异构平台的性能优势，深入挖掘SAR成像算法的可并行化特征；在此基础上，研究SAR成像算法的协同并行模式、存储优化技术和延迟隐藏技术；最后建立统一度量综合计算性能和开发开销的综合性能度量模型。本项目的研究成果将推动基于异构系统的并行优化理论研究，对于促进高性能SAR成像技术的发展和应用具有重要意义。
片上光网络的温度敏感性分析和功耗优化	温度敏感性;功耗优化;热效应;片上多核处理器;片上光网络	近年来片上多核处理器成为嵌入式系统和高性能计算领域的重要平台。基于硅光互连的片上光网络被提出在未来的片上多核处理器中替代电互连，克服在功耗和带宽上的局限性。然而由于热光效应，片上光网络中波长选择性器件和激光器的波长对温度敏感。考虑到芯片上存在温度梯度，这些温度敏感性将导致额外的功耗。本课题将重点研究以下问题解决这一挑战：1）如何对片上光网络的温度敏感性进行系统级建模和分析；2）如何通过低温度敏感性设计方法和功耗优化技术解决温度敏感性难题；3）如何评估芯片上温度梯度对片上光网络功耗和性能的影响？本课题将提出一个参数化的片上光网络热效应分析平台；并以此为基础，提出高性能三维片上光网络的低温度敏感性设计和功耗优化技术，包括低温度敏感性光开关、低功耗路由算法和动态功耗控制；最后开发一套仿真平台评估新架构在芯片上温度梯度影响下的功耗和网络性能。本课题为解决片上光网络的温度敏感性问题有重要的研究意义。
基于数据多维属性的低能耗数据分布策略研究	数据多维属性;数据布局;能耗管理	海量数据中心的存储系统规模不断扩大导致系统的能耗相关运行维护费用大幅增加，如何在确保应用获得连续快捷的数据服务同时，降低系统运行能耗，成为海量数据中心急需解决的问题。本项目从存储系统全局出发，提出基于数据多维属性的低能耗数据分布策略，根据数据在时间、空间、功能、内容等方面的多维度属性和用户访问特征，主动将新增写数据汇集保存在与之有关联的活动数据区域，减少数据被动重定向而引入的存储设备反复启/停运行能耗；根据数据布局状态和设备能耗特征以及用户对服务质量的期望，动态选择不同服务策略；建立低运行能耗、高性能、可扩展的"绿色"海量存储系统。
大规模计算平台的失效分析方法研究	海量系统日志;失效规律和模式;故障定位与修复;检索;失效分析	因良好的易构建性和可扩展性，机群系统已成为生产性计算平台的首选，但随系统规模的不断增加，失效已经成为一种常态，已有研究显示它对性能和运行成本有着重要的影响，因而成为研究热点。系统的海量日志是失效分析的重要数据源，因为缺少全局时钟，数据源隐藏的信息通常会乱序，而且内容具有不完整性，因而失效分析成本极高，而现有工作主要基于数据的预处理做离线分析。本项目旨在探索可应用于海量系统日志的信息检索方法，在基于可检索的系统日志的基础上发展适用于大规模计算平台的失效分析方法，提供时序和空间相关的失效在线检索；进一步从负载、热量、功耗，软件老化等多因素的角度研究失效规律，及各因素对失效的独立、综合影响；挖掘失效模式，刻画其在时间和空间维度上的特性；探索失效关联机制和故障定位方法，及辅助的失效解决方案。 从而提高大规模计算平台的生产率，降低总拥有成本。
带电生物大分子相互作用预测的高性能算法研究	密度泛函;生物大分子;Newton-GMRES;GPU;异构多核	带电生物大分子的相互作用已成为生命科学、医学以及物理学的研究热点。双电层密度泛函理论对带电生物大分子的相互作用的预测准确性已被证明超过众多传统理论，但二维问题求解时该理论对计算能力和内存有巨大需求。本项目拟提出一套基于Jacobi-free Newton-GMRES方法求解二维双电层密度泛函的高性能计算解决方案，分别从算法优化、并行求解以及硬件加速这三个层面来提高计算性能。其中，采用Lagrange乘子法处理电中性约束等约束条件。双电层密度泛函理论涉及到的短程作用计算采用FFT算法进行加速，而静电作用采用求解微分方程的方法降低计算复杂度。根据这两种作用的计算分属计算密集型和通信密集型的特点以及密度泛函中不同作用可叠加的性质，将算法异构性映射到CPU-GPU这一异构体系结构，通过优化CPU-GPU的负载平衡充分发挥硬件的性能。该方案使密度泛函预测带电生物大分子同电荷吸引现象的成为可能。
虚拟机计算资源调度中关键技术的研究	调度算法;并行计算;虚拟计算	由于在计算资源管理方面带来的便利性，近年来，虚拟计算技术在复兴后得到了广泛的普及。然而，该技术在应用中也暴露出很多的不足与尚待提高的地方，其中，对调度算法的不断改进和完善是非常重要的部分，也是该领域的研究热点和重点。同时，由于虚拟机在调度单位上与传统操作系统的不同，导致在该环境下的调度问题与在传统操作系统中的调度相比，出现了很多新的特点，使得传统的调度思想无法直接应用到虚拟机中，这使得调度问题也同时成为研究的难点。本课题的提出，旨在对虚拟机环境下的计算资源调度问题中的关键技术进行研究，提出新的算法解决虚拟客户环境下的调度问题，并在此算法的基础上提出协同调度思想以及解决过载调度问题的方法。本课题的研究成果将对虚拟计算技术的发展，以及应用的普及起到积极的推动作用。
高性能异构多轨网络体系结构研究	高性能计算机;高性能互连网络;多轨网络	多轨网络的出现，为突破半导体工艺限制，解决网络带宽墙问题提供了方法。但现实中采用多轨的多层并行网络由于资源消耗大，无法用于大规模并行系统。如何有效降低对资源的需求，发挥多轨网络的性能优势，是网络体系结构研究面临的重大挑战。同时，不同应用在通信模式上存在巨大的差异，而同构多轨网络不能满足所有应用的需求。如何融合不同特性的网络，支持不同通信模式是网络体系结构研究的又一挑战。另外，随着网络规模的扩大，网络失效将成为常态。如何在网络部件失效频繁发生的情况下，维持网络的高可用性并且防止性能的下降是网络体系结构研究的另一挑战。基于以上问题，本课题提出了异构多轨网络结构，通过融合不同类型的网络，以达到降低资源需求、满足不同通信模式的目的。同时，通过引入一套基于链路失效消息传播的分布式动态容错路由机制，来保证网络失效情况下的高可用性和高性能。在资源受限的条件下，实现高性能高可用的异构多轨互连网络。
大规模集群环境中虚拟机迁移关键技术及相关算法研究	迁移框架技术;无宕机实时迁移;迁移本体技术	数据的裂变式增长，使得面向海量数据处理的大规模集群技术成为了未来的研究方向，在这样的背景下，虚拟机迁移技术能够打包资源与服务使其跨越物理节点边界的鸿沟，将异构的集群硬件资源转换为一个同质资源池，其重要性不言而喻。但是，已有的迁移技术都存在不足，面对不断扩张的集群规模无法做到一致高效的实时迁移，同时，新环境也带来了新挑战，如多虚拟机的并行迁移调度问题。本项目拟首次提出无宕机实时迁移技术，以多模式可变粒度Shadow Memory、外存访问跟踪、堆栈式用户态迁移体系等技术和机制为支撑，以最小化迁移时间、最大化服务质量为目标，结合内外存数据调度模型与算法，实现大规模集群环境中的虚拟机一致高效实时迁移。同时，本项目拟对迁移框架技术展开深入研究，通过行为模拟和成本估算，将多种虚拟机迁移过程模型化，进而以成本最小化为目标，设计并实现一整套指导和协调大规模集群环境中虚拟机并行迁移的相关算法。
针对片上多处理器系统中软错误的高效系统级容错技术研究	软硬件协同设计;片上多处理器;片上网络;调度算法;性能	随着纳米技术的不断进步，芯片上晶体管的集成密度持续增加，从而导致软错误发生得更加频繁，由此引起的可靠性问题对下一代嵌入式片上多处理器系统的影响显得日益突出。现今针对多处理器的软错误保护技术均给系统带来不小的额外开销。它们或者需要额外硬件的辅助，从而导致芯片面积和功耗的增大；或者需要冗余软件的执行，从而导致整体性能的下降及能耗的增加。无法满足系统对更高性能、更高可靠性、更小尺寸和更好扩展性的需求。本课题将针对片上多处理器系统中的软错误问题研究一种高效率的系统化解决方案。将创新地通过软件和硬件协同工作的方式来保证系统的可靠性，降低软错误保护的软硬件开销。进而提出一套自适应的任务调度策略，来减少系统性能和能耗的损失。将基于SystemC创建软错误模拟和分析平台来验证该方案的有效性和扩展性。本课题的研究成果将为容软错误的片上多处理器系统设计提供新思路，为高性能高可靠性嵌入式系统设计提供技术支撑。
面向新型3D NAND闪存的嵌入式存储系统优化方法研究	嵌入式系统;闪存转换层;闪存;存储系统;三维NAND闪存	研究面向新型3D NAND闪存的嵌入式系统优化技术有着巨大的应用价值。NAND闪存的年销售额已超过1600亿元。作为最新一代的闪存技术，3D NAND通过三维方向堆叠闪存来实现存储容量成倍的快速增长。然而，现有的闪存管理技术都是应用于传统二维平面NAND闪存。在嵌入式系统中，如何设计闪存管理技术来有效利用3D NAND在存储机制上的新变化，是亟待解决的问题。本项目结合读写性能、数据完整性等关键指标，开展面向3D NAND闪存的系统软件优化技术研究。本项目的研究成果将为3D NAND闪存的系统软件管理方法提供新思路，为其在嵌入式系统中的应用提供技术支撑。
基于Petri网和DSM的型号产品协同设计过程和数据世系建模及分析方法研究	协同设计;世系模型;过程模型;设计结构矩阵;Petri网	随着离散制造业的飞速发展以及信息化和工业化融合（两化融合）的深入，以协同为核心的CAx一体化集成支撑环境已成为型号研制或装备设计的急迫需求，其相关理论技术也成为当前国际上研究热点和前沿领域。本项目立足型号产品协同设计过程特征，充分融合Petri 网和设计结构矩阵的建模方法，研究基于扩展C-net和实值设计结构矩阵的层次化协同过程模型群，提出依赖过程演化的数据世系模型，给出基于Petri网的过程模型、世系模型和DSM模型分析和优化技术，着力在协同设计过程的基础理论领域开展创新性探索和研究，为协同设计一体化集成支撑环境研究提供理论模型和分析手段的支撑。本项研究的开展对于攻克协同设计共性基础及前瞻性、产业化关键技术，建立起我国协同设计技术自主创新体系，具有积极的意义。
自主通信拓扑结构及合作保障机制研究	行为经济学;博弈论;对等覆盖网;拓扑结构;自主通信	当前网络呈现出的动态、异构、脆弱以及海量吞吐量等特性使得设计一种自管理、自组织，自维护，自优化，自修复等具备"自*"属性网络的需求越来越迫切。本课题引入"自主通信"的概念，研究自主通信拓扑结构和节点合作保障机制。首先，针对信息交互，研究自主通信和P2P的抽象共性，提出自主通信拓扑结构，解决拓扑结构和底层架构不匹配问题、保证局部信息唯一性和信息交互的即时、有效性，解决自主通信效率、损耗和鲁棒性无法较好兼顾的问题。其次，采用博弈论有效解决理性节点的非合作行为，提出面向拓扑结构的博弈机制，通过仿真验证不同博弈机制在拓扑结构中的博弈效果。最后，通过对社会学、管理科学和计算机科学的交叉性研究，借鉴行为经济学理论监管自主通信中非理性节点的不合作行为，建立行为经济学理论模型，提出基于利他性惩罚演化的自约束合作保障机制，进一步提高系统整体合作水平。研究成果可为更好的满足当前网络需求提供理论指导。
基于任务行为特征分析的热敏感操作系统技术研究	操作系统;任务行为特征;热敏感	计算机系统热敏感控制问题是近年来学术界研究的一个热点问题。本课题以平衡系统的性能，公平性，功耗，温度等关键参数为目标，结合操作系统和体系结构理论，研究任务行为特征分析、线程分组管理和保持功能部件状态亲等关键理论和技术，在操作系统中实现热敏感的任务和资源管理框架。与已有研究的差异在于：1.以任务的行为特征分析为研究重点，首次提出"功能事件"概念刻画线程的冷热特征和线程间的依赖关系，在此基础上抽象出任务模式；2.提出新的"线程组"的概念，按不同类型的任务模式进行线程分组，分配CPU和内存等硬件资源；3.首次提出"状态亲和性"的概念，在保证系统性能和线程公平性的前提下，降低系统关键部件的功耗和温度；4.基于线程分组管理和状态亲和性思想，设计热敏感的操作系统任务调度和资源分配算法。本项目对系统级热敏感控制理论和技术研究有重要推动和贡献，对提升我国在该领域的创新能力有重要的意义。
云计算环境下的新型访问控制理论与关键技术研究	信息安全;分布式系统;访问控制;隐私保护;云计算安全	意义：基于云计算的应用在逐步延伸，安全问题已引起人们的高度关注，特别是云计算环境的大规模、分布式、虚拟化等特征，使安全显得尤为重要。访问控制作为安全认证和控制的手段，如基于角色的访问控制方法，已经在网络信息系统中得到了广泛应用。但面对云计算环境下的大用户访问数、细粒度的访问控制策略和高安全性等要求，研究新型访问控制方法就显得十分必要，既有理论研究意义，又有实际应用价值。.内容：本项目研究云计算环境下访问控制理论与关键技术，包括：1）研究云计算环境下访问控制的数学理论与模型，研究模型中的要素及其特征，以及相互间的关系；2）研究云计算环境下访问控制模型的策略构建方法，以及策略构建方法的验证。研究访问控制模型间的等价理论，实现与传统基于角色访问控制的兼容性；3）研究云计算环境下具有隐私保护的安全访问控制方法，实现对访问者与访问内容的隐私保护，研究上述访问控制的实现算法与性能分析。
SoC中滑动窗口应用类IP核高级综合技术研究	设计空间探索;高级综合;IP核;滑动窗口应用;数据重用	IP核是SoC设计的基础和核心。SoC设计需要尽可能地使用现有IP，以搭积木的方式完成大部分设计。其中，IP核高级综合技术实现将硬件行为级描述转化为结构描述，甚至布图描述，提高了抽象级别，使设计者从繁杂的底层设计细节中解脱出来，更加专注于整个系统的设计，提高了设计的效率和正确率，降低了设计成本。本课题面向滑动窗口类应用程序，研究其高级综合技术。设计目标为输入C语言源程序，经过一系列的编译分析和优化操作，自动生成优化的底层Verilog代码编写的IP核。基于滑动窗口类应用IP核的自动生成流程，本课题将围绕如下四个方面展开研究：充分开发数据重用的存储结构模型，RTL级硬件代码自动生成，片上资源足够情况下的设计空间探索技术和片上面积资源不足情况下的多循环程序映射技术。
基于纠删码的大规模存储集群重构优化技术	重构;可靠性;存储集群;纠删码	大规模存储集群是当前云存储与大数据存储系统的基础平台,所包含的成千上万软件与硬件部件随时都可能失效，从而导致服务中断甚至数据丢失等严重问题。传统基于多副本冗余技术在数据量增大时，其运营成本急剧上升，因此使用存储效率更高的纠删码技术成为必然，但传统纠删码具有较差的重构性能。针对这一问题，观察到集群内冗余存储单元、存储节点和网络上下链路使用情况和失效模式具有较强的非平衡性，因此本课题研究通过调度存储集群内的大量处理、传输与存储资源，通过优先恢复低可靠性条带的数据，适当延迟正常用户请求以加快降级读请求，及利用纠删码编码规则提高条带内重构的并行性，设计加快纠删码存储集群的数据重构过程的方法和机制，从而提高系统整体性能、可用性及可靠性。在保持可靠性的同时，有效降低云存储与大数据存储系统的冗余成本。
面向玻色采样量子计算的光子探测技术研究	石墨烯;碳纳米管;玻色采样;量子计算;光子探测	量子计算在大数质因子分解等问题上相比经典计算具有指数加速，是学术界广泛关注的前沿研究。然而，受限于退相干等物理现象，人类至今尚未实现通用量子计算机，无法实证量子计算的强大计算能力。最新研究表明，玻色采样问题属于经典计算的难题，理论上却容易使用量子计算系统实现，有望成为验证量子计算超越经典计算计算能力的重要典范。在实现求解玻色采样问题的量子计算实验中，光子探测器是该计算系统的核心部件，光子数可区分的高效率光子探测是其中最关键的技术之一。本项目面向玻色采样量子计算的实验系统，提出一种新的光子探测方法，期望在研究玻色采样量子系统这场世界竞赛中找到突破。本项目将聚焦超导碳纳米管和石墨烯光子探测器的研究，验证碳基材料在低温环境下作为光子探测器的合理性，优化该类探测器在玻色采样量子计算适用频段下的光子吸收效率，探索光子探测技术对玻色采样实验的影响，为未来基于光量子态量子计算的最终实现提供强有力的支撑。
基于扩展的概率转移矩阵模型的高精度快速广义门电路可靠性评估方法研究	混合编码;屏蔽效应;结构转换;故障相关;迭代PTM	现有电路可靠性评估方法往往存在评估精度与计算复杂度难以同时兼顾的矛盾，导致不同抽象层次电路缺乏统一有效的高精度快速评估方法，使在实际应用中计算结果因标准的差异而不可靠。概率转移矩阵（PTM）模型是目前评估门级电路可靠性的前沿方法之一，其主要优势在于可充分利用矩阵理论对电路的行为特征精确建模，具有极大的灵活性和可扩展性，然而由于未有效处理并发性信号并忽略了电路低层信息，使其有着指数级的时空开销且实用性受限。本项目提出扩展的PTM模型：以晶体管级广义门电路为对象，首先将其结构逻辑抽象等价转换以满足模型对结构的要求；再引入二阶窄可靠度界限理论与多米诺骨牌理论以求取故障相关下的构件故障概率，并基于PTM量化3种屏蔽效应的影响；最后设计二进制与十进制相结合的混合编码机制以实现并发信号阻塞式处理，并基于弱等效原理构造针对PTM的迭代计算方法。通过该研究，以期实现不同抽象层次电路可靠性的高精度快速评估。
面向GPU的非规则应用并行效率优化关键技术研究	负载均衡;多核;非规则应用;并行优化	针对大规模科学计算中非规则应用并行效率低的问题，从负载均衡及数据局部性角度研究面向GPU的非规则应用并行优化技术。通过构建非规则度模型，建立非规则度与负载均衡的内在联系，为并行优化提供理论依据；结合非规则应用动态任务生成及多层次负载不均衡的特性，提出一种负荷敏感的多粒度负载均衡方法，通过构建动态任务划分模型实现负荷敏感的任务划分算法，研究多粒度任务窃取技术以解决多级负载不均衡问题；针对数据局部性引起的cache抖动问题，提出一种资源驱动的cache bypassing方法，通过自适应warp节流实现基于优先级的warp bypassing策略，保证资源充分利用的前提下缓解cache抖动。研究局部性敏感的cache替换策略，提出基于重用距离的指令bypassing方法以减少cache污染。本项研究旨在为非规则应用并行优化提供技术支撑及核心算法，有望提高大规模科学计算中非规则应用的并行效率。
基于64位RISC、共享前端SIMD多核架构的同构通用流处理器体系结构	流处理器;SIMD;多核;RISC;同构	流处理器（SP），如NVIDIA GPU等，将片内主要资源设计为计算单元，并采用海量线程并发机制，已被证明能高效处理计算密集型并行应用，如在医学成像处理领域取得数百倍的性能加速，在天河-1/A超级计算机系统中也被用作加速处理器。但流处理器在通用性和好用性等方面存在问题：计算单元结构的过于简单导致编程困难；CPU和SP存储空间分离，且之间的通讯带宽和延迟很难保证SP计算和数据传输的完全重叠。本课题针对流处理器存在的问题，提出一种新型的同构、基于64位RISC共享前端SIMD多核架构的多线程通用CPU+SP体系结构，该结构吸收了CPU和SP的优点：64位RISC计算单元解决了传统流处理器编程困难的难题；共享前端SIMD多核架构有利于提高流数据的并发处理效率；CPU和SP共享存储空间消除了分离带来的数据传输；同构的RISC核支持CPU和SP间计算单元数目和处理能力的灵活配置，可适应不同特征应用。
面向无线网络的实用化网络编码技术研究	无线网络;分块网络编码;性能优化;非一致性网络编码;端到端信道模型	本项目将以构建面向无线网络环境的实用化网络编码机制为核心，以建立基于随机线性网络编码的性能优化框架为切入点，结合无线网络链路特征（如传输广播特性、链路误码率高、节点处理能力低），围绕"低计算复杂度、低缓存需求、低编码负载、低时延、高容错"等网络编码性能优化目标，从面向网络编码的端到端数据传输信道模型、面向分块网络编码的数据分块与传输策略、适应无线信道特征的非一致网络编码机制、面向典型应用QoS需求的自适应网络编码机制等角度，对支持无线网络性能优化的实用化网络编码机制所涉关键技术开展研究。项目预期成果包括：建立基于超图的随机线性网络编码多路径传输性能优化框架及端到端传输信道分析模型、建立基于迭代解码的网络编码分块机制及支持多数据块并行传输的调度策略、建立基于子空间编码的低负载高容错的非一致网络编码机制、建立面向用户QoS需求的自适应网络编码机制。
近数据处理的非易失内存控制器架构和关键技术	近数据处理;存储系统结构;非易失内存	新型非易失存储器(NVM)成为下一代存储技术的首选，但是如何针对非易失存储器进行控制和管理形成高效的存储设备是面临的重要挑战。本项目以DRAM与NVM融合构成高效安全的非易失存储为目标，针对传统内存控制器硬件固定导致自适应性不足，层次调度导致缺乏一体化管理等突出问题，提出研究按需适配异构融合存储控制器架构及关键技术。包括：建立应用感知的多目标优化模型，形成存储资源异构融合按需适配方法，为控制器设计提供理论基础；研究"硬件可变"的近数据处理加速机制，基于FPGA的控制器可重构方法和一体化调度技术，以自适应满足不同应用需求；研究针对非易失存储器介质特征的性能、寿命优化调度方法，发挥其性能优势且屏蔽其缺陷；针对耐久性问题和非易失特性所带来的安全威胁，形成相应的动态磨损均衡算法和高效内存加密技术，保安全同时性能不降低。项目研究将为构建高速非易失内存系统及控制器提供理论和方法，推进非易失存储器应用。
多核全系统模拟器的加速技术研究	并行;多核;加速技术;全系统模拟器;采样	多核已成为处理器设计的主流和体系结构研究的热点。在多核研究中一方面希望使用真实应用（如ORACLE）作为测试程序，以发现这些应用所面临的性能瓶颈。另一方面也希望有效发现应用程序、系统软件和底层结构的相互作用和影响。因此，作为多核研究基础的多核模拟器在其设计中通常包含操作系统以实现全系统模拟。然而这种设计不仅使模拟器的速度问题进一步恶化从而无法运行真实应用，也使传统单核模拟器加速技术对多核模拟器失去效力。.本课题将对影响模拟器速度的三个主要因素（测试程序、功能模拟和时序模拟）进行深入分析，并结合目前多核硬件平台提供的计算资源，开展多核全系统模拟器加速技术研究，从而使使用真实应用作为测试程序成为可能。研究内容包括面向测试程序的动态采样算法，面向功能模拟的并行动态二进制翻译算法和面向时序模拟的多粒度并行划分算法。研究成果对提升多核模拟器运行速度，加速多核设计和相关研究的进程将具有普遍的适用意义。
云计算服务中基于访问控制时态的安全策略研究与探索	策略组合;安全性分析;时态;云计算;安全策略	云环境中用户需求的多样化需要相关服务的组合来实现，其访问策略具有明显的时态性。由于策略异构性以及服务流程的时态性，目前关于访问控制模型安全性评价的研究多局限于单个安全策略的风险及安全性分析，缺乏对整体任务流程安全性验证与评估的全局考虑。鉴此，本项研究拟针对云计算等典型的分布式多域环境，基于云环境中访问控制的时态特征，针对不同自治域中访问控制策略的时序组合，分析组合策略风险与单策略风险的关系，给出多策略组合的安全性评估的形式化方法，提出多自治域互操作环境中的访问策略时序组合的安全性评估模型以及验证模型，有效地评估和验证这种异构策略组合对服务流程安全性及效用的影响程度，为分布式系统尤其是云计算环境中的访问控制模型提供安全性比较和验证的理论依据。本项研究试图为云计算环境中时序访问控制模型的策略组合及安全性评估和验证提供一种新的思路和方法，具有相当的理论意义和应用价值。
面向嵌入式可信网络的实时性分析研究	实时系统;嵌入式系统;路由协议;可信网络;实时性分析	嵌入式网络作为传统互联网的延伸，是物联网传输层的重要组成部分。可信性与实时性作为嵌入式网络系统的核心设计需求，近年来成为学术界与工业界关注的热点。然而，当前面向网络的可信性与实时性的研究往往相对独立，缺乏对两者统一建模、设计及分析的理论依据。本项目将从网络系统架构、安全机制及路由协议等方面入手，对嵌入式网络的可信性与实时性之间的关系进行量化研究。首先，对可信计算平台与可信网络系统进行面向具体应用的实时性分析。在此基础上，设计融合信任预测管理模型与实时性分析的网络路由协议，同时保障网络通信的可信性与实时性需求。最终，实现路由协议、可信架构与实时性分析间的自适应反馈机制，指导对系统性能、能耗、安全强度等方面的优化与配置。本项目基于理论研究、仿真测试以及原型系统验证，将提出一套面向可信、实时嵌入式网络的建模、设计及分析的完整的系统解决方案，为推动物联网的发展与应用提供理论依据与技术支持。
面向可重构多核处理器系统的分层次自适应优化机制研究	异构多核系统;可重构计算;专用指令;资源利用;自适应优化	处理器系统的优化设计是计算机系统设计中最基础和核心的问题。现阶段，异构、多核、可重构定制已成为处理器设计尤其是嵌入式处理器设计最具前景的方案之一。但是，这也对串行软件在其中的兼容执行性能提出了挑战，同时系统处理资源的充分利用也成为需要考虑的问题。针对这一情况，本项目将多路动态并行剖析方法应用于可重构多核处理器系统中，通过识别关键数据通路获取多核任务的执行热点。在此基础上，1）从局部优化角度，针对系统可重构资源的利用和热点任务的执行，基于核间指令挪用和特征宏指令构造的思想，重点研究多核环境下的指令集自适应混成扩展技术；2）从全局优化角度，针对所有处理核和软件任务，重点研究基于并行指派和动态迁移的两阶段任务自适应并行化机制，提高系统并行度。本课题的顺利开展为解决可重构多核处理器中串行软件的执行性能问题提供了层次化的新方法，同时也对系统资源利用率的提升具有重要理论意义和实际应用价值。
面向飞腾异构并行系统的OpenCL编程模型高效实现技术研究	任务分割;代码定制;OpenCL;异构并行系统;资源管理	国产异构平台的广泛应用需要在软件层面为用户提供开放通用的编程接口与支撑环境。面向国产飞腾异构平台，本项目研究异构编程模型OpenCL的实现与优化技术。在前端，该实现一方面将接受精简的OpenCL代码，提高程序员的编程效率；另一方面将兼容扩展的OpenCL代码，以便于程序员进行性能调优与调试。在后端，本项目将研究平台映射、代码定制、代码调优、资源划分和任务划分等一系列的优化技术，从而使得该OpenCL实现能够充分发挥飞腾异构平台的潜在性能。本项目的研究成果将给程序员提供一种通用开放高效的异构编程接口，能够使得大量的遗留代码无缝地运行在飞腾异构平台上，提高国产异构平台的利用率并将其推向更加广泛的应用领域。
网格环境下传统高性能计算使用模式研究	并行编程;高性能计算;数据管理;网格	考虑到目前的网格用户主要来自对高性能计算有持续需求的传统用户，本课题研究针对这些传统高性能计算用户的实际需求，降低他们使用网格的门槛，推动网格在中国的应用和发展。本课题研究的主要内容包括网格环境下传统高性能计算命令行使用模式的研究、适合传统高性能计算用户习惯的网格集成开发模型研究、网格服务与网格操作的高效映射机制研究、网格用户数据在命令行Shell和开发环境中的目录结构映射机制和数据修改等操作的转换机制研究、网格环境下支持适合网格环境的大规模并行计算的编程机制研究等。网格环境下传统高性能计算使用模式研究的最终目的是使网格用户在使用网格的时候达到和使用传统高性能计算设备相同的体验。
可编程嵌入式系统形式化建模与自动验证技术的研究	可编程嵌入式系统;形式化建模;可靠性;模型检测	可编程嵌入式系统能更好地满足工程的需要，在众多行业中得到广泛使用。随着计算技术的发展，嵌入式软件的规模和复杂性不断增加，计算系统的可靠性更显得重要。模型检测技术能验证一个系统是否满足其属性，查找系统出现的错误，从而降低由于在系统部署前未发现错误而导致灾难性后果的风险。本项目研究可编程嵌入式系统计算技术的可靠性。提取系统关键的属性，采用形式化方法对嵌入式系统建模、抽象、精化和自动检测。利用自适应技术探讨嵌入式软件黑箱模型的自适应建模理论和检测方法。研究模型和属性的分解，提出系统和组件一致性的验证方法。将可满足度方法用于模型检测中，建立基于可满足度的推理框架可信的网络推理系统；改进UML模型检测工具，提出一个检测UML模型的验证方法。改进自动机转换和优化验证搜索算法，减少自动机状态数量，缓解状态爆炸问题。应用本项目的理论和方法，建立和完善嵌入式系统的模型检测工具，并对实际PLC系统作检测验证。
针对云计算环境的安全性分析方法	服务;云计算;安全评估;攻击图;弱点	云计算概念的迅速普及，越来越多的组织和企业采用组合服务模式，组合云环境中不同供应商提供的分布式服务组件，实现复杂的业务流程。与传统 IT环境相比， 服务面临的弱点属性发生很大变化，弱点种类和攻击方式也更加多样化。目前普遍采用的云安全评估方法，无法应对云环境组合服务新特性，评估结果准确性较低；需要依赖大量专家 经验，无法自动化评估。本项目拟依托项目组已有的网络安全领域评估技术和研究平台，以及基础设施云的设计实现技术和运维、应用经验，深入云计算环境安全问题，在此基础上分 析云环境弱点关联性，提出一种针对云计算环境的弱点量化评估方法和面向组合服务的安全评估方法，综合基于弱点和攻击模型的安全评估方法，在保证评估结果准确性基础上，实现 评估过程自动化。项目研究对于充分结合云计算环境服务特性，提高安全评估算法的准确性和自动化程度具有重要理论和实践意义。
基于错误感知的闪存存储系统性能和寿命优化技术研究	应用程序特征;NAND闪存;闪存错误率;闪存寿命;闪存控制器	闪存作为目前发展最为广泛的存储设备具有诸多优点，比如随机访问性能好、功耗低、无振动以及尺寸小等特点。然而，闪存仍然存在两个主要问题，写性能较差和寿命有限，导致闪存的进一步发展和新型应用受到严重阻碍。另一方面，随着闪存密度和尺寸的改进，写性能和寿命呈现指数下降趋势。优化闪存写性能和寿命成为闪存发展至关重要的研究课题。本项目将从闪存性能和寿命的深层次原因，闪存错误，研究如何从控制器和应用程序访问特征两个角度改善闪存存储系统的性能和寿命。首先根据闪存错误源特征，构建以闪存性能和寿命为优化目标的闪存错误模型，为闪存错误感知研究提供基础框架。然后在此基础上开展基于控制器的闪存错误感知研究和基于应用程序特征的闪存错误优化研究，最终达到闪存性能和寿命优化的目标。本项目的成果将为闪存存储设备在嵌入式系统、个人计算机甚至高性能计算机中的性能和寿命优化提供新思路，为各类闪存存储设备的广泛应用提供技术支撑。
大数据流式在线应用的拓扑感知与弹性调度理论及方法研究	大数据计算;流式计算;拓扑感知;弹性调度;在线应用	大数据的时效性日益凸显，流式应用场景越来越多，呈现出了高带宽、低延迟、长期在线等鲜明需求，这和相对成熟的大数据批量计算场景很不一样。然而，大数据流式计算的实践经验和理论研究相对较少。传统流式计算往往在数据库的基础上开展，处理小规模、单一数据源的应用场景，不能满足大数据时代流式计算场景的需求。鉴于此，本项目从系统体系结构的角度，结合具体的大数据流式计算架构，开展大数据流式计算场景中在线应用的拓扑感知与弹性调度理论及方法研究：（1）提出一种细粒度用户应用拓扑感知机制，实现用户应用转化为结构合理的有向任务图；（2）提出一种弹性、自适应的在线调度策略，满足长期在线场景下资源调度需要；（3）提出一种轻量级、快速的容错方法，满足系统对容错时效性的要求。通过对大数据流式计算中应用拓扑结构、在线资源调度、系统容错等内容的研究，完善大数据流式计算架构，满足大数据环境下流式计算中高带宽、低延迟的应用需求。
云计算中基于负载预测的高效能资源配置和管理策略研究	多服务器系统;调度机制;资源利用率;数据中心;能耗管理	对计算能力的需求，促进了大规模数据中心的飞速发展，同时也造成了严重的能耗危机。深入研究云数据中心的节能问题具有非常重要的意义。本项目申请针对云计算负载动态性造成的资源低利用率和高能耗的问题，从云平台配置、资源管理、虚拟机迁移三个层次研究云计算的节能策略。针对云服务负载高峰和低谷时段的差异性，拟采用固定配置+资源动态租用的模式对云平台进行配置。采用M/M/c+D不耐烦顾客排队模型对多服务器系统进行建模，建立能耗最优化模型，并结合偏导极值法和网格二分法求解最优值；针对已有服务器休眠机制导致的服务延迟问题，拟建立系统负载的BP神经网预测模型，提前预测负载的变化趋势，计算满足未来负载需求且能耗最优的资源配置方案，并对服务器进行休眠或者预唤醒。同时针对预测的系统状态对虚机进行动态迁移以及重调度，达到降低能耗和提高资源利用率的目的。本项目有潜力改善企业私有云的高能耗问题，促进云计算的进一步发展。
基于单片多处理器的指令级多线程研究	指令级多线程;线程级并行;指令级并行;单片多处理器	单片多处理器（CMP）由于在线延迟和功耗等方面的优势，必将成为主流微处理器。如何利用CMP中的大量资源，充分开发程序中的多种并行性，加快程序执行速度是未来的一个重要研究课题。本申请课题研究一种基于CMP的指令级多线程系统，采用线程级并行和指令级并行两种并行性提高程序的执行速度，用EPIC思想生成在CMP内部各个处理器核上运行的指令序列（线程），提出几个线程级并行和指令级并行优化算法，采用输出同步机制、动态变量换名和动态隐藏存储延迟等技术尽量消除程序执行过程中的数据相关、模糊相关（如指针和条件分支等）和随机相关（如Cache缺失等），达到或超过全部采用硬件实现的动态调度效果，而处理器核的硬件结构非常简单和规整，并在开放源代码编译器和模拟器中实现所提出算法和方法，使CMP执行程序的并行度能够达到10左右，比目前的主流微处理器提高3倍以上。
新一代汽车嵌入式系统功能安全的建模与算法研究	功能安全;可靠性;实时性;汽车安全完整性等级;可调度性	确保功能安全是汽车嵌入式系统开发的一项重要需求。从计算的视角看，新一代汽车嵌入式系统是一个复杂的"异构分布式嵌入式系统"。本项目旨在从分布式计算的角度研究新一代汽车嵌入式系统中的功能安全问题。以异构分布式集成体系结构和并行与分布式计算方法为基础，分别从可调度性、实时性和可靠性等三个方面研究汽车嵌入式系统的功能安全和汽车安全完整性等级。主要研究内容包括汽车嵌入式系统建模，分布式功能的端到端可调度性分析，多功能混合关键级系统的实时计算，可靠嵌入式系统的容错计算，面向汽车安全完整性等级的功能安全计算等计算理论与方法。通过本项目的研究，用分布式计算完善汽车安全完整性等级基础理论与方法，改善和提升新一代汽车嵌入式系统的功能安全。
云存储中基于隐式可信第三方的数据自保护	完整性检查;云存储;可信第三方;可问责性;存储安全	云存储作为云计算的一种重要形式，具有屏蔽底层管理复杂性，随时随地数据访问，按需资源部署和付费等众多优势。数据安全问题是阻碍云存储得到更广泛应用的重要原因。现有云存储中数据保护方法存在用户负担过大、缺少问责机制等局限，难以在实际系统中应用。本项目针对云存储访问模式和信任假设的特点，研究基于隐式可信第三方的数据自保护架构和方法，在半可信的云存储环境下保护用户数据的完整性和提供问责，同时最大限度减轻用户负担。具体内容包括：符合云存储访问模式特点的数据表示更新模型和自保护架构；基于自检查和自恢复的数据完整性保护，最小化用户在检查过程中参与度；基于自记录的双向问责机制，以不可抵赖的方式证明过失方的不当行为；基于部署在云服务器端的可信硬件的隐式可信第三方实现方法，提高其安全性、易部署性并降低交互开销。本项目旨在提出一种新的更加实用的数据保护方法，为提供具有安全保障的云存储服务奠定理论和技术基础。
千核级高性能、可容错无缓冲片上网络关键技术研究	片上网络;容错;偏转路由;无缓冲路由器;多播	片上网络（NoC）的出现，有效解决了大规模多核处理器的全局通信问题，提升了多核片上通信的性能。但是，随着集成度的不断提高，功耗和面积的日益增加成为制约多核处理器片上互连发展的重要因素；并且，特征尺寸的缩小、电源电压的降低以及时钟频率的提升严重影响片上网络的可靠性。因此，研究高能效、低开销、高可靠的无缓冲片上路由器对于千核级处理器片上互连的设计具有重要意义。 本项目围绕无缓冲路由器微体系结构的性能优化和可靠性设计展开，提出面向千核级处理器片上互连的低延迟、可容错无缓冲路由器体系结构。重点突破无缓冲路由器偏转路由算法的理论分析、低延迟无缓冲路由器结构设计、能够有效检测、处理瞬态故障与永久故障的完整无缓冲路由器容错体系结构以及基于偏转路由的低延迟、可容错无缓冲多播算法。研究成果将对国产多核众核微处理器设计起到重要的基础支撑作用。
备份系统中基于语义挖掘的多层次冗余消除关键技术研究	重复数据删除;差量压缩;恢复性能;备份存储系统;冗余数据消除	随着备份存储系统的数据规模持续增长，多层次冗余消除作为一种融合了重复数据删除、差量压缩和传统压缩的技术，能够分别从重复数据块、相似数据块、重复字符串等多个层次来最大化地检测和消除大规模备份系统中的冗余数据，从而获得越来越多的关注。针对多层次冗余消除带来的索引开销、计算开销、数据碎片等问题与挑战，项目提出了分析和挖掘多层次冗余数据分布与备份数据的用户、版本、文件属性、局部性等语义关联的方法，并据此研究基于备份数据语义感知的重复数据和相似数据的索引组织模式及检测机制，来减少多层次冗余消除的索引开销；研究基于多层次冗余消除计算模型学习的并行计算策略，和基于冗余负载预测的任务调度机制，来加快多层次冗余消除的计算过程；研究基于备份数据语义挖掘的碎片消除和恢复缓存替换算法，来提升冗余消除后的恢复性能。项目将为面向数据备份的多层次冗余消除研究提供新的方法和途径，并推进多层次冗余消除技术的更广泛应用。
智能手机病毒传播动力学建模理论及分析方法研究	社会关系图;病毒;智能手机;半马尔可夫过程;元胞自动机	随着智能手机的迅速普及和面向智能手机的各种应用服务的不断涌现，其安全问题逐渐成为人们关注的焦点。本项目针对如何构建科学的智能手机病毒传播模型和设计有效的手机病毒传播分析方法这一网络安全热点问题，拟对智能手机病毒传播动力学建模理论及分析方法进行研究。提出一种基于半马尔可夫过程的节点状态转换模型，来描述病毒的复杂性及其传播过程的不确定性；设计一种基于元胞自动机的蓝牙病毒传播模型，来刻画在近距离通信方式中病毒传播的动力学演化特性；建立一种基于社会关系图的短信/彩信病毒传播模型，来揭示人们的社会交往与病毒传播之间的联系；提出基于复杂网络理论的病毒传播分析方法来展示病毒传播的复杂性。根据实际应用环境，设计并实现基于多Agent的原型软件系统。通过本项目的研究，可望在理论上为手机病毒传播动力学建模技术及分析方法研究提供一种新的理论依据，并在实际应用中为手机病毒的防控提供一种有效的解决方案。
三值光学计算机乘法器及计算例程平台关键技术研究	避免进位;乘法器;并行可重构;三值光学计算机	本项目以自主设计开发的三值光学计算机系统为实验平台，研究避免进位的新型光学乘法器理论及关键技术，并以此为基础，探索建构公共计算例程平台的基本理论及关键技术，以期充分利用光学计算的特点与优势解决乘法运算及其他数值计算领域中的基本科学问题。项目拟将MSD数的并行性与光学计算的并行性相结合，研究适合于众多位数数据处理的光学乘法器理论，探讨其架构设计原理及其实现方法，建立相应的数学模型，探索低能耗、高数据宽度光学乘法器关键部件的设计方法；在此基础上，充分考虑光学计算机的硬件资源、研究适用于不同计算应用的公共数据位分配策略和技术、处理器自动重构技术及光学计算机硬件资源之间的协同工作机制，设计与第三方计算交互的通信接口与协议，探讨利用公共接口调用系统资源进行数据访问的科学方法、基本算法与应用策略。项目完成后将大大提升光学计算机的应用能力，显著降低能耗,为高性能计算开辟新的途径。
面向个人数据同步和备份服务的数据中心存储节点优化技术研究	同步;数据中心;性能优化;存储节点;个人云	随着面向个人数据同步、备份的个人云存储作为一种新的数据存储模式被越来越多的用户认可，相关领域的竞争也日趋激烈。虽然现有的存储解决方案能够满足这类服务对于后端存储可扩展性的要求，但在残酷的竞争下这种通用解决方案并不占优势。本项目旨在依托通用硬件平台的成本优势，解决目前通用解决方案直接应用于个人云存储时存在的"存储层次多开销大，纯随机读写吞吐率低，以及副本容错机制空间利用率低"等问题，研究如何利用"数据热点明显，降温很快，顺序写为主和纯随机读为辅"的负载特征，充分利用这类服务"时延不敏感，响应时间要求相对宽松"的优势，针对性地优化数据中心后端存储节点。通过采用基于物理设备的数据组织，设计针对性的读写调度策略，以及实现基于副本和纠删码的混合冗余机制等手段，达到改善存储节点性能和提高空间利用率的目标。以期通过提高单设备性能的努力，达到变相降低企业成本，提高企业竞争力的目的。
面向浅水波大气动力方程求解器的可重构计算方法研究	高性能计算;可重构计算;大气模式动力框架;异构计算;FPGA	基于高性能计算系统的大气模式数值模拟方法是目前应对重大气候变化问题的重要手段之一，也是国家战略层面的重大需求以及高性能计算技术研究的一个重要研究方向。对于传统高性能计算平台来说，由于受限于内存带宽等因素,在大气模式模拟中往往很难取得理想的计算效率，高性能计算机强大的计算能力不能得到充分发挥。近年来，基于FPGA的可重构计算方法，因FPGA硬件资源可编程特性以及较低的工作频率，逐渐成为实现高性能、低能耗数值模拟研究的一个极具潜力的平台。针对地球系统模式中大气模式动力框架模拟对高性能平台计算效率与能耗效率越来越高的需求，本项目基于FPGA可重构计算平台研究优化方法，从存储、精度等角度出发设计高性能、低能耗的可重构方法，取得在计算性能上至少一个数量级的提升以及在能耗方面至少一个数量级的降低。
高效粗粒度可重构阵列计算关键技术研究	面向应用的定制处理器;微体系结构;可重构计算	本课题针对应用程序对处理器硬件高计算能力和灵活性的两种迫切需求，以分析目标程序的计算特征、访存模式等程序特性为基础，提出并研究新型粗粒度可重构阵列计算及其相关支持技术，探讨新型粗粒度可重构阵列面向特定应用的高效体系结构、针对应用特征的专用处理单元自动设计方法、增加系统并行性的方法以及程序快速映射算法，使可重构阵列计算结构更高效地满足各种应用场景的需求。.本课题主要研究创新内容有：.　 1）新型的可重构阵列计算体系结构.　 2）面向特定应用的可重构阵列设计方法.　 3）增加可重构阵列并行性的多发射方法.　 4）基于启发式算法的快速程序映射
基于分片复用的多版本容器镜像加载方法研究	基础架构;容器镜像;虚拟化技术;去冗余	容器将应用和支持软件、库文件等封装为镜像，通过发布新版本镜像实现应用升级，导致不同版本之间存在大量相同数据。镜像加载消耗大量时间，使容器启动时间从毫秒级延迟为秒级甚至是分钟级。复用不同版本之间的相同数据有利于减少容器加载时间。当前容器镜像采用继承和分层加载机制，有效实现了支持软件、库文件等数据的复用，但对于应用内部数据还没有一种可靠的复用机制。本课题拟研究一种基于分片复用的多版本容器镜像加载方法，通过复用不同版本之间的相同数据，提升镜像加载效率。研究分为3个方面：1）针对容器镜像数量较多、相同数据查找效率低下的问题，研究一种基于启发式聚类的相似镜像查找方法；2）针对不同版本之间重复数据分布碎片化问题，研究一种基于重写的数据自组织优化方法；3）针对数据复用过程中瓶颈资源不同导致复用效果难以预测的问题，研究一种效用最大化的镜像加载策略决策方法。最后通过原型系统实现，完成对以上方法的检验。
计算机硬盘基片原子级平整表面污染物的作用机制与超精密清洗技术	污染物;计算机硬盘基片;超精密清洗;原子级平整表面	针对下一代垂直存储技术中计算机硬盘基片原子级平整表面"超净、无损伤"的超精密清洗要求，以及现有化学机械抛光（CMP）后清洗技术存在的问题，提出研究硬盘基片原子级精度表面与污染物的相互作用机制，并依此设计相应的多功能表面活性剂及硬盘基片的钝化-清洗新工艺。通过综合考虑原子级硬盘基片表面特性、粒子特性、清洗剂物理化学、清洗方式及其相互作用规律，探索实现硬盘基片"超净、无损伤"清洗的方法与途径。为下一代适合垂直存储技术要求的计算机硬盘基片制造提供技术与理论支持。
虚拟计算环境下磁盘资源管理机制的研究	虚拟化;磁盘性能;磁盘带宽控制;磁盘IO调度	计算系统虚拟化技术是十大IT 关键技术之一，我国已将虚拟计算系统定位于面向国家重大战略需求的基础研究。当前虚拟计算环境下，由于虚拟化技术将计算资源抽象分离，造成磁盘协议栈的复杂和信息感知的障碍，使得现有磁盘IO调度算法效率急速降低；另外，同一物理资源集上同时运行多个虚拟机导致虚拟机间相互影响，急需磁盘带宽控制保证磁盘服务质量。针对这些问题，本项目将在虚拟计算环境下，为用户设计一套高效率的磁盘IO调度算法和磁盘带宽控制机制。首先，通过时序分析提取虚拟计算系统磁盘负载的特点，并使用Black-box和预测行为方式为虚拟计算系统的磁盘协议栈建模；然后，利用多点感知的方式为磁盘IO调度算法建立磁盘请求的统一视图并通过磁盘协议栈模型优化磁盘IO调度算法；接着，通过多队列方式实现磁盘带宽控制，并使用反馈机制协调磁盘IO调度算法和磁盘带宽控制机制；最后，优化CPU调度算法，在CPU层次上提高虚拟机性能。
异构众核芯片的可扩展全局功耗管理机制与算法研究	可扩展;功率控制;众核体系结构;异构多处理器	众核芯片及系统可为应用程序提供高度并行性而成为高性能计算领域的研究热 点，但芯片的动态异构性和功耗管理问题颇具挑战。运行时的任务调度和功耗管理器需要快速适应芯片的变化来保证计算效能。本研究针对众核芯片处理器内核数量不断增加的特点，基于芯片内部程序性能计数器（PMC）设计具有良好准确性、可分解性、适应性和响应特点的功耗模型。然后针对众核芯片所面临的动态异构性设计功耗敏感运行时调度算法，并进一步构建层次化全局功耗管理机制。使用层次化的方式实现芯片级的可扩展功耗管理、内核级的任务映射与功耗规划、以及运行时线程调度，使众核芯片在面临动态异构性时，在给定功率预算约束下保持一个高效能的计算水平。最后，我们将集成阶段性研究成果，通过数值模拟、基于FPGA仿真平台和原型系统实测平台对所提算法和机制进行验证。
分布式存储系统中数据再生的磁盘读写开销优化研究	磁盘阵列;再生码;磁盘I/O开销;分布式存储系统	在分布式存储系统中，由于磁盘或网络故障造成的存储节点失效事件频发，为了保证数据的可靠性和可用性，系统需要以较小的资源开销快速再生出失效数据。再生码虽然可以减少再生过程中的带宽开销，但是它却以增加数据再生过程中从磁盘读取的数据量为代价，即磁盘I/O开销比较大。本项目主要研究再生过程中磁盘I/O开销的优化问题。主要研究内容包括：1)以RAID-6为代表的高存储效率场景下进行精确数据修复的最小磁盘I/O开销；2)探讨混合式修复策略是否可以减小再生过程中的磁盘I/O开销；3)研究数据更新操作的计算复杂度与再生操作的磁盘I/O开销的相互影响。通过研究，了解影响磁盘I/O开销的因素，并设计一类实用的、具有较小磁盘I/O开销和高存储效率的分布式存储编码。
面向高干扰共享云的性能异常检测方法研究	在线迁移;虚拟机;异常检测;云计算	云计算利用虚拟机整合技术有效提升了物理资源的利用率，但同时带来了共享资源的竞争和干扰问题，进而导致应用程序的性能异常。如何对云计算系统中存在的性能异常问题进行有效的检测和精准的定位，使其更好地适应云计算系统的大规模、高复杂度、资源共享等特征，提高云计算系统的可靠性，是一个非常有意义的课题。本项目拟围绕当前云计算系统面临的性能异常诊断的实时性差、检测精度低、开销大等问题，从异常特征提取及跨层次相关性感知的异常检测、程序请求路径追踪及基于依赖图动态构建的异常源概率推理等方面开展研究，并构建一个高干扰共享云系统性能异常诊断原型系统。研究成果将能提升云计算系统的可靠性，增强云用户体验，同时也可为云计算运营商带来巨大的经济收益。
面向混合体系结构的先进并行算法研究	加速处理机;子空间迭代法;粒子输运;异构混合体系结构;并行算法	采用商用通用微处理器与定制加速计算协处理器相结合的混合体系结构成为构造千万亿次计算机系统的一种可行途径，这种体系结构对大规模并行算法研究提出新的挑战，需要深入研究与混合计算机体系结构相适应的先进并行算法。.本项目结合千万亿次异构混合体系结构高性能计算机系统的特点，研究与混合计算机体系结构相适应的先进并行算法，从系统的结点间、结点内和加速处理机间各个层次来开发核心基础算法和典型应用问题的并行性，研制预条件Krylov子空间方法的新型并行算法、粒子输运与粒子模拟典型应用问题的先进混合并行算法和针对协处理器的细粒度并行算法，设计开放式的偏微分方程混合并行计算支撑框架，实现高效能计算。
CPU/GPU异构平台下并行保结构算法的研究	保结构算法;电磁波方程;并行算法;CPU/GPU异构并行	本项目目标是通过分析保结构算法离散得到的系统方程组的结构特征，遵循尽可能保持和体现保结构算法原有保某种结构特征的原则，研究CPU/GPU异构并行的高效、稳定、精确求解的并行算法，使其适合大规模科学工程计算问题的需要。基于此，本项目选择以解决电磁波问题为例，重点考虑选择局部保结构算法、多辛算法、离散变分积分方法、分裂方法等方法，研究推广构建电磁波方程的保结构算法；分析离散得到的方程组系数矩阵特征，研究CPU/GPU异构的高效并行求解算法；研究保持某种结构特征，并适宜于CPU/GPU异构的高效并行预条件子；研究异构并行程序设计的异构编程模式、高效的协同方式、优化策略及性能评价方法等。本项目将形成一套相对完善的适宜于保结构算法解决大规模问题的CPU/GPU异构并行求解器，对完善保结构算法的基本理论、拓宽其应用领域，以及推动CPU/GPU异构并行计算的发展都有着重要的价值和意义。
面向云计算平台的数据安全与隐私保护关键技术研究	云存储;云计算;数据安全;隐私保护	云计算已经成为当前信息技术的主流，云计算安全问题主要源于云计算平台的资源租用特征、多租户资源共享特征，以及平台的开放特征。云计算中数据安全与隐私保护面临的问题主要包括：1）服务提供商有可能对用户存储在云端的数据或程序进行窃取；2）云平台的多租户特性，会导致多租户之间可能通过隐蔽通道之间进行窃取；3）为数据安全引入的第三方审计也可能会导致数据泄漏；4）云平台面临的攻击众多，会导致数据与程序的泄漏途径更加多样化。本项目针对云计算中的数据安全与隐私保护关键技术展开研究，包括基于加密算法的数据安全与隐私保护、云中数据完整性保障技术、云端程序防篡改和防分析技术、用户位置信息隐私保护技术以及基于第三方审计的隐私保护技术等，在此基础上，结合中国移动安全技术研究所移动云平台的用户数据隐私保护需求，开发移动云平台数据隐私保护示范应用系统，对以上关键技术进行验证。
基于传输质量的延迟敏感无线感知系统网络拥塞控制策略研究	传输质量;拥塞控制;无线感知系统;延迟敏感	拥塞控制被广泛用于感知系统传输质量优化，本项目研究拥塞控制算法对无线感知系统中传输质量的多维影响关系，建立基于多维度的网络拥塞与传输质量关系模型，设计基于传输质量的自适应拥塞控制策略；建立感知过程中的传输质量综合衡量体系，通过马尔科夫预测与小参数优化理论计算，结合节点本地信息，估算并建立当拥塞动态产生时节点信息与传输质量的实时影响模型；利用排队论分析关系模型，引入数据压缩与分包的数据优化式拥塞规避算法，研究数据优化在感知周期内对传输质量的影响关系；研究数据转移矩阵与网络负载均衡的关系，结合节点服务队列模型，建立贯穿网络完整生命周期的网络负载特征模型，分析拥塞演化与迁移规律；在数据优化式拥塞规避算法的基础上设计自适应拥塞控制策略，根据节点本地信息与数据转发矩阵动态设定数据优化、路由调度策略，对延迟敏感无线感知系统的发展具有理论价值与实用意义。
多线程条件下众核处理器的访存优化方法研究	众核多线程;轨迹拟合;线程同步;数据推送	众核结构已经成为现代处理器发展的方向，多线程技术也成为高效利用众核资源的途径。在很多应用领域，具有相同代码段的线程之间的执行路径存在很大的相似性，其访存轨迹也存在一定的规律性，这些特性虽然引起了重视，但一直没有在众核多线程情况下找到合适的解决方案，破坏了这些特性，引发了大量的冗余访存操作。本课题将按同步执行、检测规律、优化设计等三个步骤来探索多线程条件下的众核处理器访存优化方法：（1）针对具有相同代码段的多线程，研究核内和核间多线程的同步执行方法，使线程间访存的局部性和规律性得到保持。（2）在同步执行多线程的基础上，检测线程间访存轨迹的相似性和规律性，分别实现核内和核间多线程间的数据预取，提高访存性能。（3）在线程间数据预取的基础上，实现数据的主动推送机制，将预取到的数据继续推送到需要的相关的处理器核内，降低对共享Cache的冗余访问，并提高各级Cache的命中率。
面向异构系统的多面体编译优化技术	多面体模型;代码生成;非规则应用程序;并行编译;异构系统	体系结构和编程模型的差异给异构系统上的编程带来了巨大挑战，利用编译工具自动生成面向异构系统的并行代码是解决这一问题的有效手段。基于多面体模型的编译技术代表了程序自动并行化领域众多方向最先进的水平，然而，目前国内还没有专门从事多面体模型的研究团队。因此，本课题以动态边界循环程序、标准测试集中的复杂应用程序和数值计算函数程序为驱动，利用多面体编译技术开展面向异构系统的代码生成。该课题的研究将扩展多面体模型在新领域中的应用，有助于我国计算机事业在多面体模型领域取得突破，并有希望在异构系统的程序自动并行化领域跻身世界前列。
异构多核嵌入式实时系统能量高效与故障恢复关键技术研究	能耗优化;实时嵌入式系统;异构多核处理器;故障恢复	能量高效与故障恢复是嵌入式系统持久可靠工作的两大基础，两者相互促进，互相制约。通过软硬件贯通的研究方法，构建异构多核嵌入式硬件平台，研究多核实时节能调度算法与控制流故障恢复策略，实现嵌入式实时系统能耗与可靠性综合优化目标。针对嵌入式应用多样化特性，提出支持动态电压调整的异构多核硬件构架。通过动态调整各处理核性能，构建出匹配软件特性的硬件平台。研究异构多核系统能耗与时间多目标优化调度算法，在满足服务时间前提下，最大程度优化能耗。针对故障检测系统能耗高且检测程序易失效问题，提出检测程序与主程序硬件分离的硬件构架。针对异构多核系统程序控制流高并发、异步等特性，研究基于有色Petri网理论的异构系统控制流规约模型。结合此规约模型与标签分析控制流检测算法，实现对异构多核系统控制流故障的检测。预期成果将延长设备工作寿命，修复系统控制流故障，将增强嵌入式系统持久可靠工作性能，促进嵌入式技术发展与应用。
基于虚拟计算环境生命周期的服务器资源调度方法研究	生命周期;虚拟机;虚拟机管理器	在虚拟化技术已经广泛应用于各种服务器系统的背景下，本项目面向的科学命题是：在基于虚拟化技术的大规模服务器系统中，虚拟计算环境具有怎样的生命周期特性？符合什么样的规律？如何利用这种规律来优化服务器资源管理方法，提高资源调度和使用效率？本项目从虚拟计算环境状态特征随时间变化规律的角度，审视和研究了虚拟化环境下服务器资源调度问题。通过系统性地研究虚拟计算环境的生命周期特征，提出基于该特征的服务器资源调度方法，克服传统调度方法以虚拟计算环境空间特性作为调度依据的局限性，使得服务器资源调度机制更加高效合理。主要研究内容包括虚拟机生命周期的特征分析与重置方法、虚拟机管理器老化状态的机理分析与处理方法、基于虚拟计算环境生命周期的服务器整合方法等。本项目研究成果将有效提高服务器资源的管理效率、优化资源调度机制，对于虚拟化技术的发展也具有重要意义。
网络空间安全关键技术研究	安全体系架构;身份认证;风险评估;访问控制;密钥管理	随着信息技术的快速发展，网络已经渗入到人类社会的各个方面，各种设施连接并形成了一个网络空间，和整个人类社会密不可分，随之而来的网络空间的安全性问题也越来越凸显。如何在复杂性不断增长的网络环境中，建立强大有效的信息安全保障体系，是当今全球瞩目的重大课题，研究网络空间安全关键技术具有重要意义。本项目将研究网络空间安全关键技术，集中在安全体系架构和访问控制等方面进行深入研究，并在这些理论上形成突破，建立有效的应对方法与手段，为建设安全、可控、可靠、可信的网络空间，提供丰富的理论参考和有力的技术支撑。
支持动态可扩展Cache一致性分区的众核处理器关键技术研究	动态可重构子网;动态资源调度;众核处理器;动态可扩展Cache一致性;动态分区	目前，片内集成大量轻量级处理器核的众核处理器已经成为研究热点。类似于MPP系统，众核处理器的合理使用模式为多任务共享，每个任务分区（Partition）使用处理器资源。 但是，一方面，不同任务具有不同的资源需求，同一任务的资源需求在执行过程中也体现出明显的阶段性特征。另一方面，由于Cache一致性的协议开销，在众核处理器中实现全局Cache一致性是不可行的。 基于以上原因，本课题研究支持动态可扩展Cache一致性分区（SCCP）的众核处理器关键技术，实现任务资源的按需分配和分区隔离，达到提高资源利用率并有效降低Cache一致性开销的目的。本课题的主要研究内容包括：硬件动态可重构子网、动态可扩展Cache一致性协议，以及支持SCCP的软件动态资源调度机制。 本课题拟采用软件模拟器和关键模块FPGA实现相结合的方法进行仿真验证，并通过典型应用和benchmark进行性能评测。
面向虚实网络融合的灵活与透明链路仿真技术	链路仿真;计算机系统安全;安全评估;网络仿真;网络安全	在网络仿真研究中，基于虚拟化技术，可有效提升仿真规模以及灵活性，基于实物网络，可对各类真实的软硬件设备进行评估，因此虚实网络融合的仿真技术成为当前计算机系统安全评估与网络安全评估的主流。虽然虚实网络融合可保证节点仿真的逼真度，但是，其所依托的硬件资源的固化性以及虚实节点的异构性给链路仿真的灵活性、透明性与逼真性带来了挑战。针对这一问题，本项目首先研究面向虚实网络融合的链路构建技术，实现基于物理服务器集群的虚拟链路以及虚实链路的灵活构建与重构；进一步，研究硬件资源感知的高性能仿真任务划分技术，可为高逼真度链路仿真提供资源保障；最后，研究面向数据报文转发的链路性能参数仿真技术，基于固化与异构的硬件资源，可构建具有任意性能参数的，统一、透明、逼真的虚拟链路以及虚实链路。本项目的研究有助于网络仿真技术在安全评估、新型网络与信息技术的验证等方面发挥重要作用。